question_id,question,model_answer,ground_truth,confidence,cosine_sim
0038b073b7cca847033177024f9719c971692042,How is the input triple translated to a slot-filling task?,,"The relation R(x,y) is mapped onto a question q whose answer is y",0.8380144834518433,0.016371306031942368
00bcdffff7e055f99aaf1b05cf41c98e2748e948,What is the baseline method for the task?, tensorflow backend,"For the emotion recognition from text they use described neural network as baseline.
For audio and face there is no baseline.",0.10429699718952179,0.14180725812911987
00ef9cc1d1d60f875969094bb246be529373cb1d,What methodology is used to compensate for limited labelled data?,,Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.,0.2940821349620819,0.03608874976634979
01123a39574bdc4684aafa59c52d956b532d2e53,By how much does their method outperform state-of-the-art OOD detection?,,"AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average",0.9459685683250427,0.09622544050216675
01dc6893fc2f49b732449dfe1907505e747440b0,What debate topics are included in the dataset?,,"Ethics, Gender, Human rights, Sports, Freedom of Speech, Society, Religion, Philosophy, Health, Culture, World, Politics, Environment, Education, Digital Freedom, Economy, Science and Law",0.5628700852394104,0.005993422120809555
01edeca7b902ae3fd66264366bf548acea1db364,What are the results achieved from the introduced method?,,"Their model resulted in values of 0.476, 0.672 and 0.893 for recall at position 1,2 and 5 respectively in 10 candidates.",0.8886665105819702,0.02686569094657898
02348ab62957cb82067c589769c14d798b1ceec7,What simpler models do they look at?,,"BiGRUs with attention, ROUGE, Language model, and next sentence prediction ",0.6870931386947632,0.13931652903556824
02417455c05f09d89c2658f39705ac1df1daa0cd,How much does it minimally cost to fine-tune some model according to benchmarking framework?,,"$1,728",0.9250831604003906,0.06557273119688034
02e4bf719b1a504e385c35c6186742e720bcb281,How are relations used to propagate polarity?, discourse relations,"based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ",0.06384245306253433,0.25993549823760986
03ce42ff53aa3f1775bc57e50012f6eb1998c480,What 6 language pairs is experimented on?,,"EN<->ES
EN<->DE
EN<->IT
EN<->EO
EN<->MS
EN<->FI",0.9691565036773682,0.07846681028604507
04012650a45d56c0013cf45fd9792f43916eaf83,How much is performance hurt when using too small amount of layers in encoder?,,"comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ",0.25512272119522095,0.009332764893770218
0457242fb2ec33446799de229ff37eaad9932f2a,Which elements of the platform are modular?, integrative,"handling large volume incoming data, sentiment analysis on tweets and predictive online learning",0.6414250135421753,0.15159547328948975
04b43deab0fd753e3419ed8741c10f652b893f02,What are the two decoding functions?,,"a linear projection and a bijective function with continuous transformation though  ‘affine coupling layer’ of (Dinh et al.,2016). ",0.5737118124961853,0.014361999928951263
04f72eddb1fc73dd11135a80ca1cf31e9db75578,How much more coverage is in the new dataset?,,278 more annotations,0.9422232508659363,0.15289105474948883
05671d068679be259493df638d27c106e7dd36d0,What is the performance proposed model achieved on MathQA?,,"Operation accuracy: 71.89
Execution accuracy: 55.95",0.9293819069862366,0.0959615409374237
056fc821d1ec1e8ca5dc958d14ea389857b1a299,How many feature maps are generated for a given triple?,,3 feature maps for a given tuple,0.9673343300819397,0.13143295049667358
06095a4dee77e9a570837b35fc38e77228664f91,"Is all text in this dataset a question, or are there unrelated sentences in between questions?",,the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ,0.3469330668449402,-0.018355201929807663
068dbcc117c93fa84c002d3424bafb071575f431,How was quality measured?, agreement between different consolidated annotations,"Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.",0.08304253965616226,0.687026858329773
07580f78b04554eea9bb6d3a1fc7ca0d37d5c612,Can the approach be generalized to other technical domains as well? ,,"There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.",0.094434954226017,0.024134475737810135
07c59824f5e7c5399d15491da3543905cfa5f751,How big is dataset used for training/testing?,,"4,261  days for France and 4,748 for the UK",0.8581764698028564,0.013602979481220245
0828cfcf0e9e02834cc5f279a98e277d9138ffd9,How was the dataset collected?, we extracted 200 sentences from sorani kurdish books of grades one to three of the primary school in the kurdistan region of iraq,extracted text from Sorani Kurdish books of primary school and randomly created sentences,0.08332439512014389,0.8707160949707031
08333e4dd1da7d6b5e9b645d40ec9d502823f5d7,How much performance gap between their approach and the strong handcrafted method?,,"0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C",0.9632483124732971,-0.048720475286245346
085147cd32153d46dd9901ab0f9195bfdbff6a85,What are the baseline models?,,"MC-CNN
MVCNN
CNN",0.4434809684753418,0.1515083909034729
093039f974805952636c19c12af3549aa422ec43,Is this library implemented into Torch or is framework agnostic?, modularity,It uses deep learning framework (pytorch),0.39752259850502014,0.104822538793087
097ab15f58cb1fce5b5ffb5082b8d7bbee720659,Which language has the lowest error rate reduction?,,thai,0.6615796685218811,0.24923749268054962
0a3a8d1b0cbac559f7de845d845ebbfefb91135e,What accuracy is achieved by the speech recognition system?,,"Accuracy not available: WER results are reported 42.6 German, 35.9 English",0.914291501045227,0.06139291450381279
0a7ac8eccbc286e0ab55bc5949f3f8d2ea2d1a60,How many improvements on the French-German translation benchmark?,,one,0.9861795902252197,0.4496035575866699
0b31eb5bb111770a3aaf8a3931d8613e578e07a8,"What are the selection criteria for ""causal statements""?", one occurrence only of the exact unigrams,"Presence of only the exact unigrams 'caused', 'causing', or 'causes'",0.15351665019989014,0.5847139358520508
0b411f942c6e2e34e3d81cc855332f815b6bc123,What's the method used here?,,Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8.,0.08861719071865082,0.01894042268395424
0b54032508c96ff3320c3db613aeb25d42d00490,What is the training and test data used?,,"Tweets related to a Bank of America DDos attack were used as training data. The test datasets contain tweets related to attacks to Bank of America, PNC and Wells Fargo.",0.10196863859891891,0.01571107655763626
0bd683c51a87a110b68b377e9a06f0a3e12c8da0,What are the tasks that this method has shown improvements?,,"bilingual dictionary induction, monolingual and cross-lingual word similarity, and cross-lingual hypernym discovery",0.15242309868335724,0.02858811803162098
0bd864f83626a0c60f5e96b73fb269607afc7c09,How are sentence embeddings incorporated into the speech recognition system?,,BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer.,0.369165301322937,0.06288011372089386
0c09a0e8f9c5bdb678563be49f912ab6e3f97619,How many roles are proposed?,,12,0.7685595154762268,0.33374154567718506
0c7823b27326b3f5dff51f32f45fc69c91a4e06d,In which setting they achieve the state of the art?, different thresholds,in open-ended task esp. for counting-type questions ,0.3446195423603058,0.1398128718137741
0d7de323fd191a793858386d7eb8692cc924b432,What writing styles are present in the corpus?,,"current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.",0.449660986661911,0.18690267205238342
0da6cfbc8cb134dc3d247e91262f5050a2200664,What topic clusters are identified by LDA?,,"Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club",0.8382615447044373,0.07152681052684784
0fcac64544842dd06d14151df8c72fc6de5d695c,What previous methods is the proposed method compared against?,,"BLSTM+Attention+BLSTM
Hierarchical BLSTM-CRF
CRF-ASN
Hierarchical CNN (window 4)
mLSTM-RNN
DRLM-Conditional
LSTM-Softmax
RCNN
CNN
CRF
LSTM
BERT",0.18707077205181122,0.07516016811132431
0fd678d24c86122b9ab27b73ef20216bbd9847d1,What evaluation metrics are used?,,Accuracy on each dataset and the average accuracy on all datasets.,0.7914016246795654,0.04160938784480095
10ddc5caf36fe9d7438eb5a3936e24580c4ffe6a,Which competitive relational classification models do they test?,,For relation prediction they test TransE and for relation extraction they test position aware neural sequence model,0.4547114670276642,-0.024442540481686592
1165fb0b400ec1c521c1aef7a4e590f76fee1279,How do they model travel behavior?,,The data from collected travel surveys is used to model travel behavior.,0.49913856387138367,0.021332982927560806
11d2f0d913d6e5f5695f8febe2b03c6c125b667c,How is performance of this system measured?, bleu bibref30 metric,using the BLEU score as a quantitative metric and human evaluation for quality,0.05646573752164841,0.47903209924697876
11dde2be9a69a025f2fc29ce647201fb5a4df580,By how much does the new parser outperform the current state-of-the-art?,,Proposed method achieves 94.5 UAS and 92.4 LAS  compared to 94.3 and 92.2 of best state-of-the -art greedy based parser. Best state-of-the art parser overall achieves 95.8 UAS and 94.6 LAS.,0.8000441789627075,0.06652761995792389
126e8112e26ebf8c19ca7ff3dd06691732118e90,What are simulated datasets collected?,,There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents,0.48093539476394653,0.05654290318489075
12ac76b77f22ed3bcb6430bcd0b909441d79751b,What are the competing models?,,"TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.",0.9100167751312256,0.03179953247308731
12cfbaace49f9363fcc10989cf92a50dfe0a55ea,what results do they achieve?,,91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task,0.06630188226699829,0.038780152797698975
12eaaf3b6ebc51846448c6e1ad210dbef7d25a96,How many convolutional layers does their model have?, five,wav2vec has 12 convolutional layers,0.36525753140449524,0.08937554806470871
13d92cbc2c77134626e26166c64ca5c00aec0bf5,What baseline approaches do they compare against?,,"HotspotQA: Yang, Ding, Muppet
Fever: Hanselowski, Yoneda, Nie",0.15896053612232208,0.14853692054748535
13e87f6d68f7217fd14f4f9a008a65dd2a0ba91c,What is the performance of the model?,,"Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates
Experiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)",0.32881999015808105,0.011384285986423492
144714fe0d5a2bb7e21a7bf50df39d790ff12916,What are state of the art methods authors compare their work with? , baselines,"ISOT dataset: LLVM
Liar dataset: Hybrid CNN and LSTM with attention",0.3775252103805542,0.12417025864124298
14634943d96ea036725898ab2e652c2948bd33eb,What is the accuracy of the model for the six languages tested?,,"Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)",0.9743586182594299,0.024534745141863823
14e259a312e653f8fc0d52ca5325b43c3bdfb968,"Is any data-to-text generation model trained on this new corpus, what are the results?", before training the model on the viggo dataset,"Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.",0.015262296423316002,0.09369154274463654
14eb2b89ba39e56c52954058b6b799a49d1b74bf,How are their changes evaluated?,,The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset,0.3980531096458435,0.038432277739048004
157b9f6f8fb5d370fa23df31de24ae7efb75d6f3,How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?,"How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?float selected : table 8. results ( accuracy ) on the test set for variety, gender and their joint prediction. ; for the final evaluation we submitted our system, n - gram, as described in section 2. overall, n - gram came first","They achieved best result in the PAN 2017 shared task with accuracy for Variety prediction task 0.0013 more than the 2nd best baseline, accuracy for Gender prediction task 0.0029 more than 2nd best baseline and accuracy for Joint prediction task 0.0101 more than the 2nd best baseline",0.1269190013408661,0.6353535056114197
160e6d2fc6e04bb0b4ee8d59c06715355dec4a17,What accuracy score do they obtain?,,the best performing model obtained an accuracy of 0.86,0.6990033984184265,0.06305548548698425
16af38f7c4774637cf8e04d4b239d6d72f0b0a3a,How large is the dataset?,,over 104k documents,0.22147037088871002,0.10633870959281921
16f71391335a5d574f01235a9c37631893cd3bb0, What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?,,"Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)",0.2553914189338684,0.05938588082790375
171ebfdc9b3a98e4cdee8f8715003285caeb2f39,How better is accuracy of new model compared to previously reported models?,,"Average accuracy of proposed model vs best prevous result:
Single-task Training: 57.57 vs 55.06
Multi-task Training: 50.17 vs 50.59",0.753188967704773,0.07158610969781876
1771a55236823ed44d3ee537de2e85465bf03eaf,What is the difference in recall score between the systems?,,"Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.",0.9655894041061401,0.06175461784005165
18288c7b0f8bd7839ae92f9c293e7fb85c7e146a,How strong was the correlation between exercise and diabetes?,,weak correlation with p-value of 0.08,0.9020286798477173,-0.04861520975828171
182b6d77b51fa83102719a81862891f49c23a025,What limitations are mentioned?,,"deciding publisher partisanship, risk annotator bias because of short description text provided to annotators",0.1259874552488327,0.07394106686115265
18fbf9c08075e3b696237d22473c463237d153f5,Did the annotators agreed and how much?, moderate to substantial,"For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.",0.25967496633529663,0.29998454451560974
1959e0ebc21fafdf1dd20c6ea054161ba7446f61,How is the clinical text structuring task defined?, cts,CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.,0.10274289548397064,0.3861468434333801
19c9cfbc4f29104200393e848b7b9be41913a7ac,How many questions are in the dataset?," 2, 714","2,714 ",0.5098666548728943,1.0
1a69696034f70fb76cd7bb30494b2f5ab97e134d,By how much does their model outperform existing methods?,,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result.,0.8840893507003784,0.010613290593028069
1a8b7d3d126935c09306cacca7ddb4b953ef68ab,What were their results?, nalcs ( english ) and lms ( traditional chinese ) datasets,Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set,0.2855956554412842,0.4234667122364044
1adbdb5f08d67d8b05328ccc86d297ac01bf076c,What languages do they use?,,"Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.",0.36603257060050964,-0.04099542647600174
1b9119813ea637974d21862a8ace83bc1acbab8e,What dataset do they use?, imbalance,They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).,0.31767240166664124,-0.020395565778017044
1baf87437b70cc0375b8b7dc2cfc2830279bc8b5,"What is the source of the ""control"" corpus?",,"Randomly selected from a Twitter dump, temporally matched to causal documents",0.8342921137809753,0.08076364547014236
1c4cd22d6eaefffd47b93c2124f6779a06d2d9e1,How big is their dataset?,,"3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing",0.9123899936676025,0.10786581039428711
1c68d18b4b65c4d75dc199d2043079490f6310f8,What are the two PharmaCoNER subtasks?,,Entity identification with offset mapping and concept indexing,0.2790275812149048,-0.0006706118583679199
1cbca15405632a2e9d0a7061855642d661e3b3a7,How much improvement do they get?,,Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.,0.9088564515113831,0.06790986657142639
1d74fd1d38a5532d20ffae4abbadaeda225b6932,What is their f1 score and recall?,,"F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.",0.9674019813537598,0.03425201028585434
1ed6acb88954f31b78d2821bb230b722374792ed,What is private dashboard?,"What is private dashboard?for each model having the best fit on the dev set, we export the probability distribution of classes for each sample in the dev set. in this case, we only use the result of model that has f1 _ macro score that larger than 0. 67. the probability distribution of classes is then used as feature to input into a dense model with only one hidden layer ( size 128 ). the training process of the ensemble model is done on samples of the dev set. the best fit result is 0. 7356. the final result submitted in public leaderboard is 0. 73019 and in private leaderboard is 0. 58455",Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).,0.29615435004234314,0.6900635957717896
1f63ccc379f01ecdccaa02ed0912970610c84b72,How much is the gap between using the proposed objective and using only cross-entropy objective?,,The mixed objective improves EM by 2.5% and F1 by 2.2%,0.9668667316436768,0.08144989609718323
1fb73176394ef59adfaa8fc7827395525f9a5af7,Where did they get training data?,,AmazonQA and ConciergeQA datasets,0.336503803730011,0.08691810071468353
20ec88c45c1d633adfd7bff7bbf3336d01fb6f37,what are the evaluation metrics?,,"Precision, Recall, F1",0.8842268586158752,0.13076850771903992
2122bd05c03dde098aa17e36773e1ac7b6011969,What task do they evaluate on?,,Fill-in-the-blank natural language questions,0.26984503865242004,0.097025066614151
21663d2744a28e0d3087fbff913c036686abbb9a,How does their model differ from BERT?,"How does their model differ from BERT?in all our experiments, we used the out - of - the - box bert models without any task - specific fine - tuning",Their model does not differ from BERT.,0.19637461006641388,0.6816678047180176
2210178facc0e7b3b6341eec665f3c098abef5ac,What type of recurrent layers does the model use?,,GRU,0.9531487226486206,0.39389264583587646
22b8836cb00472c9780226483b29771ae3ebdc87,What is the new initialization method proposed in this paper?, skip - gram with negative - sampling ( sgns ) algorithm,They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.,0.1346948891878128,0.33194059133529663
22c802872b556996dd7d09eb1e15989d003f30c0,How do they correlate NED with emotional bond levels?,,They compute Pearson’s correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating,0.14128030836582184,-0.008697954937815666
234ccc1afcae4890e618ff2a7b06fc1e513ea640,How big is performance improvement proposed methods are used?,"How big is performance improvement proposed methods are used?the performance of the base model described in the previous section is shown in the first row of table tabref8 for the nematus cs - en (  ), fb mt system cs - en ( cs ) and es - en ( es ), sequence autoencoder ( seq2seq ), and the average of the adversarial sets ( avg ). we also included the results for the ensemble model, which combines the decisions of five separate baseline models that differ in batch order, initialization, and dropout masking. we can see that, similar to the case in computer vision bibref4, the adversarial examples seem to stem from fundamental properties of the neural networks and ensembling helps only a little","Data augmentation (es)  improved Adv es by 20% comparing to baseline 
Data augmentation (cs) improved Adv cs by 16.5% comparing to baseline
Data augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline 
All models show improvements over adversarial sets  
",0.1888650804758072,0.5575146675109863
2376c170c343e2305dac08ba5f5bda47c370357f,How was the dataset collected?, we crawled travel information in beijing from the web,"They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ",0.0958375558257103,0.48127925395965576
238ec3c1e1093ce2f5122ee60209b969f7669fae,How is the fluctuation in the sense of the word and its neighbors measured?,,"Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.
4) Computing the mean m and the sample variance σ for the uniformities of ai .
5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.",0.8377780318260193,0.0625484362244606
23d32666dfc29ed124f3aa4109e2527efa225fbc,Do the use word embeddings alone or they replace some previous features of the model with word embeddings?,,They use it as addition to previous model - they add new edge between words if word embeddings are similar.,0.4882902503013611,0.061246324330568314
255fb6e20b95092c548ba47d8a295468e06698bd,What datasets are used to evaluate the introduced method?,,"They used a dataset from Taobao which contained a collection of conversation records between customers and customer service staffs. It contains over five kinds of conversations,
including chit-chat, product and discount consultation, querying delivery progress and after-sales feedback. ",0.996306836605072,0.08695058524608612
25b2ae2d86b74ea69b09c140a41593c00c47a82b,How were the navigation instructions collected?,,using Amazon Mechanical Turk using simulated environments with topological maps,0.11274534463882446,0.10208764672279358
25e4dbc7e211a1ebe02ee8dff675b846fb18fdc5,What external sources are used?,,"Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily",0.48530250787734985,0.09087956696748734
25fd61bb20f71051fe2bd866d221f87367e81027,What baselines have been used in this work?,,"NDM, LIDM, KVRN, and TSCP/RL",0.24754929542541504,0.028786711394786835
26c290584c97e22b25035f5458625944db181552,What is the size of their dataset?,,"10,001 utterances",0.9308255910873413,0.1054321825504303
26faad6f42b6d628f341c8d4ce5a08a591eea8c2,How many documents are in the Indiscapes dataset?,,508,0.9948908090591431,0.27738961577415466
271019168ed3a2b0ef5e3780b48a1ebefc562b57,What was performance of classifiers before/after using distant supervision?,,"Bi-LSTM: For low resource <17k clean data: Using distant supervision resulted in huge boost of F1 score (1k eg. ~9 to ~36 wit distant supervision)
BERT: <5k clean data boost of F1 (1k eg. ~32 to ~47 with distant supervision)",0.9746327996253967,0.09146750718355179
2815bac42db32d8f988b380fed997af31601f129,What is improvement in accuracy for short Jokes in relation other types of jokes?,What is improvement in accuracy for short Jokes in relation other types of jokes?our experiment with the short jokes dataset found the transformer model ' s accuracy and f1 score to be 0. 986. this was a jump of 8 percent,It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%,0.1545582413673401,0.4307416081428528
281cd4e78b27a62713ec43249df5000812522a89,What is the average length of the claims?,,Average claim length is 8.9 tokens.,0.29205939173698425,0.03911701217293739
2869d19e54fb554fcf1d6888e526135803bb7d75,What performance did they obtain on the SemEval dataset?,,F1 score of 82.10%,0.9223290681838989,0.08331740647554398
28b2a20779a78a34fb228333dc4b93fd572fda15,Does proposed end-to-end approach learn in reinforcement or supervised learning manner?,,supervised learning,0.38650691509246826,0.1469505876302719
29d917cc38a56a179395d0f3a2416fca41a01659,How are the potentially relevant text fragments identified?,," Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5–10 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.",0.2871280014514923,0.08886009454727173
2a1e6a69e06da2328fc73016ee057378821e0754,How did they detect entity mentions?,"How did they detect entity mentions?in an offline step, we organize the content of each training instance in a graph connecting mentions of candidate answers within and across supporting documents. for a given query , we identify mentions in  of the entities in  and create one node per mention",Exact matches to the entity string and predictions from a coreference resolution system,0.09751931577920914,0.5391517281532288
2a46db1b91de4b583d4a5302b2784c091f9478cc,How many examples do they have in the target domain?,,"Around 388k examples, 194k from tst2013 (in-domain) and 194k from newstest2014 (out-of-domain)",0.8284105062484741,0.10446160286664963
2a6469f8f6bf16577b590732d30266fd2486a72e,What is novel in author's approach?,,"They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data.",0.08876077830791473,0.0354609340429306
2cf8825639164a842c3172af039ff079a8448592,How is the data annotated?,,The data are self-reported by Twitter users and then verified by two human experts.,0.12108909338712692,0.0312250517308712
2d274c93901c193cf7ad227ab28b1436c5f410af,What are the baselines that Masque is compared against?,,"BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D",0.973028838634491,0.1363036036491394
2d307b43746be9cedf897adac06d524419b0720b,How long are the datasets?,,"Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses",0.49936360120773315,-0.017003275454044342
2d3bf170c1647c5a95abae50ee3ef3b404230ce4,Which baseline methods are used?,,standard parametrized attention and a non-attention baseline,0.8156160116195679,0.10070629417896271
2d47cdf2c1e0c64c73518aead1b94e0ee594b7a5,How big is slot filing dataset?,,"Dataset has 1737 train, 497 dev and 559 test sentences.",0.9798946976661682,0.06334497034549713
2d4d0735c50749aa8087d1502ab7499faa2f0dd8,By how much do they outperform previous state-of-the-art models?,,"Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)",0.8095644116401672,0.056842245161533356
2d536961c6e1aec9f8491e41e383dc0aac700e0a,What are all 15 types of modifications ilustrated in the dataset?,,"- paraphrase 1
- paraphrase 2
- different meaning
- opposite meaning
- nonsense
- minimal change
- generalization
- gossip
- formal sentence
- non-standard sentence
- simple sentence
- possibility
- ban
- future
- past",0.3609854280948639,0.09196222573518753
2df2f6e4efd19023434c84f5b4f29a2f00bfc9fb,What other non-neural baselines do the authors compare to? ,,"bag of words, tf-idf, bag-of-means",0.9558702111244202,0.14182277023792267
2e1660405bde64fb6c211e8753e52299e269998f,How long is the dataset?,,"645, 600000",0.6740909218788147,0.09842821955680847
2e1ededb7c8460169cf3c38e6cde6de402c1e720,What is the prediction accuracy of the model?,,"mean prediction accuracy 0.99582651
S&P 500 Accuracy 0.99582651",0.22306238114833832,0.09373427927494049
2ec97cf890b537e393c2ce4c2b3bd05dfe46f683,How do they measure correlation between the prediction and explanation quality?,,They look at the performance accuracy of explanation and the prediction performance,0.4795933961868286,0.11329574882984161
2fa0b9d0cb26e1be8eae7e782ada6820bc2c037f,What were their accuracy results on the task?,,97.32%,0.3194107711315155,0.1440259963274002
3070d6d6a52aa070f0c0a7b4de8abddd3da4f056,How do they measure performance of language model tasks?,,"BPC, Perplexity",0.08495225757360458,0.07400581240653992
30803eefd7cdeb721f47c9ca72a5b1d750b8e03b,How well does their system perform on the development set of SRE?,,"EER 16.04, Cmindet 0.6012, Cdet 0.6107",0.8079544901847839,0.07186440378427505
30b5e5293001f65d2fb9e4d1fdf4dc230e8cf320,What text classification task is considered?,,To classify a text as belonging to one of the ten possible classes.,0.1425313800573349,0.03143944591283798
30dad5d9b4a03e56fa31f932c879aa56e11ed15b,What is the 12 class bilingual text?,,"Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant",0.9854592084884644,0.20183199644088745
3103502cf07726d3eeda34f31c0bdf1fc0ae964e,How do Zipf and Herdan-Heap's laws differ?,,"Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)",0.791756808757782,0.012361535802483559
311a7fa62721e82265f4e0689b4adc05f6b74215,How do they define upward and downward reasoning?,,"Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.",0.7690857648849487,0.05925970524549484
3213529b6405339dfd0c1d2a0f15719cdff0fa93,What is the baseline model used?,,"The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data",0.23757071793079376,0.0746762752532959
327e06e2ce09cf4c6cc521101d0aecfc745b1738,What evaluation metrics did they look at?," promedios normalizados entre [ 0, 1 ] y de su desviacion estandar",accuracy with standard deviation,0.12076355516910553,0.14016211032867432
327e6c6609fbd4c6ae76284ca639951f03eb4a4c,By how much is performance on CN-Celeb inferior to performance on VoxCeleb?,,"For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb",0.9749214053153992,0.014275431632995605
32e8eda2183bcafbd79b22f757f8f55895a0b7b2,How many categories of offensive language were there?,,3,0.8084109425544739,0.49110347032546997
334f90bb715d8950ead1be0742d46a3b889744e7,What semantic features help in detecting whether a piece of text is genuine or generated? of , factually correct,"No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.",0.23523467779159546,0.12817369401454926
33d864153822bd378a98a732ace720e2c06a6bc6,What is new state-of-the-art performance on CoNLL-2009 dataset?,,In closed setting 84.22 F1 and in open 87.35 F1.,0.9151449799537659,0.07172469794750214
33f72c8da22dd7d1378d004cbd8d2dcd814a5291,What is the metric that is measures in this paper?,,error rate in a minimal pair ABX discrimination task,0.36461958289146423,0.01460464671254158
34af2c512ec38483754e94e1ea814aa76552d60a,What benchmarks are created?,,Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples,0.8481760025024414,0.012605380266904831
34fab25d9ceb9c5942daf4ebdab6c5dd4ff9d3db,What dataset did they use?,,"weibo-100k, Ontonotes, LCQMC and XNLI",0.1503864973783493,0.18293973803520203
36a9230fadf997d3b0c5fc8af8d89bd48bf04f12,Which model architecture do they for sentence encoding?,,"Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs
- RNN",0.2268015444278717,0.10041478276252747
36b25021464a9574bf449e52ae50810c4ac7b642,Where does the information on individual-level demographics come from?,,From Twitter profile descriptions of the users.,0.8819626569747925,0.1319243311882019
36feaac9d9dee5ae09aaebc2019b014e57f61fbf,How do they measure model size?," sim has no slot - specific neural network structures, its model size is much smaller",By the number of parameters.,0.06062759459018707,0.10181428492069244
3703433d434f1913307ceb6a8cfb9a07842667dd,What learning paradigms do they cover in this survey?,,"Considering ""What"" and ""How"" separately versus jointly optimizing for both.",0.8488606214523315,0.009681330993771553
3748787379b3a7d222c3a6254def3f5bfb93a60e,What linguistic quality aspects are addressed?, five,"Grammaticality, non-redundancy, referential clarity, focus, structure & coherence",0.7731969356536865,0.13661222159862518
37753fbffc06ce7de6ada80c89f1bf5f190bbd88,What document context was added?, a preceding and a following sentence,Preceding and following sentence of each metaphor and paraphrase are added as document context,0.37155598402023315,0.4310067296028137
37bc8763eb604c14871af71cba904b7b77b6e089,How is module that analyzes behavioral state trained?, multi - label classification scheme,pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus,0.2284708172082901,0.13237455487251282
37c7c62c9216d6cf3d0858cf1deab6db4b815384,how was annotation done?, distributing it over dozens of people,Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations,0.0629439577460289,0.13730652630329132
37e8f5851133a748c4e3e0beeef0d83883117a98,How better is performance of proposed model compared to baselines?, final win rate,"Proposed model achive 66+-22 win rate, baseline CNN 13+-1  and baseline FiLM 32+-3 .",0.21621963381767273,0.38702264428138733
384d571e4017628ebb72f3debb2846efaf0cb0cb,On what dataset is Aristo system trained?,,"Aristo Corpus
Regents 4th
Regents 8th
Regents `12th
ARC-Easy
ARC-challenge ",0.9309393167495728,0.12523242831230164
38a5cc790f66a7362f91d338f2f1d78f48c1e252,What baseline is used?,What baseline is used?the baseline classifier uses a linear support vector machine bibref7,SVM,0.1854802817106247,0.400425523519516
38c74ab8292a94fc5a82999400ee9c06be19f791,How large is the corpus?,,"It contains 106,350 documents",0.7957276701927185,0.038321107625961304
39a450ac15688199575798e72a2cc016ef4316b5,How much performance improvements they achieve on SQuAD?,,Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ,0.8221079111099243,0.020150955766439438
39c78924df095c92e058ffa5a779de597e8c43f4,How are the topics embedded in the #MeToo tweets extracted?,,Using Latent Dirichlet Allocation on TF-IDF transformed from the corpus,0.7237467765808105,0.04575853794813156
39f8db10d949c6b477fa4b51e7c184016505884f,How does their model learn using mostly raw data?,,by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity,0.7073200941085815,0.09038536250591278
3a3a65c65cebc2b8c267c334e154517d208adc7d,What extraction model did they use?, constrained - decoder,"Multi-Encoder, Constrained-Decoder model",0.29404252767562866,0.8229295015335083
3aa7173612995223a904cc0f8eef4ff203cbb860,What baseline models do they compare against?,,"SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)",0.8875253200531006,0.07376773655414581
3aee5c856e0ee608a7664289ffdd11455d153234,What was the performance of their model?,,"For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81",0.10131146013736725,0.02893693372607231
3b391cd58cf6a61fe8c8eff2095e33794e80f0e3,What is the dataset used in the paper?, webhose archived data,"historical S&P 500 component stocks
 306242 news articles",0.08801177889108658,0.0838361382484436
3b745f086fb5849e7ce7ce2c02ccbde7cfdedda5,By how much do they outperform other models in the sentiment in intent classification tasks?,,In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average,0.20887723565101624,0.054223448038101196
3b995a7358cefb271b986e8fc6efe807f25d60dc,What types of word representations are they evaluating?,,GloVE; SGNS,0.20254479348659515,0.20303863286972046
3bfdbf2d4d68e01bef39dc3371960e25489e510e,how do they measure discussion quality?,,"Measuring three aspects: argumentation, specificity and knowledge domain.",0.29842671751976013,0.10608576238155365
3c362bfa11c60bad6c7ea83f8753d427cda77de0,Why did they think this was a good idea?,,They think it will help human TCM practitioners make prescriptions.,0.21932750940322876,0.021742552518844604
3d49b678ff6b125ffe7fb614af3e187da65c6f65,"What does ""explicitly leverages their probabilistic correlation to guide the training process of both models"" mean?", regularization,"The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization.",0.20101004838943481,0.5022867918014526
3d6015d722de6e6297ba7bfe7cb0f8a67f660636,What are the 12 categories devised?,,"Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study",0.9825913310050964,0.02593783661723137
3e839783d8a4f2fe50ece4a9b476546f0842b193,What was their result on Stance Sentiment Emotion Corpus?,,F1 score of 66.66%,0.7925357222557068,0.06734007596969604
3f326c003be29c8eac76b24d6bba9608c75aa7ea,What evaluation metric is used?,,F1 and Weighted-F1,0.19865542650222778,0.08578145503997803
3f3c09c1fd542c1d9acf197957c66b79ea1baf6e,How many annotators participated?,,1,0.9350078105926514,0.47673094272613525
3f5f74c39a560b5d916496e05641783c58af2c5d,How are the synthetic examples generated?,,"Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out",0.1613047868013382,0.11667664349079132
3fb4334e5a4702acd44bd24eb1831bb7e9b98d31,How big are the datasets used?,,"Evaluation datasets used:
CMRC 2018 - 18939 questions, 10 answers
DRCD - 33953 questions, 5 answers
NIST MT02/03/04/05/06/08 Chinese-English - Not specified

Source language train data:
SQuAD - Not specified",0.22946184873580933,0.05938699096441269
405964517f372629cda4326d8efadde0206b7751,How is performance measured?,,they use ROC curves and cross-validation,0.06716124713420868,0.05436611920595169
4059c6f395640a6acf20a0ed451d0ad8681bc59b,How is the delta-softmax calculated?,,Answer with content missing: (Formula) Formula is the answer.,0.2396252602338791,0.034683384001255035
40c2bab4a6bf3c0628079fcf19e8b52f27f51d98,How do they generate the synthetic dataset?, generative process,using generative process,0.10212603211402893,0.9558075070381165
415f35adb0ef746883fb9c33aa53b79cc4e723c3,"In the targeted data collection approach, what type of data is targetted?",,Gendered characters in the dataset,0.3132600784301758,0.04887746274471283
41d3ab045ef8e52e4bbe5418096551a22c5e9c43,what datasets were used?," iwslt14 german - english, turkish - english, and wmt14 english - german","IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German",0.6989976167678833,0.9668579697608948
41e300acec35252e23f239772cecadc0ea986071,What neural machine translation models can learn in terms of transfer learning?,,Multilingual Neural Machine Translation Models,0.6962076425552368,0.03737093508243561
4226a1830266ed5bde1b349205effafe7a0e2337,What meta-information is being transferred?, common relation information,"high-order representation of a relation, loss gradient of relation meta",0.10271774977445602,0.4117949604988098
42394c54a950bae8cebecda9de68ee78de69dc0d,What is the source of external knowledge?,,counts of predicate-argument tuples from English Wikipedia,0.7905699610710144,0.08337201178073883
427252648173c3ba78c211b86fa89fc9f4406653,What domains are detected in this paper?,,"Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW),
“Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). Exceptionally, GEN does
not have a natural root category.",0.9968434572219849,0.09028274565935135
43f86cd8aafe930ebb35ca919ada33b74b36c7dd,In what way is the input restructured?,"In what way is the input restructured?our approach consists of structuring input to the transformer network to use and guide the self - attention of the transformers, conditioning it on the entity","In four entity-centric ways - entity-first, entity-last, document-level and sentence-level",0.08549977093935013,0.3767327070236206
44104668796a6ca10e2ea3ecf706541da1cec2cf,What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?,,Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.,0.9378130435943604,-0.008454643189907074
445e792ce7e699e960e2cb4fe217aeacdd88d392,How do this framework facilitate demographic inference from social media?, using following equation : inlineform2,Demographic information is predicted using weighted lexicon of terms.,0.02029455080628395,0.06904034316539764
44c4bd6decc86f1091b5fc0728873d9324cdde4e,How big is the Japanese data?,,"7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus",0.9786403179168701,0.0627359002828598
4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb,what amounts of size were used on german-english?,,"Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development",0.8050881028175354,0.043165966868400574
45893f31ef07f0cca5783bd39c4e60630d6b93b3,How do they select monotonicity facts?,,They derive it from Wordnet,0.9233362078666687,0.09467898309230804
458dbf217218fcab9153e33045aac08a2c8a38c6,"How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets?",,"Total number of annotated data:
Semeval'15: 10712
Semeval'16: 28632
Tass'15: 69000
Sentipol'14: 6428",0.9639543294906616,0.06423719227313995
45a2ce68b4a9fd4f04738085865fbefa36dd0727,what dataset was used?, a joint adapt - microsoft project,The dataset from a joint ADAPT-Microsoft project,0.16719824075698853,0.7415263652801514
4640793d82aa7db30ad7b88c0bf0a1030e636558,what previous systems were compared to?,,"Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Søgaard and Goldberg (2016) ",0.384594202041626,0.06109265983104706
4688534a07a3cbd8afa738eea02cc6981a4fd285,How do they combine MonaLog with BERT?,,They use Monalog for data-augmentation to fine-tune BERT on this task,0.4236614406108856,0.06081486493349075
4704cbb35762d0172f5ac6c26b67550921567a65,By how much does transfer learning improve performance on this task?,,"In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%",0.8351026773452759,-0.02660607546567917
471d624498ab48549ce492ada9e6129da05debac,What context modelling methods are evaluated?, 13,"Concat
Turn
Gate
Action Copy
Tree Copy
SQL Attn
Concat + Action Copy
Concat + Tree Copy
Concat + SQL Attn
Turn + Action Copy
Turn + Tree Copy
Turn + SQL Attn
Turn + SQL Attn + Action Copy",0.20557452738285065,0.07875534892082214
496b4ae3c0e26ec95ff6ded5e6790f24c35f0f5b,How do they incorporate human advice?, calculating gradients,by converting human advice to first-order logic format and use as an input to calculate gradient,0.18428073823451996,0.5781600475311279
4a4616e1a9807f32cca9b92ab05e65b05c2a1bf5,What were the sizes of the test sets?,,Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences,0.8446433544158936,0.01926514133810997
4a61260d6edfb0f93100d92e01cf655812243724,Which 3 NLP areas are cited the most?,,"machine translation, statistical machine, sentiment analysis",0.3095020353794098,0.0619213730096817
4b41f399b193d259fd6e24f3c6e95dc5cae926dd,How big dataset is used for training this system?, two,"For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.",0.200209379196167,0.2035534381866455
4b8257cdd9a60087fa901da1f4250e7d910896df,How do the authors define or exemplify 'incorrect words'?,,typos in spellings or ungrammatical words,0.3664310872554779,0.118612140417099
4b8a0e99bf3f2f6c80c57c0e474c47a5ee842b2c,To what other competitive baselines is this approach compared?, vhred ( attn ) and mmi,"LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL",0.21146073937416077,0.3949581980705261
4c50f75b1302f749c1351de0782f2d658d4bea70,How is quality of annotation measured?,"How is quality of annotation measured?to control the quality, we ensured that a single annotator annotates maximum 120 headlines ( this protects the annotators from reading too many news headlines and from dominating the annotations ). secondly, we let only annotators who geographically reside in the u. s. contribute to the task. ; we test the annotators on a set of  test questions for the first phase ( about 10 % of the data ) and 500 for the second phase. annotators were required to pass 95 %",Annotators went through various phases to make sure their annotations did not deviate from the mean.,0.08603101223707199,0.5216661095619202
4c7ac51a66c15593082e248451e8f6896e476ffb,What is the performance proposed model achieved on AlgoList benchmark?,,"Full Testing Set Accuracy: 84.02
Cleaned Testing Set Accuracy: 93.48",0.27113574743270874,0.08906778693199158
4c822bbb06141433d04bbc472f08c48bc8378865,How do they extract causality from text?,,"They identify documents that contain the unigrams 'caused', 'causing', or 'causes'",0.8027219176292419,0.034985676407814026
4ca0d52f655bb9b4bc25310f3a76c5d744830043,How large is the first dataset?,,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,0.9901763200759888,0.17945659160614014
4d05a264b2353cff310edb480a917d686353b007,What kind of information do the HMMs learn that the LSTMs don't?,,The HMM can identify punctuation or pick up on vowels.,0.6302598118782043,0.057891152799129486
4d5e2a83b517e9c082421f11a68a604269642f29,how many domains did they experiment with?,,2,0.24584898352622986,0.45951521396636963
4dad15fee1fe01c3eadce8f0914781ca0a6e3f23,How do they prevent the model complexity increasing with the increased number of slots?,,They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).,0.2487315684556961,0.04404022917151451
4e9684fd68a242cb354fa6961b0e3b5c35aae4b6,What is the results of multimodal compared to unimodal models?,,"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4 ",0.9713867902755737,0.05292564630508423
4ef2fd79d598accc54c084f0cca8ad7c1b3f892a,What is the size of their collected dataset?, 30 hours,3347 unique utterances ,0.39920318126678467,0.13015544414520264
5067e5eb2cddbb34b71e8b74ab9210cd46bb09c5,Which matching features do they employ?, multi - perspective,Matching features from matching sentences from various perspectives.,0.41206422448158264,0.376869261264801
50716cc7f589b9b9f3aca806214228b063e9695b,What language technologies have been introduced in the past?,,"- Font & Keyboard
- Speech-to-Text
- Text-to-Speech
- Text Prediction
- Spell Checker
- Grammar Checker
- Text Search
- Machine Translation
- Voice to Text Search
- Voice to Speech Search",0.9496586918830872,0.09037499874830246
51aaec4c511d96ef5f5c8bae3d5d856d8bc288d3,What baselines did they compare their model with?,,the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search,0.3364562690258026,0.05970761924982071
51de39c8bad62d3cbfbec1deb74bd8a3ac5e69a8,Which modifications do they make to well-established Seq2seq architectures?,,"Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible",0.6675190925598145,0.09610206633806229
521a7042b6308e721a7c8046be5084bc5e8ca246,What is a confusion network or lattice?, cn,"graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences",0.5914298295974731,0.027129195630550385
52e8f79814736fea96fd9b642881b476243e1698,What systems are tested?,,"BULATS i-vector/PLDA
BULATS x-vector/PLDA
VoxCeleb x-vector/PLDA
PLDA adaptation (X1)
 Extractor fine-tuning (X2) ",0.24828597903251648,0.04445023462176323
52f7e42fe8f27d800d1189251dfec7446f0e1d3b,How much better is performance of proposed method than state-of-the-art methods in experiments?, significantly outperforms,"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.",0.3468327522277832,0.22655057907104492
53a0763eff99a8148585ac642705637874be69d4,How does the active learning model work?,,"Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.",0.8716183304786682,0.09057885408401489
53bf6238baa29a10f4ff91656c470609c16320e1,What is the source of the textual data? ,,Users' tweets,0.9214814305305481,0.18912255764007568
540e9db5595009629b2af005e3c06610e1901b12,How was a quality control performed so that the text is noisy but the annotations are accurate?,,The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.,0.8050372004508972,0.01474824920296669
5455b3cdcf426f4d5fc40bc11644a432fa7a5c8f,What do they mean by answer styles?,What do they mean by answer styles?we conducted experiments on the two tasks of ms marco 2. 1 bibref5. the answer styles considered in the experiments corresponded to the two tasks,well-formed sentences vs concise answers,0.26762378215789795,0.4051681160926819
545ff2f76913866304bfacdb4cc10d31dbbd2f37,What data were they used to train the multilingual encoder?,,WMT 2014 En-Fr parallel corpus,0.5469601154327393,0.0971795916557312
54c7fc08598b8b91a8c0399f6ab018c45e259f79,How better is performance compared to competitive baselines?,,"Proposed method vs best baseline result on Vecmap (Accuracy P@1):
EN-IT: 50 vs 50
IT-EN: 42.67 vs 42.67
EN-DE: 51.6 vs 51.47
DE-EN: 47.22 vs 46.96
EN-FI: 35.88 vs 36.24
FI-EN: 39.62 vs 39.57
EN-ES: 39.47 vs 39.30
ES-EN: 36.43 vs 36.06",0.5751052498817444,-0.032141052186489105
54c9147ffd57f1f7238917b013444a9743f0deb8,Which are the sequence model architectures this method can be transferred across?," lstm - based, the cnn - based, and the transformer - based",The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models,0.45154184103012085,0.5568974614143372
54fa5196d0e6d5e84955548f4ef51bfd9b707a32,"Are this techniques used in training multilingual models, on what languages?","we extract data from the wmt ' 14 english - french ( en - fr ) and english - german ( en - de ) datasets. to create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance",English to French and English to German,0.03598820045590401,0.38651543855667114
54fe8f05595f2d1d4a4fd77f4562eac519711fa6,How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?,,Systems do not perform well both in Facebook and Twitter texts,0.25764837861061096,0.06410844624042511
551f77b58c48ee826d78b4bf622bb42b039eca8c,What are the weaknesses of their proposed interpretability quantification method?,,can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories,0.713322103023529,0.04426472634077072
55588ae77496e7753bff18763a21ca07d9f93240,What are the characteristics of the rural dialect?," small, scattered population",It uses particular forms of a concept rather than all of them uniformly,0.33483782410621643,0.07094580680131912
564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee,Which languages do they validate on?,,"Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur",0.9768714904785156,0.15357428789138794
5712a0b1e33484ebc6d71c70ae222109c08dede2,What benchmark datasets they use?,,VQA and GeoQA,0.2972707748413086,0.15177109837532043
572458399a45fd392c3a4e07ce26dcff2ad5a07d,How much more accurate is the model than the baseline?,,"For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. ",0.6104879379272461,0.0649459958076477
57388bf2693d71eb966d42fa58ab66d7f595e55f,How is morphology knowledge implemented in the method?,,A BPE model is applied to the stem after morpheme segmentation.,0.5129474997520447,0.03814397752285004
579941de2838502027716bae88e33e79e69997a6,What is difference in peformance between proposed model and state-of-the art on other question types?, exact match ( em ) and f1,"For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.
For number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. 
For date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.",0.3376496136188507,0.3331701457500458
58a340c338e41002c8555202ef9adbf51ddbb7a1,Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?, imdb,SST-2 dataset,0.11322265863418579,0.07989570498466492
58edc6ed7d6966715022179ab63137c782105eaf,Which one of the four proposed models performed best?, lft,the hybrid model MinAvgOut + RL,0.2251811921596527,0.13475768268108368
58f50397a075f128b45c6b824edb7a955ee8cba1,How many shared layers are in the system?,,1,0.9955484867095947,0.47673094272613525
5908d7fb6c48f975c5dfc5b19bb0765581df2b25,How big is the dataset?, 7934 messages,Resulting dataset was 7934 messages for train and 700 messages for test.,0.17647530138492584,0.5925534963607788
593e307d9a9d7361eba49484099c7a8147d3dade,What are causal attribution networks?, a collection of text pairs that reflect cause - effect relationships proposed by humans,"networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans",0.13520143926143646,0.6612588167190552
5a0841cc0628e872fe473874694f4ab9411a1d10,By how much did they outperform the other methods?,,"on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI",0.9742991924285889,-0.029364317655563354
5a23f436a7e0c33e4842425cf86d5fd8ba78ac92,How big is dataset used?,,"553,451 documents",0.9475765824317932,0.03826819360256195
5a29b1f9181f5809e2b0f97b4d0e00aea8996892,What makes it a more reliable metric?,,It takes into account the agreement between different systems,0.816991925239563,0.0958484560251236
5a33ec23b4341584a8079db459d89a4e23420494,What is public dashboard?,,"Public dashboard where competitors can see their results during competition, on part of the test set (public test set).",0.5954087376594543,0.17781713604927063
5a9f94ae296dda06c8aec0fb389ce2f68940ea88,By how much does their method outperform the multi-head attention model?,,Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points.,0.9843817353248596,-0.0069693922996521
5b2480c6533696271ae6d91f2abe1e3a25c4ae73,Is the assumption that natural language is stationary and ergodic valid?, we follow,It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement.,0.1614442765712738,0.038966141641139984
5b6aec1b88c9832075cd343f59158078a91f3597,How does proposed word embeddings compare to Sindhi fastText word representations?,,"Proposed SG model vs SINDHI FASTTEXT:
Average cosine similarity score: 0.650 vs 0.388
Average semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391",0.9384526014328003,-0.0035120234824717045
5bcc12680cf2eda2dd13ab763c42314a26f2d993,What evaluation metrics were used in the experiment?, accuracy and mrr ( mean reciprocal ranking ),"For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy",0.39553722739219666,0.4413164556026459
5be94c7c54593144ba2ac79729d7545f27c79d37,What is the challenge for other language except English,,not researched as much as English,0.5650314092636108,-0.020838569849729538
5c4c8e91d28935e1655a582568cc9d94149da2b2,Does DCA or GMM-based attention perform better in experiments?,,About the same performance,0.6944431066513062,0.13523751497268677
5c90e1ed208911dbcae7e760a553e912f8c237a5,How big are the datasets?,,"In-house dataset consists of  3716 documents 
ACE05 dataset consists of  1635 documents",0.38505247235298157,0.04452846199274063
5c95808cd3ee9585f05ef573b0d4a52e86d04c60,Which journal and conference are cited the most in recent years?,,CL Journal and EMNLP conference,0.5913112759590149,0.12670911848545074
5d6cc65b73f428ea2a499bcf91995ef5441f63d4,How they evaluate quality of generated output?, human evaluation,Through human evaluation where they are asked to evaluate the generated output on a likert scale.,0.27089202404022217,0.6057270169258118
5d9b088bb066750b60debfb0b9439049b5a5c0ce,what processing was done on the speeches before being parsed?,,Remove numbers and interjections,0.9374850392341614,0.0790129229426384
5dc1aca619323ea0d4717d1f825606b2b7c21f01,Which major geographical regions are studied?," northeast, south, west, and midwest","Northeast U.S, South U.S., West U.S. and Midwest U.S.",0.7543348670005798,0.9021651744842529
5dfa59c116e0ceb428efd99bab19731aa3df4bbd,How many natural language explanations are human-written?,,Totally 6980 validation and test image-sentence pairs have been corrected.,0.9965652823448181,0.1266629695892334
5e5460ea955d8bce89526647dd7c4f19b173ab34,How many of the utterances are transcribed?,,Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned),0.9260793924331665,0.07028641551733017
5e65bb0481f3f5826291c7cc3e30436ab4314c61,What discourse features are used?,,Entity grid with grammatical relations and RST discourse relations.,0.22077880799770355,0.08704061806201935
5e9732ff8595b31f81740082333b241d0a5f7c9a,How much better were results of the proposed models than base LSTM-RNN model?, statistically significantly higher f1 values,on diversity 6.87 and on relevance 4.6 points higher,0.12960878014564514,0.2280169129371643
5f7850254b723adf891930c6faced1058b99bd57,"What kind of features are used by the HMM models, and how interpretable are those?", 10 lstm state dimensions and 10 hmm states,"A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. 
The interpretability of the model is shown in Figure 2. ",0.47508418560028076,0.6104306578636169
5f9bd99a598a4bbeb9d2ac46082bd3302e961a0f,What are the models evaluated on?,,They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA),0.9534249901771545,0.058420754969120026
5fb348b2d7b012123de93e79fd46a7182fd062bd,What datasets are used to evaluate the approach?, nell - one and wiki - one,"NELL-One, Wiki-One",0.37948036193847656,0.9868476390838623
5fb6a21d10adf4e81482bb5c1ec1787dc9de260d,How do they quantify moral relevance?,,By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence,0.8119293451309204,0.15177318453788757
602396d1f5a3c172e60a10c7022bcfa08fa6cbc9,By how much do they outperform BiLSTMs in Sentiment Analysis?,,Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets.,0.5661863088607788,0.03668222948908806
61fb982b2c67541725d6db76b9c710dd169b533d,Is infinite-length sequence generation a result of training with maximum likelihood?,,There are is a strong conjecture that it might be the reason but it is not proven.,0.31505855917930603,0.027995990589261055
6270d5247f788c4627be57de6cf30112560c863f,Did they experiment with tasks other than word problems in math?,,They experimented with sentiment analysis and natural language inference task,0.3893529772758484,0.023692883551120758
63723c6b398100bba5dc21754451f503cb91c9b8,What is the state of the art?, conll 2018 shared task results with predicted tokenisation and segmentation in an updated version of the paper,"POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)",0.21042971312999725,0.5249215364456177
63850ac98a47ae49f0f49c1c1a6e45c6c447272c,What is the problem with existing metrics that they are trying to address?,,"Answer with content missing: (whole introduction) However, recent
studies observe the limits of ROUGE and find in
some cases, it fails to reach consensus with human.
judgment (Paulus et al., 2017; Schluter, 2017).",0.9944337606430054,-0.0050966814160346985
6389d5a152151fb05aae00b53b521c117d7b5e54,What is typical GAN architecture for each text-to-image synhesis group?,,"Semantic Enhancement GANs: DC-GANs, MC-GAN
Resolution Enhancement GANs: StackGANs, AttnGAN, HDGAN
Diversity Enhancement GANs: AC-GAN, TAC-GAN etc.
Motion Enhancement GAGs: T2S, T2V, StoryGAN",0.22654902935028076,0.09428366273641586
63c0128935446e26eacc7418edbd9f50cba74455,What is the size of the released dataset?,,"440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples.",0.9516475200653076,0.09132225811481476
6412e97373e8e9ae3aa20aa17abef8326dc05450,What baseline model is used?,,Human evaluators,0.8785475492477417,0.20726263523101807
6472f9d0a385be81e0970be91795b1b97aa5a9cf,Do they train a different training method except from scheduled sampling?,,"Answer with content missing: (list missing) 
Scheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.

Yes.",0.645356297492981,0.09287501871585846
657edbf39c500b2446edb9cca18de2912c628b7d,What was their perplexity score?,,Perplexity score 142.84 on dev and 138.91 on test,0.4723193049430847,0.015680914744734764
675f28958c76623b09baa8ee3c040ff0cf277a5a,What is the size of the dataset?,,"300,000 sentences with 1.5 million single-quiz questions",0.6288971900939941,0.11905722320079803
67ee7a53aa57ce0d0bc1a20d41b64cb20303f4b7,How much training data is used?,,"163,110,000 utterances",0.4879785478115082,0.06356222182512283
68794289ed6078b49760dc5fdf88618290e94993,What are proof paths?,,A sequence of logical statements represented in a computational graph,0.9125298261642456,0.10791809856891632
68e3f3908687505cb63b538e521756390c321a1c,What is the performance difference of using a generated summary vs. a user-written one?,,2.7 accuracy points,0.7898404002189636,0.12178130447864532
68ff2a14e6f0e115ef12c213cf852a35a4d73863,Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?,,The dataset contains about 590 tweets about DDos attacks.,0.14447109401226044,0.0269891619682312
6a9eb407be6a459dc976ffeae17bdd8f71c8791c,What is the reward model for the reinforcement learning appraoch?," 1 for successfully completing the task, and 0 otherwise","reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail",0.29981401562690735,0.7097521424293518
6b91fe29175be8cd8f22abf27fb3460e43b9889a,what genres do they songs fall under?,,"Gospel, Sertanejo, MPB, Forró, Pagode, Rock, Samba, Pop, Axé, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda",0.19289900362491608,0.07174737751483917
6baf5d7739758bdd79326ce8f50731c785029802,Which four languages do they experiment with?,,"German, English, Italian, Chinese",0.9808058738708496,0.060536593198776245
6ce057d3b88addf97a30cb188795806239491154,What models are included in baseline benchmarking results?,,"BERT, XLNET RoBERTa, ALBERT, DistilBERT",0.4901049733161926,0.1477772295475006
6dcbe941a3b0d5193f950acbdc574f1cfb007845,What are the domains covered in the dataset?, 17,"Alarm
Bank
Bus
Calendar
Event
Flight
Home
Hotel
Media
Movie
Music
RentalCar
Restaurant
RideShare
Service
Travel
Weather",0.49657201766967773,0.07222118973731995
6e8c587b6562fafb43a7823637b84cd01487059a,How much is the BLEU score?,,Ranges from 44.22 to 100.00 depending on K and the sequence length.,0.8625219464302063,-0.004859297536313534
6e97c06f998f09256be752fa75c24ba853b0db24,How do the authors measure performance?,,Accuracy across six datasets,0.6991140842437744,0.02562667429447174
6ea63327ffbab2fc734dd5c2414e59d3acc56ea5,How large is the gap in performance between the HMMs and the LSTMs?,,"With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.",0.9974385499954224,-0.04499754309654236
6f2118a0c64d5d2f49eee004d35b956cb330a10e,What datasets are used for training/testing models? ,,"Microsoft Research dataset containing movie, taxi and restaurant domains.",0.6597142219543457,0.026474151760339737
6f2f304ef292d8bcd521936f93afeec917cbe28a,How much improvement is gained from the proposed approaches?,,It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.,0.14994561672210693,0.026285801082849503
707db46938d16647bf4b6407b2da84b5c7ab4a81,How much F1 was improved after adding skip connections?,,"Simple Skip improves F1 from 74.34 to 74.81
Transformer Skip improes F1 from 74.34 to 74.95 ",0.9905542135238647,0.08907245844602585
7182f6ed12fa990835317c57ad1ff486282594ee,How does the SCAN dataset evaluate compositional generalization?," in a sequence - to - sequence ( seq2seq ) setting by systematically holding out of the training set all inputs containing a basic primitive verb ( "" jump "" ), and testing on sequences containing that verb","it systematically holds out inputs in the training set containing basic primitive verb, ""jump"", and tests on sequences containing that verb.",0.061573587357997894,0.7743220329284668
71d59c36225b5ee80af11d3568bdad7425f17b0c,How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?,,Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ,0.9973641633987427,0.12442844361066818
728a55c0f628f2133306b6bd88af00eb54017b12,What geometric properties do embeddings display?,,Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.,0.8086372017860413,0.012025998905301094
7348e781b2c3755b33df33f4f0cab4b94fcbeb9b,"Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",,Only automatic methods,0.9440898299217224,0.07815612107515335
7358a1ce2eae380af423d4feeaa67d2bd23ae9dd,How do their train their embeddings?, embeddings train set,"The embeddings are learned several times using the training set, then the average is taken.",0.17110970616340637,0.5650845170021057
73633afbefa191b36cca594977204c6511f9dad4,"Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?",,"Not at the moment, but summaries can be additionaly extended with this annotations.",0.7071030735969543,-0.004252862185239792
737397f66751624bcf4ef891a10b29cfc46b0520,Which datasets are used in the paper?,,"Google N-grams
COHA
Moral Foundations Dictionary (MFD)
",0.7922714948654175,0.04111911356449127
73bb8b7d7e98ccb88bb19ecd2215d91dd212f50d,What human evaluation method is proposed?,,comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant,0.3833651840686798,0.01783180981874466
73bbe0b6457423f08d9297a0951381098bd89a2b,what were the baselines?,,"2008 Punyakanok et al. 
2009 Zhao et al. + ME 
2008 Toutanova et al. 
2010 Bjorkelund et al.  
2015 FitzGerald et al. 
2015 Zhou and Xu 
2016 Roth and Lapata 
2017 He et al. 
2017 Marcheggiani et al.
2017 Marcheggiani and Titov 
2018 Tan et al. 
2018 He et al. 
2018 Strubell et al. 
2018 Cai et al. 
2018 He et al. 
2018 Li et al. 
",0.7481469511985779,0.16763359308242798
74091e10f596428135b0ab06008608e09c051565,How is knowledge stored in the memory?,,entity memory and relational memory.,0.8542832732200623,0.03749312832951546
74261f410882551491657d76db1f0f2798ac680f,What are the six target languages?,,"Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI).",0.9901371598243713,0.03548479825258255
74cd51a5528c6c8e0b634f3ad7a9ce366dfa5706,What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?,,it is less expensive and quantifies interpretability using continuous values rather than binary evaluations,0.3075617551803589,0.02051548659801483
74db8301d42c7e7936eb09b2171cd857744c52eb,How is the performance on the task evaluated?,,Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors,0.11672758311033249,0.03358779847621918
753990d0b621d390ed58f20c4d9e4f065f0dc672,What is the seed lexicon?, positive and negative predicates,a vocabulary of positive and negative predicates that helps determine the polarity score of an event,0.673158586025238,0.67779541015625
75b69eef4a38ec16df63d60be9708a3c44a79c56,How much better peformance is achieved in human evaluation when model is trained considering proposed metric?,,"Pearson correlation to human judgement - proposed vs next best metric
Sample level comparison:
- Story generation: 0.387 vs 0.148
- Dialogue: 0.472 vs 0.341
Model level comparison:
- Story generation:  0.631 vs 0.302
- Dialogue: 0.783 vs 0.553",0.430416077375412,-0.018417932093143463
76377e5bb7d0a374b0aefc54697ac9cd89d2eba8,How do they obtain word lattices from words?,,By considering words as vertices and generating directed edges between neighboring words within a sentence,0.08412878215312958,0.10937859117984772
78577fd1c09c0766f6e7d625196adcc72ddc8438,What dataset is used for train/test of this method?," tts system dataset : we trained our tts system with a mixture of neutral and newscaster style speech. for a total of 24 hours of training data, split in 20 hours of neutral ( 22000 utterances ) and 4 hours of newscaster styled speech ( 3000 utterances ). ; ( ii ) embedding selection dataset : as the evaluation was carried out only on the newscaster speaking style, we restrict our linguistic search space to the utterances associated to the newscaster style : 3000 sentences. ; experimental protocol : : : datasets : : : evaluation dataset ; the systems were evaluated on two datasets : ; ( i ) common prosody errors ( cpe ) : the dataset on which the baseline prostron model fails to generate appropriate prosody. this dataset consists of complex utterances like compound nouns ( 22 % ), “ or "" questions ( 9 % ), “ wh "" questions ( 18 % ) . this set is further enhanced by sourcing complex utterances ( 51 % ) from bibref24. ; ( ii ) lfr : as demonstrated in bibref25, evaluating sentences in isolation does not suffice if we want to evaluate the quality of long - form speech. thus, for evaluations on lfr we curated a dataset of news samples",Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset.,0.13835455477237701,0.699205756187439
785eb3c7c5a5c27db14006ac357299ed1216313a,What they formulate the question generation as?,,LASSO optimization problem,0.9549680948257446,0.13385172188282013
78c010db6413202b4063dc3fb6e3cc59ec16e7e3,What is different in the improved annotation protocol?, better coverage,a trained worker consolidates existing annotations ,0.047380685806274414,0.08972223848104477
7920f228de6ef4c685f478bac4c7776443f19f39,What language is the Twitter content in?,,English,0.9255678653717041,0.28494974970817566
7994b4001925798dfb381f9aa5c0545cdbd77220,How do they perform data augmentation?,,They randomly sample sentences from Wikipedia that contains an object RC and add them to training data,0.8374921083450317,0.04913243651390076
7997b9971f864a504014110a708f215c84815941,"What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?",,"Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text",0.6253905296325684,0.07895972579717636
79a28839fee776d2fed01e4ac39f6fedd6c6a143,What is the main contribution of the paper? ,,"Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance",0.0975559800863266,0.021804973483085632
79bb1a1b71a1149e33e8b51ffdb83124c18f3e9c,What accuracy does CNN model achieve?,,Combined per-pixel accuracy for character line segments is 74.79,0.8306382298469543,0.095168337225914
79ed71a3505cf6f5e8bf121fd7ec1518cab55cae,How do they damage different neural modules?, randomly initializing their weights,"Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.",0.3672883212566376,0.3985375761985779
7a53668cf2da4557735aec0ecf5f29868584ebcf,What kind of instructional videos are in the dataset?, screencast tutorial videos,tutorial videos for a photo-editing software,0.20942339301109314,0.6287986040115356
7a7e279170e7a2f3bc953c37ee393de8ea7bd82f,What two types the Chinese reading comprehension dataset consists of?,,cloze-style reading comprehension and user query reading comprehension questions,0.7721896767616272,0.06257975101470947
7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed,Is ROUGE their only baseline?,,"No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.",0.3824578821659088,-0.06059153750538826
7af01e2580c332e2b5e8094908df4e43a29c8792,How was lexical diversity measured?,,By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions,0.9813126921653748,0.05424151197075844
7b2bf0c1a24a2aa01d49f3c7e1bdc7401162c116,How are the keywords associated with events such as protests selected?,,"By using a Bayesian approach  and by using word-pairs, where they extract all the pairs of co-occurring words within each tweet.  They search for the words that achieve the highest number of spikes matching the days of events.",0.7456737160682678,0.05349764972925186
7b44bee49b7cb39cb7d5eec79af5773178c27d4d,How is the data in RAFAEL labelled?,,"Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner",0.5739042162895203,0.04295603930950165
7b89515d731d04dd5cbfe9c2ace2eb905c119cbc,Which is the baseline model?, i - vector model,"The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. ",0.3312724828720093,0.28182452917099
7cf726db952c12b1534cd6c29d8e7dfa78215f9e,What is a word confusion network?,,It is a network used to encode speech lattices to maintain a rich hypothesis space.,0.35960128903388977,0.06898647546768188
7d3c036ec514d9c09c612a214498fc99bf163752,What is the source of the dataset?,,"Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera",0.5481283068656921,0.08818536251783371
7d59374d9301a0c09ea5d023a22ceb6ce07fb490,How do they measure the diversity of inferences?,,by number of distinct n-grams,0.96275395154953,0.014698488637804985
7ee29d657ccb8eb9d5ec64d4afc3ca8b5f3bcc9f,What were the results of the first experiment?,,Best performance achieved is 0.72 F1 score,0.39212608337402344,0.07138655334711075
7f9bc06cfa81a4e3f7df4c69a1afef146ed5a1cf,What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?,,"10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%
50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%",0.12641121447086334,0.013212332502007484
8051927f914d730dfc61b2dc7a8580707b462e56,What baseline algorithms were presented?,,"a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm",0.7966832518577576,0.12804822623729706
81064bbd0a0d72a82d8677c32fb71b06501830a0,By how much is precission increased?,,"ROUGE-1 increases by 0.05, ROUGE-2 by 0.06 and ROUGE-L by 0.09",0.9978789687156677,0.017176993191242218
81cee2fc6edd9b7bc65bbf6b4aa35782339e6cff,What type of documents are supported by the annotation platform?,,"Variety of formats supported (PDF, Word...), user can define content elements of document",0.1887263059616089,-0.035654421895742416
81e8d42dad08a58fe27eea838f060ec8f314465e,What is the state-of-the art?,,neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder,0.4814768135547638,0.08871814608573914
8255f74cae1352e5acb2144fb857758dda69be02,How do they measure grammaticality?,,by calculating log ratio of grammatical phrase over ungrammatical phrase,0.313770592212677,0.04782509058713913
82a28c1ed7988513d5984f6dcacecb7e90f64792,How big are negative effects of proposed techniques on high-resource tasks?,,The negative effects were insignificant.,0.9583495259284973,0.07858891040086746
83f14af3ccca4ab9deb4c6d208f624d1e79dc7eb,Which of the two ensembles yields the best performance?, e - bert,Answer with content missing: (Table 2) CONCAT ensemble,0.20327748358249664,0.06784844398498535
8427988488b5ecdbe4b57b3813b3f981b07f53a5,On which task does do model do best?, joint task,Variety prediction task,0.25532492995262146,0.25067585706710815
8434974090491a3c00eed4f22a878f0b70970713,How big is their model?,,Proposed model has 1.16 million parameters and 11.04 MB.,0.6485583186149597,0.07550743222236633
8568c82078495ab421ecbae38ddd692c867eac09,How many layers of self-attention does the model have?,,"1, 4, 8, 16, 32, 64",0.8275801539421082,0.11470288038253784
858c51842fc3c1f3e6d2d7d853c94f6de27afade,Which of the classifiers showed the best performance?,,Logistic regression,0.9758942723274231,0.15959547460079193
85912b87b16b45cde79039447a70bd1f6f1f8361,How large is the corpus they use?,,449050,0.9836582541465759,0.12692587077617645
85e45b37408bb353c6068ba62c18e516d4f67fe9,What is the baseline?,,The baseline is a multi-task architecture inspired by another paper.,0.12023704499006271,0.05755560100078583
8602160e98e4b2c9c702440da395df5261f55b1f,What are the three datasets used in the paper?," age, dialect, and gender",Data released for APDA shared task contains 3 datasets.,0.5422763228416443,0.06212536245584488
863d5c6305e5bb4b14882b85b6216fa11bcbf053,What are the 12 AV approaches which are examined?,,"MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD",0.03446279838681221,0.14795459806919098
86cd1228374721db67c0653f2052b1ada6009641,What domain does the dataset fall into?,,YouTube videos,0.2501760721206665,0.16601240634918213
880a76678e92970791f7c1aad301b5adfc41704f,What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?,,"ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.
NLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing ""retweets"", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation.",0.4308265149593353,0.05275699496269226
894c086a2cbfe64aa094c1edabbb1932a3d7c38a,What are the state-of-the-art systems?, semeval 2016 task 6,"For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN",0.1922503560781479,0.12271085381507874
8951fde01b1643fcb4b91e51f84e074ce3b69743,How they evaluate their approach?," in several low - resource settings across different languages with real, distantly supervised data with non - synthetic noise","They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise",0.10670207440853119,0.9492777585983276
8958465d1eaf81c8b781ba4d764a4f5329f026aa,What are the three measures of bias which are reduced in experiments?,,"RIPA, Neighborhood Metric, WEAT",0.7451254725456238,0.10174405574798584
8985ead714236458a7496075bc15054df0e3234e,What is the performance of the models on the tasks?, well below estimated human agreement,"Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)",0.2533332407474518,0.20369482040405273
89d1687270654979c53d0d0e6a845cdc89414c67,How do they obtain human judgements?, crowdsourced,Using crowdsourcing ,0.5587368011474609,0.8618167638778687
8a0a51382d186e8d92bf7e78277a1d48958758da,How better is gCAS approach compared to other approaches?,,"For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52",0.9697872996330261,0.0490821972489357
8a1d4ed00d31c1f1cb05bc9d5e4f05fe87b0e5a4,Who manually annotated the semantic roles for the set of learner texts?,,Authors,0.9956468939781189,0.23107966780662537
8a5254ca726a2914214a4c0b6b42811a007ecfc6,How much transcribed data is available for for Ainu language?,,Transcribed data is available for duration of 38h 54m 38s for 8 speakers.,0.5161550641059875,0.05640548840165138
8a871b136ccef78391922377f89491c923a77730,What are the baseline state of the art models?,,"Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention",0.8071017861366272,0.06144119054079056
8ad815b29cc32c1861b77de938c7269c9259a064,What languages are represented in the dataset?, english and japanese,"EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO",0.4014604091644287,0.3154429793357849
8c0a0747a970f6ea607ff9b18cfeb738502d9a95,What was the performance of both approaches on their dataset?, % ) results of the i - vector and x - vector systems trained on voxceleb and evaluated on three evaluation sets,ERR of 19.05 with i-vectors and 15.52 with x-vectors,0.08459388464689255,0.31618034839630127
8c8a32592184c88f61fac1eef12c7d233dbec9dc,Are this models usually semi/supervised or unsupervised?,,"Both supervised and unsupervised, depending on the task that needs to be solved.",0.8265207409858704,0.04912120848894119
8cc56fc44136498471754186cfa04056017b4e54,By how much does their system outperform the lexicon-based models?,,"Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . 
Under the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029",0.9779157638549805,0.04644162207841873
8d793bda51a53a4605c1c33e7fd20ba35581a518,what bottlenecks were identified?,,Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System.,0.7978054881095886,0.0915507897734642
8e2b125426d1220691cceaeaf1875f76a6049cbd,By how much do they improve the accuracy of inferences over state-of-the-art methods?,,"ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.
On Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.",0.9154724478721619,-0.00017115846276283264
8e9de181fa7d96df9686d0eb2a5c43841e6400fa,"Is CRWIZ already used for data collection, what are the results?",,"Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.",0.038783203810453415,0.06175189092755318
8ea4bd4c1d8a466da386d16e4844ea932c44a412,What dataset do they use?,,A parallel corpus where the source is an English expression of code and the target is Python code.,0.08253052830696106,0.08914417028427124
8f87215f4709ee1eb9ddcc7900c6c054c970160b,how is quality measured?, accuracy and the macro - f1,Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.,0.06622141599655151,0.7135559320449829
90159e143487505ddc026f879ecd864b7f4f479e,How much of the ASR grapheme set is shared between languages?,,Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script.,0.559126079082489,-0.03976153954863548
90bc60320584ebba11af980ed92a309f0c1b5507,How do they enrich the positional embedding with length information,,They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).,0.3707548975944519,0.0156675074249506
9299fe72f19c1974564ea60278e03a423eb335dc,What was the weakness in Hassan et al's evaluation design?,,"MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set
",0.5117928385734558,0.031120415776968002
9349acbfce95cb5d6b4d09ac626b55a9cb90e55e,What are the citation intent labels in the datasets?,,"Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.",0.8402664065361023,0.0433996245265007
93b299acfb6fad104b9ebf4d0585d42de4047051,Which datasets are used?," english, spanish, french, dutch, russian and turkish","ABSA SemEval 2014-2016 datasets
Yelp Academic Dataset
Wikipedia dumps",0.15441752970218658,0.14709383249282837
9447ec36e397853c04dcb8f67492ca9f944dbd4b,What is the dataset used as input to the Word2Vec algorithm?, a dump of the italian wikipedia,Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words,0.03368232026696205,0.6263319253921509
944d5dbe0cfc64bf41ea36c11b1d378c408d40b8,Which of the two speech recognition models works better overall on CN-Celeb?, i - vector and x - vector,x-vector,0.254868745803833,0.8159942626953125
94bee0c58976b58b4fef9e0adf6856fe917232e5,How much bigger is Switchboard-2000 than Switchboard-300 database?,,Switchboard-2000 contains 1700 more hours of speech data.,0.9285499453544617,-0.014678806066513062
94e0cf44345800ef46a8c7d52902f074a1139e1a,What web and user-generated NER datasets are used for the analysis?,,"MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC",0.14666473865509033,0.013064324855804443
954c4756e293fd5c26dc50dc74f505cc94b3f8cc,What are dilated convolutions?, skip some input values,Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.,0.6626221537590027,0.3489258885383606
9555aa8de322396a16a07a5423e6a79dcd76816a,By how much does their model outperform both the state-of-the-art systems?,,w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%,0.9114621877670288,0.042725205421447754
955ca31999309685c1daa5cb03867971ca99ec52,What datasets are used to evaluate the model?, wn18 and fb15k,"WN18, FB15k",0.6430220007896423,0.9366908669471741
957bda6b421ef7d2839c3cec083404ac77721f14,What stylistic features are used to detect drunk texts?,,"LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) 
 and Sentiment Ratio",0.6181055903434753,0.046308085322380066
96be67b1729c3a91ddf0ec7d6a80f2aa75e30a30,What language do the agents talk in?,,English,0.953309953212738,0.28494974970817566
96c09ece36a992762860cde4c110f1653c110d96,What was the result of the highest performing system?,,"For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.
For task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2",0.8647336363792419,-0.01324462704360485
973f6284664675654cc9881745880a0e88f3280e,What proficiency indicators are used to the score the utterances?,,"6 indicators:
- lexical richness
- pronunciation and fluency
- syntactical correctness
- fulfillment of delivery
- coherence and cohesion
- communicative, descriptive, narrative skills",0.18167872726917267,0.06581206619739532
9776156fc93daa36f4613df591e2b49827d25ad2,"By how much, the proposed method improves BiDAF and DCN on SQuAD dataset?",,"In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively.",0.9881904125213623,0.04270074516534805
98515bd97e4fae6bfce2d164659cd75e87a9fc89,What is the source of the user interaction data? ,,Sociability from ego-network on Twitter,0.1461692899465561,0.11773383617401123
98785bf06e60fcf0a6fe8921edab6190d0c2cec1,How do they determine which words are informative?,,Informative are those that will not be suppressed by regularization performed.,0.5044309496879578,0.11102758347988129
98eb245c727c0bd050d7686d133fa7cd9d25a0fb,Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?,,BLEU scores,0.1834956854581833,0.14647558331489563
993b896771c31f3478f28112a7335e7be9d03f21,What novel class of recurrent-like networks is proposed?,,"A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state.",0.702785074710846,0.04579886421561241
999b20dc14cb3d389d9e3ba5466bc3869d2d6190,What is the latest paper covered by this survey?,,Kim et al. (2019),0.4539705812931061,0.19129791855812073
99c50d51a428db09edaca0d07f4dab0503af1b94,What kind of Youtube video transcripts did they use?,,"youtube video transcripts on news covering different topics like technology, human rights, terrorism and politics",0.11729194968938828,0.06973117589950562
9a596bd3a1b504601d49c2bec92d1592d7635042,What is the performance of their model?,,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220.,0.3048860728740692,0.010929780080914497
9a65cfff4d99e4f9546c72dece2520cae6231810,What is the performance of proposed model on entire DROP dataset?, the best overall,"The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev",0.1769065409898758,0.12841396033763885
9aa52b898d029af615b95b18b79078e9bed3d766,How faster is training and decoding compared to former models?,,"Proposed vs best baseline:
Decoding: 8541 vs 8532 tokens/sec
Training: 8h vs 8h",0.8134002685546875,0.06653116643428802
9ab43f941c11a4b09a0e4aea61b4a5b4612e7933,What approach did previous models use for multi-span questions?,,Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span,0.15019568800926208,0.05182488262653351
9adcc8c4a10fa0d58f235b740d8d495ee622d596,How many additional task-specific layers are introduced?,,2 for the ADE dataset and 3 for the CoNLL04 dataset,0.9958393573760986,0.03536754474043846
9ae084e76095194135cd602b2cdb5fb53f2935c1,What metrics are used for evaluation?,,word error rate,0.12356962263584137,0.033466361463069916
9bcc1df7ad103c7a21d69761c452ad3cd2951bda,On which task does do model do worst?, gender,Gender prediction task,0.15429019927978516,0.6551563739776611
9c4a4dfa7b0b977173e76e2d2f08fa984af86f0e,How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?, outperforms all existing models,"Full Testing Set accuracy: 84.02
Cleaned Testing Set accuracy: 93.48",0.2879461646080017,0.26403024792671204
9c68d6d5451395199ca08757157fbfea27f00f69,Which OpenIE systems were used?,,OpenIE4 and MiniIE,0.3433489203453064,0.1493542492389679
9d578ddccc27dd849244d632dd0f6bf27348ad81,What are the results?,,"Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. 
Using a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.",0.12240515649318695,0.058286115527153015
9d5df9022cc9eb04b9f5c5a9d8308a332ebdf50c,What is the new labeling strategy?, two - stage,They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations,0.1329088658094406,0.28088366985321045
9d9b11f86a96c6d3dd862453bf240d6e018e75af,How does counterfactual data augmentation aim to tackle bias?, by augmenting our dataset with a copy of every dialogue with gendered words swapped using the gendered word pair list provided by bibref21,The training dataset is augmented by swapping all gendered words by their other gender counterparts,0.1201612651348114,0.7568497061729431
9e04730907ad728d62049f49ac828acb4e0a1a2a,What were their performance results?,,"On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%",0.882486879825592,-0.027461033314466476
9ebb2adf92a0f8db99efddcade02a20a219ca7d9,How is the proficiency score calculated?,,"They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.",0.609466016292572,0.0057534584775567055
9ef182b61461d0d8b6feb1d6174796ccde290a15,Do they annotate their own dataset or use an existing one?,,Use an existing one,0.8211841583251953,0.07639410346746445
9fe4a2a5b9e5cf29310ab428922cc8e7b2fc1d11,What are state of the art methods MMM is compared to?,What are state of the art methods MMM is compared to?float selected : table 3 : accuracy on the dream dataset. performance marked by? is reported by ( sun et al. 2019 ). numbers in parentheses indicate the accuracy increased by mmm compared to the baselines,"FTLM++, BERT-large, XLNet",0.24635779857635498,0.08107385039329529
a02696d4ab728ddd591f84a352df9375faf7d1b4,How large is the Dialog State Tracking Dataset?,,"1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs",0.9004927277565002,0.13093821704387665
a09633584df1e4b9577876f35e38b37fdd83fa63,"How is human evaluation performed, what was the criteria?", plausibility and content richness,Through Amazon MTurk annotators to determine plausibility and content richness of the response,0.21613608300685883,0.473515123128891
a1064307a19cd7add32163a70b6623278a557946,How many uniue words are in the dataset?,,908456 unique words are available in collected corpus.,0.9636049866676331,0.07313452661037445
a1557ec0f3deb1e4cd1e68f4880dcecda55656dd,Which geographical regions correlate to the trend?," northeast, west and south","Northeast U.S., West U.S. and South U.S.",0.2111758291721344,0.8195436596870422
a24a7a460fd5e60d71a7e787401c68caa4702df6,What monolingual word representations are used?,,"AraVec for Arabic, FastText for French, and Word2vec Google News for English.",0.9642292857170105,0.0956755056977272
a25c1883f0a99d2b6471fed48c5121baccbbae82,What performance does the Entity-GCN get on WIKIHOP?,,"During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models",0.6851372122764587,0.05435844138264656
a2be2bd84e5ae85de2ab9968147b3d49c84dfb7f,What genres are covered?,,"genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement",0.8302373290061951,0.1151217371225357
a379c380ac9f67f824506951444c873713405eed,What are the baselines?, scores of top 3 participants on valid and test datasets,"CNN, LSTM, BERT",0.24748730659484863,0.11797776073217392
a381ba83a08148ce0324b48b8ff35128e66f580a,what models did they compare to?,,"High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM ",0.4607508182525635,0.11644503474235535
a3d83c2a1b98060d609e7ff63e00112d36ce2607,How many sentence transformations on average are available per unique sentence in dataset?,,27.41 transformation on average of single seed sentence is available in dataset.,0.5969302654266357,0.02774469554424286
a3f108f60143d13fe38d911b1cc3b17bdffde3bd,what was their system's f1 performance?,"what was their system's f1 performance?float selected : table 2 : f1 results3 ; the approach we have developed establishes a new state of the art for classifying hate speech, outperforming previous results by as much as 12 f1 points","Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.",0.14089550077915192,0.5866201519966125
a48c6d968707bd79469527493a72bfb4ef217007,Which training dataset allowed for the best generalization to benchmark sets?,,MultiNLI,0.7515602707862854,0.2701606750488281
a4d8fdcaa8adf99bdd1d7224f1a85c610659a9d3,"When they say ""comparable performance"", how much of a performance drop do these new embeddings result in?",,"Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.",0.7529723644256592,0.007227011024951935
a4fe5d182ddee24e5bbf222d6d6996b3925060c8,which datasets were used in evaluation?,,"CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0",0.6867678165435791,0.07119486480951309
a4ff1b91643e0c8a0d4cc1502d25ca85995cf428,Which two datasets does the resource come from?, two different surveys,two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor,0.29666516184806824,0.3681817352771759
a516b37ad9d977cb9d4da3897f942c1c494405fe,Which models do they try out?,,"DocQA, SAN, QANet, ASReader, LM, Random Guess",0.6254135966300964,0.11129289120435715
a56fbe90d5d349336f94ef034ba0d46450525d19,What DCGs are used?,,Author's own DCG rules are defined from scratch.,0.36771973967552185,0.025704819709062576
a5b67470a1c4779877f0d8b7724879bbb0a3b313,what metrics are used in evaluation?, micro - averaged inlineform0,micro-averaged F1,0.5269143581390381,0.6227043867111206
a71ebd8dc907d470f6bd3829fa949b15b29a0631,how did they ask if a tweet was racist?,,"if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.",0.766510009765625,0.05228229984641075
a7510ec34eaec2c7ac2869962b69cc41031221e5,What was their F1 score on the Bengali NER corpus?, over 5 points,52.0%,0.3562319576740265,0.2967539131641388
a7adb63db5066d39fdf2882d8a7ffefbb6b622f0,what was the baseline?,,There is no baseline.,0.8882684707641602,0.08728748559951782
a81941f933907e4eb848f8aa896c78c1157bff20,"Can the model add new relations to the knowledge graph, or just new entities?",,The model does not add new relations to the knowledge graph.,0.9245525598526001,0.015034038573503494
a891039441e008f1fd0a227dbed003f76c140737,What MC abbreviate for?,,machine comprehension,0.9343255162239075,0.18535223603248596
a978a1ee73547ff3a80c66e6db3e6c3d3b6512f4,How much improvement does their method get over the fine tuning baseline?,,"0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.",0.749431848526001,0.008337900042533875
a979749e59e6e300a453d8a8b1627f97101799de,Why does the model improve in monolingual spaces as well? ,,because word pair similarity increases if the two words translate to similar parts of the cross-lingual embedding space,0.1286303550004959,-0.010863039642572403
a996b6aee9be88a3db3f4127f9f77a18ed10caba,What's the precision of the system?,,"0.8320 on semantic typing, 0.7194 on entity matching",0.49868929386138916,0.026507938280701637
aa54e12ff71c25b7cff1e44783d07806e89f8e54,What is an example of a health-related tweet?,,"The health benefits of alcohol consumption are more limited than previously thought, researchers say",0.565240740776062,0.0034631602466106415
aacb0b97aed6fc6a8b471b8c2e5c4ddb60988bf5,How many attention layers are there in their model?,,one,0.9805077910423279,0.4496035575866699
aaed6e30cf16727df0075b364873df2a4ec7605b,What is WNGT 2019 shared task?,,efficiency task aimed  at reducing the number of parameters while minimizing drop in performance,0.4669053256511688,0.04919378459453583
ab9453fa2b927c97b60b06aeda944ac5c1bfef1e,Which datasets are used in experiments?,,Sequence Copy Task and WMT'17,0.17806167900562286,0.0041053518652915955
ac148fb921cce9c8e7b559bba36e54b63ef86350,What dataset they use for evaluation?, bibref7,The same 2K set from Gigaword used in BIBREF7,0.23146218061447144,0.5856055021286011
acc8d9918d19c212ec256181e51292f2957b37d7,What are the differences with previous applications of neural networks for this task?,What are the differences with previous applications of neural networks for this task?one common point in all the approaches yet has been the use of only textual features available in the dataset,This approach considers related images,0.07569227367639542,0.2586647868156433
ace60950ccd6076bf13e12ee2717e50bc038a175,How are the two different models trained?, vary the number of word - pieces from each article that are used in training,They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.,0.11767687648534775,0.511885941028595
ad0a7fe75db5553652cd25555c6980f497e08113,How does the model compute the likelihood of executing to the correction semantic denotation?,,By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x.,0.8680468797683716,0.07479161769151688
ad1f230f10235413d1fe501e414358245b415476,Which models were compared?," esim bibref19, which includes cross - sentence attention, and kim bibref2, which has cross - sentence attention and utilizes external knowledge. we also selected two model involving a pre - trained language model, namely esim + elmo bibref20 and bert bibref0","BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT",0.08790815621614456,0.38303342461586
ad5898fa0063c8a943452f79df2f55a5531035c7,Which embeddings do they detect biases in?,,Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset,0.6497824788093567,0.06854462623596191
ad67ca844c63bf8ac9fdd0fa5f58c5a438f16211,Which unlabeled data do they pretrain with?,,1000 hours of WSJ audio data,0.8589149713516235,-0.00032520294189453125
aeda22ae760de7f5c0212dad048e4984cd613162,What annotations are available in the dataset?,,"For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)",0.9475730061531067,0.013636221177875996
af75ad21dda25ec72311c2be4589efed9df2f482,How much does this system outperform prior work?,,"The system outperforms by 27.7% the LSTM model, 38.5% the RL-SPINN model and 41.6% the Gumbel Tree-LSTM",0.786105215549469,0.02926776558160782
b0376a7f67f1568a7926eff8ff557a93f434a253,How big is the performance difference between this method and the baseline?,,"Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.",0.6511709690093994,0.01936884969472885
b13d0e463d5eb6028cdaa0c36ac7de3b76b5e933,What datasets are used to evaluate the model?, wn18 and fb15k,WN18 and FB15k,0.6451854109764099,1.0
b2254f9dd0e416ee37b577cef75ffa36cbcb8293,How many domains of ontologies do they gather data from?,,"5 domains: software, stuff, african wildlife, healthcare, datatypes",0.9281512498855591,0.053704120218753815
b27f7993b1fe7804c5660d1a33655e424cea8d10,What is the source of the visual data? ,,Profile pictures from the Twitter users' profiles.,0.15100592374801636,0.1413697898387909
b3857a590fd667ecc282f66d771e5b2773ce9632,What is a string kernel?, a way of using information at the character level by measuring the similarity of strings through character n - grams,String kernel is a technique that uses character n-grams to measure the similarity of strings,0.12679423391819,0.7875440120697021
b39f2249a1489a2cef74155496511cc5d1b2a73d,What is the accuracy reported by state-of-the-art methods?,,"Answer with content missing: (Table 1)
Previous state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)",0.6052335500717163,0.07787546515464783
b3a09d2e3156c51bd5fdc110a2a00a67bb8c0e42,what are the off-the-shelf systems discussed in the paper?,,"Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier.",0.976412832736969,0.07786571234464645
b3fcab006a9e51a0178a1f64d1d084a895bd8d5c,what are the state of the art methods?,,"S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al.",0.578906238079071,0.07616016268730164
b43fa27270eeba3e80ff2a03754628b5459875d6,What domains are present in the data?,,"Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather",0.8826842308044434,0.08288782835006714
b540cd4fe9dc4394f64d5b76b0eaa4d9e30fb728,Could you tell me more about the metrics used for performance evaluation?," micro - average precision, recall, and f1 - score","BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy",0.1642637699842453,0.4359590411186218
b5a2b03cfc5a64ad4542773d38372fffc6d3eac7,How are EAC evaluated?,,"Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement.",0.8383834362030029,0.006640469655394554
b5bfa6effdeae8ee864d7d11bc5f3e1766171c2d,Which regions of the United States do they consider?, lower 48 states,all regions except those that are colored black,0.4601615071296692,0.2672261893749237
b5e883b15e63029eb07d6ff42df703a64613a18a,How were topics of interest about DDEO identified?, topic modeling approach,using topic modeling model Latent Dirichlet Allocation (LDA),0.12016291916370392,0.8020528554916382
b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1,What were the non-neural baselines used for the task?,,The Lemming model in BIBREF17,0.8938414454460144,0.12230713665485382
b6b5f92a1d9fa623b25c70c1ac67d59d84d9eec8,By how much do they outpeform previous results on the word discrimination task?,,Their best average precision tops previous best result by 0.202,0.9504774212837219,0.04756275564432144
b6f5860fc4a9a763ddc5edaf6d8df0eb52125c9e,Which 5 languages appear most frequently in AA paper titles?,,"English, Chinese, French, Japanese and Arabic",0.9547340869903564,-0.004277737811207771
b6fb72437e3779b0e523b9710e36b966c23a2a40,How many rules had to be defined?,,"WikiSQL - 2 rules (SELECT, WHERE)
SimpleQuestions - 1 rule
SequentialQA - 3 rules (SELECT, WHERE, COPY)",0.9657425284385681,-0.011849703267216682
b7708cbb50085eb41e306bd2248f1515a5ebada8,How do they get the formal languages?,,These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.,0.15974698960781097,-0.001995914615690708
b8dea4a98b4da4ef1b9c98a211210e31d6630cf3,What is specific to gCAS cell?,,"It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner.",0.5212733149528503,0.0978156104683876
b8f711179a468fec9a0d8a961fb0f51894af4b31,What kind of neural network architecture do they use?,,CNN,0.4799707233905792,0.2444264441728592
b9025c39838ccc2a79c545bec4a676f7cc4600eb,Why do they think this task is hard?  What is the baseline performance?,,"1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.
2. Macro F1 = 14.6 (MLR, length 96 snippet)
Weighted F1 = 31.1 (LSTM, length 128 snippet)",0.24973706901073456,0.05786603316664696
b9f2a30f5ef664ff845d860cf4bfc2afb0a46e5a,How do they gather human judgements for similarity between relations?, a subset of wikidata bibref8,By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4,0.09539509564638138,0.5550143718719482
ba1da61db264599963e340010b777a1723ffeb4c,What does recurrent deep stacking network do?, stacks and concatenates the outputs of previous frames into the input features of the current frame,Stacks and joins outputs of previous frames with inputs of the current frame,0.6496394276618958,0.8971679210662842
ba56afe426906c4cfc414bca4c66ceb4a0a68121,What are the datasets used for the task?,,"Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)",0.12800167500972748,-0.019833553582429886
ba6422e22297c7eb0baa381225a2f146b9621791,What is the performance difference between proposed method and state-of-the-arts on these datasets?,"What is the performance difference between proposed method and state-of-the-arts on these datasets?. comparing with cmlm bibref8 with 10 iterations of refinement, which is a contemporaneous work that achieves state - of - the - art translation performance, flowseq obtains competitive performance on both wmt2014 and wmt2016 corpora, with only slight degradation in translation quality",Difference is around 1 BLEU score lower on average than state of the art methods.,0.23630084097385406,0.3546891212463379
bab8c69e183bae6e30fc362009db9b46e720225e,What are two strong baseline methods authors refer to?,,Marcheggiani and Titov (2017) and Cai et al. (2018),0.9510754346847534,0.18249046802520752
bb4de896c0fa4bf3c8c43137255a4895f52abeef,What is the baseline model?,,a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model,0.19133634865283966,0.022835049778223038
bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2,How do the various social phenomena examined manifest in different types of communities?, substantially higher rates of monthly user retention,"Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
",0.06841714680194855,0.6811290979385376
bbdb2942dc6de3d384e3a1b705af996a5341031b,What type of model are the ELMo representations used in?,,A bi-LSTM with max-pooling on top of it,0.08427409827709198,0.08600901067256927
bc9c31b3ce8126d1d148b1025c66f270581fde10,What datasets are used to evaluate this approach?, data statistics," Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ",0.1485113501548767,0.10738678276538849
bd1a3c651ca2b27f283d3f36df507ed4eb24c2b0,Is it a neural model? How is it trained?,,"No, it is a probabilistic model trained by finding feature weights through gradient ascent",0.7225834131240845,0.04768664762377739
bd5379047c2cf090bea838c67b6ed44773bcd56f,Which experiments are perfomed?,,They used BERT-based models to detect subjective language in the WNC corpus,0.7244919538497925,0.03126739710569382
bd99aba3309da96e96eab3e0f4c4c8c70b51980a,Which existing models does this approach outperform?,,"RNN-context, SRB, CopyNet, RNN-distract, DRGD",0.711772084236145,0.05090063437819481
bdc93ac1b8643617c966e91d09c01766f7503872,What is the size of the second dataset?,,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,0.5111241936683655,0.17945659160614014
bdd8368debcb1bdad14c454aaf96695ac5186b09,How is the intensity of the PTSD established?,,"Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.",0.920602560043335,0.041609540581703186
bde6fa2057fa21b38a91eeb2bb6a3ae7fb3a2c62,what state of the accuracy did they obtain?,,51.5,0.5040487051010132,0.14524215459823608
bfce2afe7a4b71f9127d4f9ef479a0bfb16eaf76,What human evaluation metrics were used in the paper?,"What human evaluation metrics were used in the paper?for the human evaluation, we follow the standard approach in evaluating machine translation systems bibref23, as used for question generation by bibref9. we asked three workers to rate 300 generated questions between 1 ( poor ) and 5 ( good ) on two separate criteria : the fluency of the language used, and the relevance of the question to the context document and answer",rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context,0.09341532737016678,0.5812851190567017
c000a43aff3cb0ad1cee5379f9388531b5521e9a,how are the bidirectional lms obtained?, concatenate the forward and backward lm embeddings,"They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs.",0.12459084391593933,0.6154187917709351
c0035fb1c2b3de15146a7ce186ccd2e366fb4da2,By how much of MGNC-CNN out perform the baselines?,,"In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. 
In case of Irony the difference is about 2.0. 
",0.9877788424491882,0.034581176936626434
c029deb7f99756d2669abad0a349d917428e9c12,How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?,,3%,0.5891627669334412,0.27855485677719116
c034f38a570d40360c3551a6469486044585c63c,How better is proposed method than baselines perpexity wise?,"How better is proposed method than baselines perpexity wise?table tabref34 gives the perplexity scores obtained by the three models on the two validation sets and the test set. as shown in the table, meed achieves the lowest perplexity score on all three sets. we also conducted t - test on the perplexity obtained, and results show significant improvements",Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.,0.11616344749927521,0.5684512853622437
c13fe4064df0cfebd0538f29cb13e917fc5c3be0,What is the network architecture?, multi - task bi - directional recurrent neural network where junior tasks in the hierarchy are supervised at lower layers. this architecture builds upon sogaard2016deep,"The network architecture has a multi-task Bi-Directional Recurrent Neural Network, with an unsupervised sequence labeling task and a low-dimensional embedding layer between tasks. There is a hidden layer after each successive task with skip connections to the senior supervised layers.",0.054796814918518066,0.7198822498321533
c165ea43256d7ee1b1fb6f5c0c8af5f7b585e60d,How much is proposed model better than baselines in performed experiments?,,"most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)
GDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)
GDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)",0.9681466221809387,-0.003164682537317276
c1c611409b5659a1fd4a870b6cc41f042e2e9889,What evaluations did the authors use on their system?,,"BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.",0.29389017820358276,0.04648374021053314
c1f4d632da78714308dc502fe4e7b16ea6f76f81,Which language-pair had the better performance?, french,French-English,0.16012774407863617,0.8732277750968933
c33d0bc5484c38de0119c8738ffa985d1bd64424,Do the images have multilingual annotations or monolingual ones?,,monolingual,0.8160480856895447,0.19813929498195648
c348a8c06e20d5dee07443e962b763073f490079,What two components are included in their proposed framework?,,evidence extraction and answer synthesis,0.22427192330360413,0.07279251515865326
c359ab8ebef6f60c5a38f5244e8c18d85e92761d,How many paraphrases are generated per question?,,"10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans",0.9598642587661743,0.06100825220346451
c45feda62f23245f53e855706e2d8ea733b7fd03,Which translation system do they use to translate to English?,,Attention-based translation model with convolution sequence to sequence model,0.22139960527420044,0.08070993423461914
c47e87efab11f661993a14cf2d7506be641375e4,How does new evaluation metric considers critical informative entities?,,Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities,0.9149587750434875,0.006987310945987701
c4a6b727769328333bb48d59d3fc4036a084875d,What baseline did they compare Entity-GCN to?,,"Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN",0.26093682646751404,0.11910717189311981
c4b5cc2988a2b91534394a3a0665b0c769b598bb,How do they define local variance?, the reciprocal of its variance,The reciprocal of the variance of the attention distribution,0.21819399297237396,0.5235741138458252
c4c9c7900a0480743acc7599efb359bc81cf3a4d,How much is pre-training loss increased in Low/Medium/Hard level of pruning?,,"The increase is linearly from lowest on average 2.0 , medium around 3.5, and the largest is 6.0",0.968993604183197,-0.06389158964157104
c515269b37cc186f6f82ab9ada5d9ca176335ded,What evidence do they present that the model attends to shallow context clues?,,Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues,0.27525725960731506,0.10110251605510712
c554a453b6b99d8b59e4ef1511b1b506ff6e5aa4,how was the dataset built?,,"Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable"" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is “yes"" or “no""",0.9003820419311523,-0.01534152589738369
c69f4df4943a2ca4c10933683a02b179a5e76f64,What approach performs better in experiments global latent or sequence of fine-grained latent variables?, sequential,"PPL: SVT
Diversity: GVT
Embeddings Similarity: SVT
Human Evaluation: SVT",0.2792873978614807,0.026612306013703346
c77d6061d260f627f2a29a63718243bab5a6ed5a,How different is the dataset size of source and target?,,the training dataset is large while the target dataset is usually much smaller,0.27132275700569153,0.05869968235492706
c7d99e66c4ab555fe3d616b15a5048f3fe1f3f0e,What is an example of a computational social science NLP task?,,Visualization of State of the union addresses,0.6737347841262817,0.10091562569141388
c82e945b43b2e61c8ea567727e239662309e9508,What additional features are proposed for future work?,"What additional features are proposed for future work?our current feature set for training a machine learning classifier is relatively small, consisting of paragraph domain scores, bag - of - words, length of stay, and number of previous admissions, but we intend to factor in many additional features that extend beyond the scope of the present study. these include a deeper analysis of clinical narratives in ehrs",distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort,0.13457906246185303,0.328108549118042
ca26cfcc755f9d0641db0e4d88b4109b903dbb26,How better are results compared to baseline models?, similar performance,F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.,0.14257760345935822,0.30188190937042236
ca7e71131219252d1fab69865804b8f89a2c0a8f,How does this compare to traditional calibration methods like Platt Scaling?,,No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.,0.40007108449935913,-0.1473211795091629
cacb83e15e160d700db93c3f67c79a11281d20c5,Does this paper propose a new task that others can try to improve performance on?,,"No, there has been previous work on recognizing social norm violation.",0.05204736068844795,0.07772450149059296
caf9819be516d2c5a7bfafc80882b07517752dfa,Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?,,They evaluate quantitatively.,0.8411660194396973,0.025191601365804672
cb78e280e3340b786e81636431834b75824568c3,How many emotions do they look at?,,9,0.28930312395095825,0.34639349579811096
cb8a6f5c29715619a137e21b54b29e9dd48dad7d,"What is an ""answer style""?",,well-formed sentences vs concise answers,0.6032989621162415,0.07681811600923538
cbbcafffda7107358fa5bf02409a01e17ee56bfd,Was any variation in results observed based on language typology?," it only encodes at most  more information than some trivial baseline knowledge ( a type - level representation ). this indicates that the task of pos labeling ( word - level pos tagging ) is not an ideal task for contemplating the syntactic understanding of contextual word embeddings. ; we know  can generate text in many languages, here we assess how much does it actually know about syntax in those languages. and how much more does it know than simple type - level baselines. tab : results - full presents this results, showing how much information , fasttext and onehot embeddings encode about pos tagging . we see that — in all analysed languages — type level embeddings can already capture most of the uncertainty in pos tagging. we also see that bert only shares a small amount of extra information with the task, having small ( or even negative ) gains in all languages. ; finally, when put into perspective, multilingual  ' s representations do not seem to encode much more information about syntax than a trivial baseline",It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information.,0.012054421938955784,0.5474556684494019
cc5d3903913fa2e841f900372ec74b0efd5e0c71,Which sentiment analysis tasks are addressed?,,12 binary-class classification and multi-class classification of reviews based on rating,0.19255001842975616,0.023935765027999878
cc5d8e12f6aecf6a5f305e2f8b3a0c67f49801a9,What percentage fewer errors did professional translations make?,,36%,0.9390484094619751,0.2286577820777893
cd8de03eac49fd79b9d4c07b1b41a165197e1adb,How are multimodal representations combined?,,The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.,0.7865608334541321,0.06795907020568848
cdf1bf4b202576c39e063921f6b63dc9e4d6b1ff,What metric is used to measure performance?,,"Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks",0.3998304009437561,0.04801899194717407
ce807a42370bfca10fa322d6fa772e4a58a8dca1,What are the four forums the data comes from?,,"Darkode,  Hack Forums, Blackhat and Nulled.",0.9643705487251282,0.13425622880458832
cebf3e07057339047326cb2f8863ee633a62f49f,In which languages did the approach outperform the reported results?,,"Arabic, German, Portuguese, Russian, Swedish",0.8083164095878601,0.03339097276329994
cf63a4f9fe0f71779cf5a014807ae4528279c25a,How does the semi-automatic construction process work?,,Automatic transcription of 5000 tokens through sequential neural models trained on the annotated part of the corpus,0.6077522039413452,0.08577048778533936
cf93a209c8001ffb4ef505d306b6ced5936c6b63,From when are many VQA datasets collected?,,late 2014,0.8100338578224182,0.2947567105293274
cfbccb51f0f8f8f125b40168ed66384e2a09762b,How are discourse embeddings analyzed?, t - sne clustering bibref20,They perform t-SNE clustering to analyze discourse embeddings,0.021894287317991257,0.4062538146972656
cfffc94518d64cb3c8789395707e4336676e0345,What approaches without reinforcement learning have been tried?,,"classification, regression, neural methods",0.9557844996452332,0.014356069266796112
d028dcef22cdf0e86f62455d083581d025db1955,What are the strong baselines you have?,,optimize single task with no synthetic data,0.905311107635498,0.002923766151070595
d05d667822cb49cefd03c24a97721f1fe9dc0f4c,How did they get relations between mentions?,,"Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain.",0.6765204668045044,-0.05456002056598663
d0bc782961567dc1dd7e074b621a6d6be44bb5b4,How big is seed lexicon used for training?,,30 words,0.8976349234580994,0.27408045530319214
d0c79f4a5d5c45fe673d9fcb3cd0b7dd65df7636,What are new best results on standard benchmark?,,"New best results of accuracy (P@1) on Vecmap:
Ours-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43",0.7019129991531372,0.054875798523426056
d0f831c97d345a5b8149a9d51bf321f844518434,What labels are in the dataset?,,binary label of stress or not stress,0.8886075019836426,0.003597487695515156
d2fbf34cf4b5b1fd82394124728b03003884409c,Who was the top-scoring team?,,IDEA,0.3607550859451294,0.3686330318450928
d3092f78bdbe7e741932e3ddf997e8db42fa044c,What experimental evaluation is used?,,root mean square error between the actual and the predicted price of Bitcoin for every minute,0.43984535336494446,0.06656944751739502
d3bcfcea00dec99fa26283cdd74ba565bc907632,How big is dataset for this challenge?,,"133,287 images",0.12600232660770416,0.15895618498325348
d484a71e23d128f146182dccc30001df35cdf93f,How much is proposed model better in perplexity and BLEU score than typical UMT models?,,"Perplexity of the best model is 65.58 compared to best baseline 105.79.
Bleu of the best model is 6.57 compared to best baseline 5.50.",0.9750887155532837,0.03461086377501488
d5256d684b5f1b1ec648d996c358e66fe51f4904,what is the practical application for this paper?, linguistic analysis,Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools.,0.100183866918087,0.5066361427307129
d5498d16e8350c9785782b57b1e5a82212dbdaad,How accurate is model trained on text exclusively?,,Relative error is less than 5%,0.9290770888328552,-0.020757203921675682
d5fa26a2b7506733f3fa0973e2fe3fc1bbd1a12d,"How are possible sentence transformations represented in dataset, as new sentences?", 150 seed sentences,"Yes, as new sentences.",0.31650274991989136,0.30006682872772217
d60a3887a0d434abc0861637bbcd9ad0c596caf4,What semantic rules are proposed?, ten,rules that compute polarity of words after POS tagging or parsing steps,0.3107859790325165,0.06397596746683121
d653d994ef914d76c7d4011c0eb7873610ad795f,How were breast cancer related posts compiled from the Twitter streaming API?,,"By using  keywords `breast' AND `cancer' in tweet collecting process. 
",0.1314668208360672,0.10481686145067215
d6e2b276390bdc957dfa7e878de80cee1f41fbca,What models other than standalone BERT is new model compared to?,,Only Bert base and Bert large are compared to proposed approach.,0.9666609168052673,0.03564438968896866
d6e8b32048ff83c052e978ff3b8f1cb097377786,"How are customer satisfaction, customer frustration and overall problem resolution data collected?",,By annotators on Amazon Mechanical Turk.,0.5106433629989624,0.11917804181575775
d70ba6053e245ee4179c26a5dabcad37561c6af0,Which datasets did they experiment on?,,ConciergeQA and AmazonQA,0.3671800494194031,0.19412672519683838
d71cb7f3aa585e256ca14eebdc358edfc3a9539c,Which models achieve state-of-the-art performances?,,"CELEX (Dutch and English) - SVM-HMM
Festival, E-Hitz and OpenLexique - Liang hyphenation
IIT-Guwahat - Entropy CRF",0.43724724650382996,0.12889088690280914
d77c9ede2727c28e0b5a240b2521fd49a19442e0,What's the input representation of OpenIE tuples into the model?, word embedding,word embeddings,0.13608203828334808,0.9776166677474976
d79d897f94e666d5a6fcda3b0c7e807c8fad109e,What result from experiments suggest that natural language based agents are more robust?,,Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances,0.19387321174144745,0.11094291508197784
d7d611f622552142723e064f330d071f985e805c,How many utterances are in the corpus?,,Total number of utterances available is: 70607 (37344 ENG + 33263 GER),0.9795781970024109,0.04609785974025726
d824f837d8bc17f399e9b8ce8b30795944df0d51,How do they show their model discovers underlying syntactic structure?," to avoid inter - word attention connection, and use the hidden states of space ( separator ) tokens to summarize previous information",By visualizing syntactic distance estimated by the parsing network,0.049158550798892975,0.5112007856369019
d8de12f5eff64d0e9c9e88f6ebdabc4cdf042c22,How much gain does the model achieve with pretraining MVCNN?,,0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj,0.9902410507202148,0.13857299089431763
da845a2a930fd6a3267950bec5928205b6c6e8e8,How was speed measured?,"How was speed measured?in terms of speed, our system was able to lemmatize 7. 4 million words on a personal laptop in almost 2 minutes",how long it takes the system to lemmatize a set number of words,0.05392903462052345,0.49786579608917236
da8bda963f179f5517a864943dc0ee71249ee1ce,How many layers does their system have?,,4 layers,0.9606412649154663,0.15296688675880432
dac2591f19f5bbac3d4a7fa038ff7aa09f6f0d96,what are the three methods presented in the paper?,,"Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.",0.8923810720443726,0.14042088389396667
dafa760e1466e9eaa73ad8cb39b229abd5babbda,How large is the dataset they generate?,,4.756 million sentences,0.5798213481903076,0.1421494483947754
dbdf13cb4faa1785bdee90734f6c16380459520b,What cluster identification method is used in this paper?,,"A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18",0.8314269781112671,0.058214299380779266
dbfce07613e6d0d7412165e14438d5f92ad4b004,What affective-based features are used?, different emotion models,"affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count",0.29085585474967957,0.616227388381958
dc69256bdfe76fa30ce4404b697f1bedfd6125fe,What are the languages used to test the model?," hindi, english, and german","Hindi, English and German (German task won)",0.5704472064971924,0.6937756538391113
dcb18516369c3cf9838e83168357aed6643ae1b8,Which retrieval system was used for baselines?, retrieved context,The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.,0.3216758072376251,0.21668395400047302
dd155f01f6f4a14f9d25afc97504aefdc6d29c13,What aspects have been compared between various language models?," word - level perplexity, r @ 3 in next - word prediction, latency ( ms / q ), and energy usage","Quality measures using perplexity and recall, and performance measured using latency and energy usage. ",0.3994531035423279,0.48969513177871704
dd5c9a370652f6550b4fd13e2ac317eaf90973a8,How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]?,,0.9098 correlation,0.2115418016910553,0.02881304919719696
de12e059088e4800d7d89e4214a3997994dbc0d9,What are the baseline systems that are compared against?,,"The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM",0.8787127733230591,0.008441034704446793
df2839dbd68ed9d5d186e6c148fa42fce60de64f,How big is the provided treebank?,,"1448 sentences more than the dataset from Bhat et al., 2017",0.9897807836532593,0.1492975354194641
df79d04cc10a01d433bb558d5f8a51bfad29f46b,Which languages do they test on?,,"Answer with content missing: (Applications section) We use Wikipedia articles
in five languages
(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams
et al. (2017).
Select:
Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English",0.9536309242248535,0.00958715844899416
dfbab3cd991f86d998223726617d61113caa6193,"For the purposes of this paper, how is something determined to be domain specific knowledge?",,reviews under distinct product categories are considered specific domain knowledge,0.4630112648010254,0.07633008062839508
e051d68a7932f700e6c3f48da57d3e2519936c6d,Which pre-trained English NER model do they use?, bibref7,Bidirectional LSTM based NER model of Flair,0.06896150857210159,0.08546489477157593
e09e89b3945b756609278dcffb5f89d8a52a02cd,How many speeches are in the dataset?,,5575 speeches,0.9729546308517456,0.1931958794593811
e0b7acf4292b71725b140f089c6850aebf2828d2,How is annotation projection done when languages have different word order?,,"Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments.",0.40765127539634705,0.05847736448049545
e111925a82bad50f8e83da274988b9bea8b90005,How do they collect the control corpus?,,Randomly from Twitter,0.8073812127113342,0.15832433104515076
e1b36927114969f3b759cba056cfb3756de474e4,By how much does using phonetic feedback improve state-of-the-art systems?,,Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9,0.9455705285072327,0.13063575327396393
e2427f182d7cda24eb7197f7998a02bc80550f15,How is the architecture fault-tolerant?,,By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault,0.26969966292381287,0.025453966110944748
e28019afcb55c01516998554503bc1b56f923995,"Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",,Personal thought of the annotator.,0.24962745606899261,0.12619934976100922
e286860c41a4f704a3a08e45183cb8b14fa2ad2f,Is the model evaluated?,,the English version is evaluated. The German version evaluation is in progress ,0.019365184009075165,0.12556861340999603
e292676c8c75dd3711efd0e008423c11077938b1,Which soft-selection approaches are evaluated?,previous attention - based methods,LSTM and BERT ,0.5919438004493713,0.3590553402900696
e2f269997f5a01949733c2ec8169f126dabd7571,Which data sources do they use?,,"- En-Fr (WMT14)
- En-De (WMT15)
- Skipthought (BookCorpus)
- AllNLI (SNLI + MultiNLI)
- Parsing (PTB + 1-billion word)",0.672978937625885,0.07517997175455093
e3981a11d3d6a8ab31e1b0aa2de96f253653cfb2,For which languages most of the existing MRC datasets are created?, english,English,0.42498674988746643,1.0
e414d819f10c443cbefa8bdb9bd486ffc6d1fc6a,What is the average length of the recordings?,,40 minutes,0.8686500191688538,0.26435035467147827
e42fbf6c183abf1c6c2321957359c7683122b48e,How accurate is the aspect based sentiment classifier trained only using the XR loss?,,"BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.
BiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.
",0.8830770254135132,-0.03677213937044144
e4a19b91b57c006a9086ae07f2d6d6471a8cf0ce,How are the main international development topics that states raise identified?,," They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.",0.8784729242324829,0.03856519237160683
e4cc2e73c90e568791737c97d77acef83588185f,How long is the dataset?,,8000,0.6165768504142761,0.19053280353546143
e51d0c2c336f255e342b5f6c3cf2a13231789fed,Which Twitter corpus was used to train the word vectors?,,They collected tweets in Russian language using a heuristic query specific to Russian,0.9667942523956299,0.05850346013903618
e5a965e7a109ae17a42dd22eddbf167be47fca75,What are the problems related to ambiguity in PICO sentence prediction tasks?, fine - tuning of weights within the network,Some sentences are associated to ambiguous dimensions in the hidden state output,0.046547938138246536,0.036285191774368286
e63bde5c7b154fbe990c3185e2626d13a1bad171,What is the performance achieved on NarrativeQA?,,"Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87",0.27189740538597107,0.14765474200248718
e7329c403af26b7e6eef8b60ba6fefbe40ccf8ce,How much better does this baseline neural model do?,,"The model outperforms at every point in the
implicit-tuples PR curve reaching almost 0.8 in recall",0.9861284494400024,0.10565471649169922
e76139c63da0f861c097466983fbe0c94d1d9810,Is the model presented in the paper state of the art?, state - of - the - art,"No, supervised models perform better for this task.",0.1761569380760193,0.2447119504213333
e8029ec69b0b273954b4249873a5070c2a0edb8a,How much important is the visual grounding in the learning of the multilingual representations?,,performance is significantly degraded without pixel data,0.5797027349472046,0.02081996202468872
e829f008d62312357e0354a9ed3b0827c91c9401,Which psycholinguistic and basic linguistic features are used?,,"Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features",0.29982978105545044,0.13692402839660645
e84ba95c9a188fda4563f45e53fbc8728d8b5dab,Do they build one model per topic or on all topics?,,One model per topic.,0.9580367803573608,0.10573914647102356
e8fcfb1412c3b30da6cbc0766152b6e11e17196c,What improvement does the MOE model make over the SOTA on language modelling?,,Perpexity is improved from 34.7 to 28.0.,0.40218281745910645,0.13047796487808228
e91692136033bbc3f19743d0ee5784365746a820,"It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?",,using multiple pivot sentences,0.7832722067832947,0.07042783498764038
e9ccc74b1f1b172224cf9f01e66b1fa9e34d2593,What metadata is included?,,"besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date",0.4741341173648834,0.04935232549905777
e9d9bb87a5c4faa965ceddd98d8b80d4b99e339e,How much do they outperform previous state-of-the-art?,,"On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.
On subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.",0.7654103636741638,0.08940134197473526
ea6764a362bac95fb99969e9f8c773a61afd8f39,What is the highest accuracy score achieved?,,82.0%,0.9188680052757263,0.1943320482969284
ead5dc1f3994b2031a1852ecc4f97ac5760ea977,How many category tags are considered?,,14 categories,0.9221265912055969,0.2466406226158142
eb5ed1dd26fd9adb587d29225c7951a476c6ec28,What are the results of the experiment?,"What are the results of the experiment?the bd - 4sk - asr dataset : : : the language model ; we created the language from the transcriptions. the model was created using cmusphinx in which ( fixed ) discount mass is 0. 5, and backoffs are computed using the ratio method. the model includes 283 unigrams, 5337 bigrams, and 6935 trigrams","They were able to create a language model from the dataset, but did not test.",0.16114486753940582,0.4091019928455353
eb95af36347ed0e0808e19963fe4d058e2ce3c9f,What is the accuracy of the proposed technique?,,51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge,0.3488794267177582,0.01039537601172924
ec2b8c43f14227cf74f9b49573cceb137dd336e7,How is the speech recognition system evaluated?,"How is the speech recognition system evaluated?table, extracted from bibref0, reports wers obtained on evaluation data sets with a strongly adapted asr",Speech recognition system is evaluated using WER metric.,0.2122393548488617,0.7144401669502258
ed11b4ff7ca72dd80a792a6028e16ba20fccff66,How do they obtain region descriptions and object annotations?, visual genome dataset,they are available in the Visual Genome dataset,0.1370563805103302,0.8552848100662231
ed522090941f61e97ec3a39f52d7599b573492dd,What is triangulation?,,"Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.",0.9888765215873718,0.033626802265644073
ed7985e733066cd067b399c36a3f5b09e532c844,What is different in BERT-gen from standard BERT?,,"They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens.",0.5677260160446167,0.11864624917507172
ed7a3e7fc1672f85a768613e7d1b419475950ab4,Does this approach perform better in the multi-domain or single-domain setting?,,single-domain setting,0.3956299126148224,0.06635632365942001
edb068df4ffbd73b379590762125990fcd317862,which benchmark tasks did they experiment on?, stanford sentiment treebank bibref7 and the ag english news corpus bibref3, They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task.,0.4978973865509033,0.6431533098220825
edb2d24d6d10af13931b3a47a6543bd469752f0c,How did the select the 300 Reddit communities for comparison?,,They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.,0.936737060546875,0.16906395554542542
eddabb24bc6de6451bcdaa7940f708e925010912,How are the EAU text spans annotated?,,Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.,0.9665676951408386,0.017717357724905014
ee9b95d773e060dced08705db8d79a0a6ef353da,How are content clusters used to improve the prediction of incident severity?,,they are used as additional features in a supervised classification task,0.7916123867034912,0.060948923230171204
ef7b62a705f887326b7ebacbd62567ee1f2129b3,What were the baselines?,,"Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations",0.5595808625221252,0.003442011773586273
ef872807cb0c9974d18bbb886a7836e793727c3d,What contextual features are used?,,The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.,0.31362101435661316,0.057141076773405075
efb3a87845460655c53bd7365bcb8393c99358ec,What were their results on the three datasets?,,"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR",0.7585957050323486,0.048722632229328156
efc65e5032588da4a134d121fe50d49fe8fe5e8c,What supplemental tasks are used for multitask learning?,,"Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question",0.8220147490501404,0.06556147336959839
f10325d022e3f95223f79ab00f8b42e3bb7ca040,How are discourse features incorporated into the model?, an entity grid is constructed by feeding the document through an nlp pipeline to identify salient entities,They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.,0.102741539478302,0.4877243936061859
f1831b2e96ff8ef65b8fde8b4c2ee3e04b7ac4bf,How close do clusters match to ground truth tone categories?, normalized mutual information,"NMI between cluster assignments and ground truth tones for all sylables is:
Mandarin: 0.641
Cantonese: 0.464",0.10519620776176453,0.16901348531246185
f1bd66bb354e3dabf5dc4a71e6f08b17d472ecc9,How do slot binary classifiers improve performance?, adds extra supervision to generate the slots that will be present in the response more precisely,by adding extra supervision to generate the slots that will be present in the response,0.1479722410440445,0.925115168094635
f258ada8577bb71873581820a94695f4a2c223b3,How many samples did they generate for the artificial language?,,"70,000",0.7963674068450928,0.22656433284282684
f2c5da398e601e53f9f545947f61de5f40ede1ee,How do their interpret the coefficients?,,The coefficients are projected back to the dummy variable space.,0.23186223208904266,0.0434202179312706
f37ed011e7eb259360170de027c1e8557371f002,What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?,,"Humor in headlines (TitleStylist vs Multitask baseline):
Relevance: +6.53% (5.87 vs 5.51)
Attraction: +3.72% (8.93 vs 8.61)
Fluency: 1,98% (9.29 vs 9.11)",0.9796280860900879,0.12674248218536377
f398587b9a0008628278a5ea858e01d3f5559f65,By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?, big margin,"SPNet vs best baseline:
ROUGE-1: 90.97 vs 90.68
CIC: 70.45 vs 70.25",0.27622950077056885,0.24123556911945343
f4238f558d6ddf3849497a130b3a6ad866ff38b3,How is moral bias measured?,,"Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q) − cos(b, q)
Bias is calculated as substraction of cosine similarities of question and some answer for two opposite answers.",0.9236027598381042,0.031630102545022964
f42d470384ca63a8e106c7caf1cb59c7b92dbc27,What evaluation metrics are used?,,"exact match, f1 score, edit distance and goal match",0.16346488893032074,0.06242243945598602
f463db61de40ae86cf5ddd445783bb34f5f8ab67,what are the baselines?, the local features,Perceptron model using the local features.,0.2569405436515808,0.6189770102500916
f4e17b14318b9f67d60a8a2dad1f6b506a10ab36,How is the generative model evaluated?,,Comparing BLEU score of model with and without attention,0.7394007444381714,0.04065355286002159
f513e27db363c28d19a29e01f758437d7477eb24,what are the baselines?,,"AS Reader, GA Reader, CAS Reader",0.3519152104854584,0.07973405718803406
f52b2ca49d98a37a6949288ec5f281a3217e5ae8,How do they condition the output to a given target-source class?,,"They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.",0.5148561000823975,0.04960598424077034
f5707610dc8ae2a3dc23aec63d4afa4b40b7ec1e,What are resolution model variables?,,"Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.",0.8811306357383728,0.07395479828119278
f5db12cd0a8cd706a232c69d94b2258596aa068c,How much in experiments is performance improved for models trained with generated adversarial examples?,,"Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original
examples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)",0.17105510830879211,0.08989125490188599
f5e571207d9f4701b4d01199ef7d0bfcfa2c0316,What are the linguistic differences between each class?,,"Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes",0.6610124707221985,0.012810986489057541
f62c78be58983ef1d77049738785ec7ab9f2a3ee,what datasets did the authors use?,,"Kaggle
Subversive Kaggle
Wikipedia
Subversive Wikipedia
Reddit
Subversive Reddit ",0.4308769404888153,0.1981971561908722
f6380c60e2eb32cb3a9d3bca17cf4dc5ae584eca,Why is big data not appropriate for this task?,,Training embeddings from small-corpora can increase the performance of some tasks,0.07953039556741714,0.05426957085728645
f68bd65b5251f86e1ed89f0c858a8bb2a02b233a,how do they collect the comparable corpus?,,Randomly from a Twitter dump,0.18889248371124268,0.06104198843240738
f7817b949605fb04b1e4fec9dd9ca8804fb92ae9,Why does not the approach from English work on other languages?, english does not mark grammatical gender,"Because, unlike other languages, English does not mark grammatical genders",0.290828138589859,0.8804225921630859
f7c34b128f8919e658ba4d5f1f3fc604fb7ff793,What are all the input modalities considered in prior work in question generation?,,"Textual inputs, knowledge bases, and images.",0.9512909054756165,0.07780138403177261
f8281eb49be3e8ea0af735ad3bec955a5dedf5b3,Is the semantic hierarchy representation used for any task?,,"Yes, Open IE",0.09445004165172577,0.08574079722166061
fa2a384a23f5d0fe114ef6a39dced139bddac20e,How big is the dataset?,,903019 references,0.7982617020606995,0.12136229872703552
fa2ffc6b4b046e17bc41e199855c4941673e2caf,What parallel corpus did they use?, monolingual,Parallel monolingual corpus in English and Mandarin,0.2599015533924103,0.5925001502037048
fa3312ae4bbed11a5bebd77caf15d651962e0b26,What was the performance on the self-collected corpus?,,F1 scores of 86.16 on slot filling and 94.56 on intent detection,0.9825769662857056,0.10462986677885056
fa9df782d743ce0ce1a7a5de6a3de226a7e423df,What are the languages they consider in this paper?," english, russian, arabic, chinese, german, spanish, french","The languages considered were English, Chinese, German, Russian, Arabic, Spanish, French",0.2874643802642822,0.8015304207801819
fb5ce11bfd74e9d7c322444b006a27f2ff32a0cf,What is task success rate achieved? , 97. 6 % and 96. 0 %,96-97.6% using the objects color or shape and 79% using shape alone,0.1141599714756012,0.6274603605270386
fb76e994e2e3fa129f1e94f1b043b274af8fb84c,How does the context-aware variational autoencoder learn event background information?,," CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.",0.8724977970123291,0.033759307116270065
fbe5e513745d723aad711ceb91ce0c3c2ceb669e,What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?," trac - 2018 bibref0, bibref2",nan,0.2357773333787918,0.08002734184265137
fbee81a9d90ff23603ee4f5986f9e8c0eb035b52,What are their initial results on this task?," precision, recall, and f1 scores","Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.",0.18809016048908234,0.4679903984069824
fc3f0eb297b2308b99eb4661a510c9cdbb6ffba2,What is the size of the dataset?,,3029,0.298602432012558,0.24073481559753418
fc8bc6a3c837a9d1c869b7ee90cf4e3c39bcd102,Is the baseline a non-heirarchical model like BERT?,,There were hierarchical and non-hierarchical baselines; BERT was one of those baselines,0.4560680091381073,0.031781211495399475
fd7f13b63f6ba674f5d5447b6114a201fe3137cb,What language is the experiment done in?,,english language,0.9010878205299377,0.25568723678588867
fd8b6723ad5f52770bec9009e45f860f4a8c4321,What QA models were used?,,"A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.",0.934281587600708,0.051550377160310745
fe90eec1e3cdaa41d2da55864c86f6b6f042a56c,What are the sources of the data?," online data for three domains, namely, “ hotel "", “ mobile phone ( mobile ) "", and “ travel "".","User reviews written in Chinese collected online for hotel, mobile phone, and travel domains",0.11099644005298615,0.4806864261627197
fee5aef7ae521ccd1562764a91edefecec34624d,How does explicit constraint on the KL divergence term that authors propose looks like?,,"Answer with content missing: (Formula 2) Formula 2 is an answer: 
\big \langle\! \log p_\theta({x}|{z}) \big \rangle_{q_\phi({z}|{x})}  -  \beta |D_{KL}\big(q_\phi({z}|{x}) || p({z})\big)-C|",0.7466297745704651,0.029898548498749733
ff28d34d1aaa57e7ad553dba09fc924dc21dd728,What are their correlation results?," spearman ’ s ρ, kendall ’ s τ and pearson ’ s r correlations on duc - 05, duc - 06 and duc - 07",High correlation results range from 0.472 to 0.936,0.10547421872615814,0.44946324825286865
ff2bcf2d8ffee586751ce91cf15176301267b779,What are the characteristics of the city dialect?,,Lexicon of the cities tend to use most forms of a particular concept,0.45870086550712585,0.02331821247935295
