question_id,question,model_answer,ground_truth,confidence,cosine_sim
0038b073b7cca847033177024f9719c971692042,How is the input triple translated to a slot-filling task?,natural language,"The relation R(x,y) is mapped onto a question q whose answer is y",0.24621395766735077,0.15670987963676453
00bcdffff7e055f99aaf1b05cf41c98e2748e948,What is the baseline method for the task?,validating our models on established corpora,"For the emotion recognition from text they use described neural network as baseline.
For audio and face there is no baseline.",0.5629012584686279,0.159640371799469
00ef9cc1d1d60f875969094bb246be529373cb1d,What methodology is used to compensate for limited labelled data?,,Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.,0.04640952870249748,0.17002639174461365
01123a39574bdc4684aafa59c52d956b532d2e53,By how much does their method outperform state-of-the-art OOD detection?,more than 17 points in precision @ k on average,"AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average",0.3809746205806732,0.26772961020469666
01dc6893fc2f49b732449dfe1907505e747440b0,What debate topics are included in the dataset?,"law, ethics, etc.","Ethics, Gender, Human rights, Sports, Freedom of Speech, Society, Religion, Philosophy, Health, Culture, World, Politics, Environment, Education, Digital Freedom, Economy, Science and Law",0.39630642533302307,0.5795593857765198
02348ab62957cb82067c589769c14d798b1ceec7,What simpler models do they look at?,what simpler models do they look at?,"BiGRUs with attention, ROUGE, Language model, and next sentence prediction ",0.0881817489862442,0.2851133644580841
02417455c05f09d89c2658f39705ac1df1daa0cd,How much does it minimally cost to fine-tune some model according to benchmarking framework?,,"$1,728",0.09794392436742783,-0.006793574430048466
02e4bf719b1a504e385c35c6186742e720bcb281,How are relations used to propagate polarity?,,"based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ",0.059156931936740875,0.15178608894348145
03ce42ff53aa3f1775bc57e50012f6eb1998c480,What 6 language pairs is experimented on?,,"EN<->ES
EN<->DE
EN<->IT
EN<->EO
EN<->MS
EN<->FI",0.09439918398857117,0.14247390627861023
04012650a45d56c0013cf45fd9792f43916eaf83,How much is performance hurt when using too small amount of layers in encoder?,,"comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ",0.05026539787650108,0.09329220652580261
0457242fb2ec33446799de229ff37eaad9932f2a,Which elements of the platform are modular?,integrative,"handling large volume incoming data, sentiment analysis on tweets and predictive online learning",0.6045872569084167,0.23819509148597717
04b43deab0fd753e3419ed8741c10f652b893f02,What are the two decoding functions?,a linear projection,"a linear projection and a bijective function with continuous transformation though  ‘affine coupling layer’ of (Dinh et al.,2016). ",0.41170719265937805,0.558837890625
04f72eddb1fc73dd11135a80ca1cf31e9db75578,How much more coverage is in the new dataset?,,278 more annotations,0.05118376016616821,0.2083081603050232
05671d068679be259493df638d27c106e7dd36d0,What is the performance proposed model achieved on MathQA?,state - of - the - art performance,"Operation accuracy: 71.89
Execution accuracy: 55.95",0.22158236801624298,0.4033583998680115
056fc821d1ec1e8ca5dc958d14ea389857b1a299,How many feature maps are generated for a given triple?,5,3 feature maps for a given tuple,0.5583094954490662,0.2033759206533432
06095a4dee77e9a570837b35fc38e77228664f91,"Is all text in this dataset a question, or are there unrelated sentences in between questions?",,the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ,0.09624253958463669,0.07531290501356125
068dbcc117c93fa84c002d3424bafb071575f431,How was quality measured?,"inter - annotator agreement ( iaa ) to estimate dataset consistency across different annotations, we measure f1 using our ua metric with 5 generators per predicate","Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.",0.2650299668312073,0.6939242482185364
07580f78b04554eea9bb6d3a1fc7ca0d37d5c612,Can the approach be generalized to other technical domains as well? ,,"There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.",0.10530658811330795,-0.03752109780907631
07c59824f5e7c5399d15491da3543905cfa5f751,How big is dataset used for training/testing?,"2, 557 days","4,261  days for France and 4,748 for the UK",0.2806934118270874,0.5728290677070618
0828cfcf0e9e02834cc5f279a98e277d9138ffd9,How was the dataset collected?,"the kurdish books of grades one to three of primary schools in the kurdistan region of iraq were used to extract 200 sample sentences. the dataset includes the dictionary, the phoneset, the transcriptions of the corpus sentences using the suggested phones, the recorded narrations of the sentences, and the acoustic model",extracted text from Sorani Kurdish books of primary school and randomly created sentences,0.10124586522579193,0.7403391003608704
08333e4dd1da7d6b5e9b645d40ec9d502823f5d7,How much performance gap between their approach and the strong handcrafted method?,,"0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C",0.1065419614315033,0.09075276553630829
085147cd32153d46dd9901ab0f9195bfdbff6a85,What are the baseline models?,c - cnn,"MC-CNN
MVCNN
CNN",0.25202852487564087,0.6361636519432068
093039f974805952636c19c12af3549aa422ec43,Is this library implemented into Torch or is framework agnostic?,agnostic,It uses deep learning framework (pytorch),0.7990773916244507,-0.05399831384420395
097ab15f58cb1fce5b5ffb5082b8d7bbee720659,Which language has the lowest error rate reduction?,,thai,0.04474153742194176,0.330780953168869
0a3a8d1b0cbac559f7de845d845ebbfefb91135e,What accuracy is achieved by the speech recognition system?,,"Accuracy not available: WER results are reported 42.6 German, 35.9 English",0.10584506392478943,0.04720362648367882
0a7ac8eccbc286e0ab55bc5949f3f8d2ea2d1a60,How many improvements on the French-German translation benchmark?,how many improvements on the french - german translation benchmark?,one,0.0736939013004303,0.13479700684547424
0b31eb5bb111770a3aaf8a3931d8613e578e07a8,"What are the selection criteria for ""causal statements""?",,"Presence of only the exact unigrams 'caused', 'causing', or 'causes'",0.08650349080562592,0.07465581595897675
0b411f942c6e2e34e3d81cc855332f815b6bc123,What's the method used here?,advantage actor critic bibref29,Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8.,0.5015767216682434,0.2814843952655792
0bd864f83626a0c60f5e96b73fb269607afc7c09,How are sentence embeddings incorporated into the speech recognition system?,pre - trained on large textual corpora within our end - to - end speech recognition framework,BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer.,0.2924463152885437,0.31370800733566284
0c09a0e8f9c5bdb678563be49f912ab6e3f97619,How many roles are proposed?,three or more roles,12,0.16267400979995728,0.1835310310125351
0c7823b27326b3f5dff51f32f45fc69c91a4e06d,In which setting they achieve the state of the art?,bibref7 proposed the co - attention mechanism in vqa,in open-ended task esp. for counting-type questions ,0.1560484766960144,0.1466534286737442
0d7de323fd191a793858386d7eb8692cc924b432,What writing styles are present in the corpus?,"letters s and t are written with a comma, not with a cedilla ( like s and t ). in romanian many older texts are written with cedillas instead of commas because full unicode support in windows came much later than the classic extended ascii which only contained the cedilla letters. the 16 classes are inspired by the ontonotes5 corpus bibref7 as well as the ace ( automatic content extraction ) english annotation guidelines for entities version 6. 6 2008. 06. 13 bibref8. each class will be presented in detail, with examples, in the section secref3 a summary of available classes with word counts for each is available in table tabref18. the corpus is available in two formats : brat and conll - u plus. corpus description the corpus, at its current version 1. 0 is composed of 5127 sentences, annotated with 16 classes, for a total of 26377 annotated entities. the 16 classes are : person, nat _ rel _ pol, org, gpe, loc, facility, product, event, language, work _ of _ art, datetime, period, money, quantity, numeric _ value and ordinal. it is based on copyright - free text extracted from southeast european times ( setimes ). the news portal has published “ news and views from southeast europe ” in ten languages, including romanian. setimes has been used in the past for several annotated corpora, including parallel corpora for machine translation. for ronec we have used a hand - picked selection of sentences belonging to several categories ( see table tabref16 for stylistic examples ). the corpus creation process involved a small number of people that have voluntarily joined the initiative, with the authors of this paper directing the work. initially, we searched for ner resources in romanian, and found none. then we looked at english resources and read the in - depth ace guide, out of which a 16 - class draft evolved. we then identified a copy - right free text from which we hand - picked sentences to maximize the amount of entities while maintaining style balance. the annotation process was a trial - and - error, with cycles composed of annotation, discussing confusing entities, updating the annotation guide schematic and going through the corpus section again to correct entities following","current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.",0.05544741079211235,0.1688760668039322
0da6cfbc8cb134dc3d247e91262f5050a2200664,What topic clusters are identified by LDA?,inlineform0,"Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club",0.3316846489906311,-0.0029811321292072535
0fcac64544842dd06d14151df8c72fc6de5d695c,What previous methods is the proposed method compared against?,pre - trained word embedding,"BLSTM+Attention+BLSTM
Hierarchical BLSTM-CRF
CRF-ASN
Hierarchical CNN (window 4)
mLSTM-RNN
DRLM-Conditional
LSTM-Softmax
RCNN
CNN
CRF
LSTM
BERT",0.07526476681232452,0.3319157361984253
0fd678d24c86122b9ab27b73ef20216bbd9847d1,What evaluation metrics are used?,,Accuracy on each dataset and the average accuracy on all datasets.,0.09885023534297943,0.08576095849275589
10ddc5caf36fe9d7438eb5a3936e24580c4ffe6a,Which competitive relational classification models do they test?,"inlineform0, transe rank relations according to inlineform1. for each inlineform2 in the test set, we call the relations with higher rank scores than inlineform3",For relation prediction they test TransE and for relation extraction they test position aware neural sequence model,0.03528387472033501,0.44701582193374634
1165fb0b400ec1c521c1aef7a4e590f76fee1279,How do they model travel behavior?,"embeddings : : : methodology since a choice model will typically involve other variables than the categorical ones that we learn the embeddings for, it is important to take into account their effects. figure figref24 shows the simplest travel embeddings model. as an example, the categorical variable is trip purpose, and there are a few other variables such as gender, cost of the alternatives, distance, and so on. notice that they are directly fed into the softmax output layer, together with the embeddings output. representing categorical variables we are generally concerned with random utility maximization ( rum ) models",The data from collected travel surveys is used to model travel behavior.,0.1567709743976593,0.39661985635757446
11d2f0d913d6e5f5695f8febe2b03c6c125b667c,How is performance of this system measured?,how is performance of this system measured?,using the BLEU score as a quantitative metric and human evaluation for quality,0.048342760652303696,0.330058217048645
126e8112e26ebf8c19ca7ff3dd06691732118e90,What are simulated datasets collected?,tree - structured revision chains,There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents,0.9461185932159424,0.23599408566951752
12ac76b77f22ed3bcb6430bcd0b909441d79751b,What are the competing models?,,"TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.",0.10134533792734146,0.16771213710308075
12cfbaace49f9363fcc10989cf92a50dfe0a55ea,what results do they achieve?,,91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task,0.055915627628564835,0.1192324087023735
12eaaf3b6ebc51846448c6e1ad210dbef7d25a96,How many convolutional layers does their model have?,five,wav2vec has 12 convolutional layers,0.5039186477661133,0.20813246071338654
13d92cbc2c77134626e26166c64ca5c00aec0bf5,What baseline approaches do they compare against?,,"HotspotQA: Yang, Ding, Muppet
Fever: Hanselowski, Yoneda, Nie",0.10226615518331528,0.2275499403476715
13e87f6d68f7217fd14f4f9a008a65dd2a0ba91c,What is the performance of the model?,,"Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates
Experiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)",0.03730957582592964,0.07807271927595139
14634943d96ea036725898ab2e652c2948bd33eb,What is the accuracy of the model for the six languages tested?,close to the best - reported accuracy,"Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)",0.20334383845329285,0.45650842785835266
14e259a312e653f8fc0d52ca5325b43c3bdfb968,"Is any data-to-text generation model trained on this new corpus, what are the results?",explosion of neural data - to - text generation models,"Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.",0.10613707453012466,0.17071905732154846
14eb2b89ba39e56c52954058b6b799a49d1b74bf,How are their changes evaluated?,to ensure that rasa works as intended,The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset,0.12256565690040588,0.18829187750816345
157b9f6f8fb5d370fa23df31de24ae7efb75d6f3,How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?,significantly lower than cross - validated results on data from a single year,"They achieved best result in the PAN 2017 shared task with accuracy for Variety prediction task 0.0013 more than the 2nd best baseline, accuracy for Gender prediction task 0.0029 more than 2nd best baseline and accuracy for Joint prediction task 0.0101 more than the 2nd best baseline",0.02156297117471695,0.26324713230133057
160e6d2fc6e04bb0b4ee8d59c06715355dec4a17,What accuracy score do they obtain?,what accuracy score do they obtain?,the best performing model obtained an accuracy of 0.86,0.04390527680516243,0.5853978395462036
16af38f7c4774637cf8e04d4b239d6d72f0b0a3a,How large is the dataset?,,over 104k documents,0.1085595041513443,0.09874163568019867
16f71391335a5d574f01235a9c37631893cd3bb0, What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?,,"Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)",0.10160922259092331,0.08453842997550964
171ebfdc9b3a98e4cdee8f8715003285caeb2f39,How better is accuracy of new model compared to previously reported models?,,"Average accuracy of proposed model vs best prevous result:
Single-task Training: 57.57 vs 55.06
Multi-task Training: 50.17 vs 50.59",0.1025686115026474,0.051440250128507614
1771a55236823ed44d3ee537de2e85465bf03eaf,What is the difference in recall score between the systems?,,"Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.",0.054560884833335876,0.08803405612707138
18288c7b0f8bd7839ae92f9c293e7fb85c7e146a,How strong was the correlation between exercise and diabetes?,,weak correlation with p-value of 0.08,0.040522150695323944,0.014511150307953358
182b6d77b51fa83102719a81862891f49c23a025,What limitations are mentioned?,limitations we identified some limitations during the process,"deciding publisher partisanship, risk annotator bias because of short description text provided to annotators",0.083631232380867,0.13400901854038239
1959e0ebc21fafdf1dd20c6ea054161ba7446f61,How is the clinical text structuring task defined?,fetching medical research data from electronic health records,CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.,0.18255160748958588,0.42478567361831665
19c9cfbc4f29104200393e848b7b9be41913a7ac,How many questions are in the dataset?,"2, 714","2,714 ",0.326966255903244,1.0
1a69696034f70fb76cd7bb30494b2f5ab97e134d,By how much does their model outperform existing methods?,fbk - irst bibref10,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result.,0.13575638830661774,0.252282053232193
1a8b7d3d126935c09306cacca7ddb4b953ef68ab,What were their results?,what were their results?,Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set,0.05692648887634277,0.4061627686023712
1adbdb5f08d67d8b05328ccc86d297ac01bf076c,What languages do they use?,,"Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.",0.10123451799154282,-0.061593107879161835
1b9119813ea637974d21862a8ace83bc1acbab8e,What dataset do they use?,,They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).,0.0754934549331665,0.05662671849131584
1baf87437b70cc0375b8b7dc2cfc2830279bc8b5,"What is the source of the ""control"" corpus?","what is the source of the "" control "" corpus?","Randomly selected from a Twitter dump, temporally matched to causal documents",0.046318359673023224,0.17180752754211426
1c4cd22d6eaefffd47b93c2124f6779a06d2d9e1,How big is their dataset?,,"3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing",0.10194697976112366,0.112259142100811
1c68d18b4b65c4d75dc199d2043079490f6310f8,What are the two PharmaCoNER subtasks?,"subtask 1, entities must be identified with offsets and mapped to a predefined set of four classes ( proteinas, normalizables, no _ normalizables and unclear ) ; for subtask 2",Entity identification with offset mapping and concept indexing,0.11265598982572556,0.4591642916202545
1cbca15405632a2e9d0a7061855642d661e3b3a7,How much improvement do they get?,seven times,Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.,0.07065785676240921,0.008025109767913818
1d74fd1d38a5532d20ffae4abbadaeda225b6932,What is their f1 score and recall?,what is their f1 score and recall?,"F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.",0.08760707825422287,0.8148446679115295
1ed6acb88954f31b78d2821bb230b722374792ed,What is private dashboard?,leaderboard,Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).,0.11977823078632355,0.46511608362197876
1f63ccc379f01ecdccaa02ed0912970610c84b72,How much is the gap between using the proposed objective and using only cross-entropy objective?,,The mixed objective improves EM by 2.5% and F1 by 2.2%,0.040986496955156326,0.19007067382335663
1fb73176394ef59adfaa8fc7827395525f9a5af7,Where did they get training data?,,AmazonQA and ConciergeQA datasets,0.10897617042064667,0.16350260376930237
20ec88c45c1d633adfd7bff7bbf3336d01fb6f37,what are the evaluation metrics?,,"Precision, Recall, F1",0.0983407273888588,0.17369987070560455
2122bd05c03dde098aa17e36773e1ac7b6011969,What task do they evaluate on?,semantic parsing,Fill-in-the-blank natural language questions,0.0148689029738307,0.3213269114494324
21663d2744a28e0d3087fbff913c036686abbb9a,How does their model differ from BERT?,"relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute - position embedding",Their model does not differ from BERT.,0.06776445358991623,0.20972874760627747
2210178facc0e7b3b6341eec665f3c098abef5ac,What type of recurrent layers does the model use?,fully connected layer composed of rectified linear units,GRU,0.14118234813213348,0.11879540234804153
22b8836cb00472c9780226483b29771ae3ebdc87,What is the new initialization method proposed in this paper?,skip - gram with negative - sampling,They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.,0.325425922870636,0.2516469955444336
22c802872b556996dd7d09eb1e15989d003f30c0,How do they correlate NED with emotional bond levels?,negatively,They compute Pearson’s correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating,0.3693894147872925,0.11165046691894531
234ccc1afcae4890e618ff2a7b06fc1e513ea640,How big is performance improvement proposed methods are used?,average,"Data augmentation (es)  improved Adv es by 20% comparing to baseline 
Data augmentation (cs) improved Adv cs by 16.5% comparing to baseline
Data augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline 
All models show improvements over adversarial sets  
",0.0929853618144989,0.17024670541286469
2376c170c343e2305dac08ba5f5bda47c370357f,How was the dataset collected?,,"They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ",0.040704626590013504,0.14306089282035828
238ec3c1e1093ce2f5122ee60209b969f7669fae,How is the fluctuation in the sense of the word and its neighbors measured?,,"Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.
4) Computing the mean m and the sample variance σ for the uniformities of ai .
5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.",0.05352216958999634,0.16591860353946686
23d32666dfc29ed124f3aa4109e2527efa225fbc,Do the use word embeddings alone or they replace some previous features of the model with word embeddings?,,They use it as addition to previous model - they add new edge between words if word embeddings are similar.,0.09259455651044846,0.14672711491584778
25b2ae2d86b74ea69b09c140a41593c00c47a82b,How were the navigation instructions collected?,crowd - sourcing data,using Amazon Mechanical Turk using simulated environments with topological maps,0.21813338994979858,0.3195230960845947
25fd61bb20f71051fe2bd866d221f87367e81027,What baselines have been used in this work?,preprocessing and hyper - parameters,"NDM, LIDM, KVRN, and TSCP/RL",0.7971733212471008,0.07449934631586075
26c290584c97e22b25035f5458625944db181552,What is the size of their dataset?,what is the size of their dataset?,"10,001 utterances",0.08724641054868698,0.2910034656524658
26faad6f42b6d628f341c8d4ce5a08a591eea8c2,How many documents are in the Indiscapes dataset?,508,508,0.3043762147426605,0.9999999403953552
2815bac42db32d8f988b380fed997af31601f129,What is improvement in accuracy for short Jokes in relation other types of jokes?,8 percent,It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%,0.05707215517759323,0.2726322412490845
281cd4e78b27a62713ec43249df5000812522a89,What is the average length of the claims?,inlineform2,Average claim length is 8.9 tokens.,0.09442079067230225,-0.018368341028690338
2869d19e54fb554fcf1d6888e526135803bb7d75,What performance did they obtain on the SemEval dataset?,our system comfortably surpasses the existing best system,F1 score of 82.10%,0.11900947242975235,0.15601803362369537
28b2a20779a78a34fb228333dc4b93fd572fda15,Does proposed end-to-end approach learn in reinforcement or supervised learning manner?,,supervised learning,0.1007368192076683,0.31970900297164917
29d917cc38a56a179395d0f3a2416fca41a01659,How are the potentially relevant text fragments identified?,search engine," Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5–10 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.",0.09972137212753296,0.3923982083797455
2a46db1b91de4b583d4a5302b2784c091f9478cc,How many examples do they have in the target domain?,,"Around 388k examples, 194k from tst2013 (in-domain) and 194k from newstest2014 (out-of-domain)",0.09695044904947281,0.1340610235929489
2a6469f8f6bf16577b590732d30266fd2486a72e,What is novel in author's approach?,personalization,"They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data.",0.2728928029537201,0.3747516870498657
2cf8825639164a842c3172af039ff079a8448592,How is the data annotated?,,The data are self-reported by Twitter users and then verified by two human experts.,0.03727225214242935,0.20713026821613312
2d274c93901c193cf7ad227ab28b1436c5f410af,What are the baselines that Masque is compared against?,nlg and q & a styles,"BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D",0.49878108501434326,0.3003292977809906
2d307b43746be9cedf897adac06d524419b0720b,How long are the datasets?,"how long are the datasets? two - level lstm given two training data sets ( denoted by t1 and t2 ), a new learning model should be utilized. lstm is a widely used deep neural network in deep learning - based text classification. lstm is a typical rnn model for short - term memory, which can last for a long period of time","Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses",0.07608924061059952,0.22461844980716705
2d4d0735c50749aa8087d1502ab7499faa2f0dd8,By how much do they outperform previous state-of-the-art models?,,"Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)",0.03709175065159798,0.029423534870147705
2d536961c6e1aec9f8491e41e383dc0aac700e0a,What are all 15 types of modifications ilustrated in the dataset?,one seed sentence,"- paraphrase 1
- paraphrase 2
- different meaning
- opposite meaning
- nonsense
- minimal change
- generalization
- gossip
- formal sentence
- non-standard sentence
- simple sentence
- possibility
- ban
- future
- past",0.34809303283691406,0.31054380536079407
2df2f6e4efd19023434c84f5b4f29a2f00bfc9fb,What other non-neural baselines do the authors compare to? ,what other non - neural baselines do the authors compare to?,"bag of words, tf-idf, bag-of-means",0.03601705655455589,0.10426563024520874
2e1660405bde64fb6c211e8753e52299e269998f,How long is the dataset?,2018 - 12 - 07,"645, 600000",0.14838266372680664,0.14613929390907288
2e1ededb7c8460169cf3c38e6cde6de402c1e720,What is the prediction accuracy of the model?,what is the prediction accuracy of the model?,"mean prediction accuracy 0.99582651
S&P 500 Accuracy 0.99582651",0.03418872505426407,0.6668065190315247
2ec97cf890b537e393c2ce4c2b3bd05dfe46f683,How do they measure correlation between the prediction and explanation quality?,,They look at the performance accuracy of explanation and the prediction performance,0.02881494350731373,0.13114692270755768
3070d6d6a52aa070f0c0a7b4de8abddd3da4f056,How do they measure performance of language model tasks?,,"BPC, Perplexity",0.025590216740965843,0.13076631724834442
30803eefd7cdeb721f47c9ca72a5b1d750b8e03b,How well does their system perform on the development set of SRE?,,"EER 16.04, Cmindet 0.6012, Cdet 0.6107",0.04038058966398239,0.044100694358348846
30b5e5293001f65d2fb9e4d1fdf4dc230e8cf320,What text classification task is considered?,,To classify a text as belonging to one of the ten possible classes.,0.045350395143032074,0.13190162181854248
30dad5d9b4a03e56fa31f932c879aa56e11ed15b,What is the 12 class bilingual text?,english - roman urdu,"Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant",0.7487923502922058,0.20504508912563324
3103502cf07726d3eeda34f31c0bdf1fc0ae964e,How do Zipf and Herdan-Heap's laws differ?,"consequences of zipf ' s law, and are, thus, as universal.. another interesting characterization of texts is the heaps - herdan law, which describes how the vocabulary - that is, the set of different words - grows with the size of a text, the number of which, empirically, has been found to grow as a power of the text size bibref18, bibref19. it is worth noting that it has been argued that this law is a consequence zipf ' s law. bibref20, bibref21 in figure ( figref2 ) we show zipf plots for some of the texts, including the random texts constructed as described previously. it is clear that all the texts reproduce convincingly zipf ' s law : where is the word rank, is the size of the vocabulary and is its frequency. this is in contrast to previous work in which it is argued that there are differences between the zipf plots of texts and random sequencesbibref32, this might be due to the fact that our random text construction preserves correlations between letters","Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)",0.0693746879696846,0.7754831910133362
311a7fa62721e82265f4e0689b4adc05f6b74215,How do they define upward and downward reasoning?,monotone,"Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.",0.4392327666282654,0.17527242004871368
3213529b6405339dfd0c1d2a0f15719cdff0fa93,What is the baseline model used?,bert,"The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data",0.17758943140506744,0.4009592533111572
327e06e2ce09cf4c6cc521101d0aecfc745b1738,What evaluation metrics did they look at?,,accuracy with standard deviation,0.08496425300836563,0.10632763803005219
327e6c6609fbd4c6ae76284ca639951f03eb4a4c,By how much is performance on CN-Celeb inferior to performance on VoxCeleb?,14. 24 %,"For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb",0.6915400624275208,0.14582693576812744
32e8eda2183bcafbd79b22f757f8f55895a0b7b2,How many categories of offensive language were there?,two,3,0.11969012767076492,0.6870954632759094
334f90bb715d8950ead1be0742d46a3b889744e7,What semantic features help in detecting whether a piece of text is genuine or generated? of ,"basic parameters of the language, and of the training data set","No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.",0.2543063163757324,0.26029521226882935
33d864153822bd378a98a732ace720e2c06a6bc6,What is new state-of-the-art performance on CoNLL-2009 dataset?,,In closed setting 84.22 F1 and in open 87.35 F1.,0.08524096012115479,0.1330537348985672
33f72c8da22dd7d1378d004cbd8d2dcd814a5291,What is the metric that is measures in this paper?,error rate,error rate in a minimal pair ABX discrimination task,0.8159293532371521,0.5496566891670227
34af2c512ec38483754e94e1ea814aa76552d60a,What benchmarks are created?,supporting evidence annotated,Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples,0.2027398645877838,0.11939927935600281
34fab25d9ceb9c5942daf4ebdab6c5dd4ff9d3db,What dataset did they use?,"chinese nlp datasets including named entity recognition, sentiment classification, sentence pair matching, natural language inference, etc. the results show that the proposed model brings another gain over bert bibref1, ernie bibref2 and bert - wwm bibref12, bibref13 in all the tasks. experiments : : : experiment results table tabref14 shows the experiment measuring improvements from the mwa attention on test sets of four datasets. generally, our method consistently outperforms all baselines on all of four tasks, which clearly indicates the advantage of introducing word segmentation information into the encoding of character sequences. moreover, the wilcoxon ’ s test shows that significant difference ( ) exits between our model with baseline models. we carried out experiments on four chinese nlp tasks, including emotion classification ( ec ), named entity recognition ( ner ), sentence pair matching ( spm ) and natural language inference ( nli )","weibo-100k, Ontonotes, LCQMC and XNLI",0.046338777989149094,0.2647271454334259
36a9230fadf997d3b0c5fc8af8d89bd48bf04f12,Which model architecture do they for sentence encoding?,many - to - many,"Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs
- RNN",0.12224206328392029,0.024855855852365494
36b25021464a9574bf449e52ae50810c4ac7b642,Where does the information on individual-level demographics come from?,"passively sensed social data, can shed better light on the population - level epidemiology of depression. demographic information inference on social media",From Twitter profile descriptions of the users.,0.08047255873680115,0.3839809000492096
36feaac9d9dee5ae09aaebc2019b014e57f61fbf,How do they measure model size?,no slot - specific neural network structures,By the number of parameters.,0.19450083374977112,0.10740003734827042
3703433d434f1913307ceb6a8cfb9a07842667dd,What learning paradigms do they cover in this survey?,"three emergent trends that deep learning has brought in qg : ( 1 ) the change of learning paradigm, ( 2 ) the broadening of the input spectrum, and ( 3 ) the generation of deep questions","Considering ""What"" and ""How"" separately versus jointly optimizing for both.",0.03567463904619217,0.15381087362766266
3748787379b3a7d222c3a6254def3f5bfb93a60e,What linguistic quality aspects are addressed?,"five criteria addressed are given in figure figref2. we provide a thorough evaluation on three publicly available summarization datasets from nist shared tasks, and compare the performance of our model to a wide variety of baseline methods capturing different aspects of linguistic quality. sum - qe achieves very high correlations with human ratings, showing the ability of bert to model linguistic qualities that relate to both text content and form","Grammaticality, non-redundancy, referential clarity, focus, structure & coherence",0.08730006217956543,0.2973766326904297
37bc8763eb604c14871af71cba904b7b77b6e089,How is module that analyzes behavioral state trained?,,pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus,0.04288126155734062,0.13656260073184967
37c7c62c9216d6cf3d0858cf1deab6db4b815384,how was annotation done?,distributing it over dozens of people,Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations,0.403706431388855,0.12104780972003937
384d571e4017628ebb72f3debb2846efaf0cb0cb,On what dataset is Aristo system trained?,training partition,"Aristo Corpus
Regents 4th
Regents 8th
Regents `12th
ARC-Easy
ARC-challenge ",0.16126284003257751,0.2252262681722641
38a5cc790f66a7362f91d338f2f1d78f48c1e252,What baseline is used?,svm baseline : the baseline classifier uses a linear support vector machine bibref7,SVM,0.374894917011261,0.552704930305481
38c74ab8292a94fc5a82999400ee9c06be19f791,How large is the corpus?,,"It contains 106,350 documents",0.10674065351486206,0.030105115845799446
39a450ac15688199575798e72a2cc016ef4316b5,How much performance improvements they achieve on SQuAD?,robust,Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ,0.07815694808959961,0.13971708714962006
39f8db10d949c6b477fa4b51e7c184016505884f,How does their model learn using mostly raw data?,japanese web corpus,by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity,0.06551815569400787,0.22589051723480225
3a3a65c65cebc2b8c267c334e154517d208adc7d,What extraction model did they use?,,"Multi-Encoder, Constrained-Decoder model",0.0905546098947525,0.11539624631404877
3aa7173612995223a904cc0f8eef4ff203cbb860,What baseline models do they compare against?,"what baseline models do they compare against? discussion of multi - perspective to study the effectiveness of each perspective, we conduct several experiments on the three single perspectives and their combination perspective. table 3 presents their comparison results. the first group of models are based on the three single perspectives, and we can observe that the union perspective performs best compared with the difference and similarity perspective. moreover, the union perspective achieves 82. 73 % in accuracy, exceeding the trian by 0. 79 % absolute. we can also see that the similarity perspective is inferior to the other two perspectives. the second group of models in the table 3 are formed from two perspectives. compared with the single union perspective, combining the difference perspective with the union perspective can improve 0. 11 %. composing union and similarity fusion together doesn ' t help the training. to our surprise, the combination of similarity perspective and difference perspective obtains 83. 09 % accuracy score. experimental results table 2 shows the results of our mpfn model along with the competitive models on the mcscript dataset. the trian achieves 81. 94 % in terms of test accuracy, which is the best result of the single model. the best performing ensemble result is 84. 13 %, provided by hma, which is the voting results of 7 single systems. our single mpfn model achieves 83. 52 % in terms of accuracy, outperforming all the previous models. the model exceeds the hma and trian by approximately 2. 58 % and 1. 58 % absolute respectively. our ensemble model surpasses the current state - of - the - art model with an accuracy of 84. 84 %. we got the final ensemble result by voting on 4 single models. every single model uses the same architecture but different parameters. on semeval2018 task 11, most of the models use the attention mechanism to build interactions among the passage, the question, and the choice bibref17, bibref3, bibref18, bibref19. the most competitive models are bibref17, bibref3, and both of them employ concatenation fusion to integrate the information. bibref17 utilizes choice - aware passage and choice - aware question to fuse the choice in word level. in addition, they apply the question - aware passage to fuse the passage in context level. different from bibref17, both the choice - aware passage and choice - aware question are fused into choice in the context level in bibref3","SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)",0.09895118325948715,0.3693041205406189
3aee5c856e0ee608a7664289ffdd11455d153234,What was the performance of their model?,,"For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81",0.036969345062971115,0.07968655973672867
3b391cd58cf6a61fe8c8eff2095e33794e80f0e3,What is the dataset used in the paper?,historical s & p 500 component stocks,"historical S&P 500 component stocks
 306242 news articles",0.07055966556072235,0.8527532815933228
3b745f086fb5849e7ce7ce2c02ccbde7cfdedda5,By how much do they outperform other models in the sentiment in intent classification tasks?,6 to 8,In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average,0.793289840221405,0.28077879548072815
3b995a7358cefb271b986e8fc6efe807f25d60dc,What types of word representations are they evaluating?,distributed,GloVE; SGNS,0.2822335660457611,0.15589535236358643
3bfdbf2d4d68e01bef39dc3371960e25489e510e,how do they measure discussion quality?,stepwise regressions,"Measuring three aspects: argumentation, specificity and knowledge domain.",0.414193719625473,0.07285472750663757
3d49b678ff6b125ffe7fb614af3e187da65c6f65,"What does ""explicitly leverages their probabilistic correlation to guide the training process of both models"" mean?",regularization term,"The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization.",0.14820800721645355,0.37998825311660767
3d6015d722de6e6297ba7bfe7cb0f8a67f660636,What are the 12 categories devised?,,"Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study",0.05985317751765251,0.04471996799111366
3e839783d8a4f2fe50ece4a9b476546f0842b193,What was their result on Stance Sentiment Emotion Corpus?,,F1 score of 66.66%,0.05361192300915718,0.14486125111579895
3f326c003be29c8eac76b24d6bba9608c75aa7ea,What evaluation metric is used?,weighted - f1,F1 and Weighted-F1,0.05893460288643837,0.9427905082702637
3f3c09c1fd542c1d9acf197957c66b79ea1baf6e,How many annotators participated?,how many annotators participated?,1,0.02911621518433094,0.24531115591526031
3f5f74c39a560b5d916496e05641783c58af2c5d,How are the synthetic examples generated?,randomly dropping words,"Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out",0.2460474818944931,0.5021711587905884
3fb4334e5a4702acd44bd24eb1831bb7e9b98d31,How big are the datasets used?,,"Evaluation datasets used:
CMRC 2018 - 18939 questions, 10 answers
DRCD - 33953 questions, 5 answers
NIST MT02/03/04/05/06/08 Chinese-English - Not specified

Source language train data:
SQuAD - Not specified",0.06759124249219894,0.09740070253610611
405964517f372629cda4326d8efadde0206b7751,How is performance measured?,brier score bibref14,they use ROC curves and cross-validation,0.1574789136648178,0.13118433952331543
40c2bab4a6bf3c0628079fcf19e8b52f27f51d98,How do they generate the synthetic dataset?,generative process,using generative process,0.2354089915752411,0.9492223858833313
415f35adb0ef746883fb9c33aa53b79cc4e723c3,"In the targeted data collection approach, what type of data is targetted?",,Gendered characters in the dataset,0.09133920073509216,0.14798495173454285
41d3ab045ef8e52e4bbe5418096551a22c5e9c43,what datasets were used?,"iwslt14 german - english, turkish - english, and wmt14 english - german","IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German",0.8494536280632019,0.9743741154670715
41e300acec35252e23f239772cecadc0ea986071,What neural machine translation models can learn in terms of transfer learning?,mapping from data,Multilingual Neural Machine Translation Models,0.03311406448483467,0.04460683837532997
4226a1830266ed5bde1b349205effafe7a0e2337,What meta-information is being transferred?,relation - specific,"high-order representation of a relation, loss gradient of relation meta",0.06817576289176941,0.36784377694129944
42394c54a950bae8cebecda9de68ee78de69dc0d,What is the source of external knowledge?,what is the source of external knowledge?,counts of predicate-argument tuples from English Wikipedia,0.0590810626745224,0.049836426973342896
427252648173c3ba78c211b86fa89fc9f4406653,What domains are detected in this paper?,,"Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW),
“Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). Exceptionally, GEN does
not have a natural root category.",0.20466960966587067,0.08903847634792328
43f86cd8aafe930ebb35ca919ada33b74b36c7dd,In what way is the input restructured?,conditioning it on the entity,"In four entity-centric ways - entity-first, entity-last, document-level and sentence-level",0.11428090929985046,0.4090288281440735
445e792ce7e699e960e2cb4fe217aeacdd88d392,How do this framework facilitate demographic inference from social media?,"employing statistical techniques for fusing heterogeneous sets of features obtained by processing visual, textual and user interaction data",Demographic information is predicted using weighted lexicon of terms.,0.10067243874073029,0.33192384243011475
44c4bd6decc86f1091b5fc0728873d9324cdde4e,How big is the Japanese data?,,"7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus",0.05735677853226662,0.1679346263408661
4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb,what amounts of size were used on german-english?,"1, 2 and 8","Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development",0.4808291792869568,0.11612029373645782
45893f31ef07f0cca5783bd39c4e60630d6b93b3,How do they select monotonicity facts?,"reasoning over surface forms ( as opposed to translating to symbolic representations ) using a small inventory of monotonicity facts about quantifiers, lexical items and token - level polarity bibref17",They derive it from Wordnet,0.12145276367664337,0.21143245697021484
45a2ce68b4a9fd4f04738085865fbefa36dd0727,what dataset was used?,english,The dataset from a joint ADAPT-Microsoft project,0.04910062253475189,0.10410607606172562
4640793d82aa7db30ad7b88c0bf0a1030e636558,what previous systems were compared to?,published state of the art results without additional labeled data or task specific gazetteers,"Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Søgaard and Goldberg (2016) ",0.3842260539531708,0.1832815557718277
4688534a07a3cbd8afa738eea02cc6981a4fd285,How do they combine MonaLog with BERT?,hybrid model,They use Monalog for data-augmentation to fine-tune BERT on this task,0.11874754726886749,0.1399393230676651
4704cbb35762d0172f5ac6c26b67550921567a65,By how much does transfer learning improve performance on this task?,optimal performance,"In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%",0.06425122171640396,0.37994787096977234
471d624498ab48549ce492ada9e6129da05debac,What context modelling methods are evaluated?,"13 different context modeling methods upon the same parser, including 6 methods introduced in section secref2 and 7 selective combinations of them","Concat
Turn
Gate
Action Copy
Tree Copy
SQL Attn
Concat + Action Copy
Concat + Tree Copy
Concat + SQL Attn
Turn + Action Copy
Turn + Tree Copy
Turn + SQL Attn
Turn + SQL Attn + Action Copy",0.13472609221935272,0.1936662346124649
496b4ae3c0e26ec95ff6ded5e6790f24c35f0f5b,How do they incorporate human advice?,we go beyond and request the human for advice,by converting human advice to first-order logic format and use as an input to calculate gradient,0.3312510848045349,0.4371860921382904
4a4616e1a9807f32cca9b92ab05e65b05c2a1bf5,What were the sizes of the test sets?,what were the sizes of the test sets?,Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences,0.09672880172729492,0.5212398767471313
4a61260d6edfb0f93100d92e01cf655812243724,Which 3 NLP areas are cited the most?,"sentiment analysis, information extraction, document summarization, spoken dialogue, cross lingual ( research ), dialogue, systems, language generation","machine translation, statistical machine, sentiment analysis",0.5312498211860657,0.671069324016571
4b41f399b193d259fd6e24f3c6e95dc5cae926dd,How big dataset is used for training this system?,"how big dataset is used for training this system? since there are already convolutional neural networks ( cnns ) trained on large datasets to represent images with an outstanding performance, we make use of transfer learning to integrate a pre - trained model into our algorithm. in particular, we use a resnet - 101 bibref23 model trained on imagenet. we discard the last 2 layers, since these layers classify the image into categories and we only need to extract its features. datasets one of the first requirements to develop an architecture using a machine learning approach is a training dataset. the lack of open - source datasets containing dialogues from reminiscence therapy lead as to use a dataset with content similar to the one used in the therapy. in particular, we use two types of datasets to train our models : a dataset that maps pictures with questions, and an open - domain conversation dataset. the details of the two datasets are as follows. datasets : : : persona - chat and cornell - movie corpus we use two datasets to train our chatbot model. the first one is the persona - chat bibref15","For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.",0.07060518115758896,0.38849928975105286
4b8257cdd9a60087fa901da1f4250e7d910896df,How do the authors define or exemplify 'incorrect words'?,table tabref24 exemplifies a complete and its respective incomplete sentences with different tts - stt combinations,typos in spellings or ungrammatical words,0.023595156148076057,0.24099329113960266
4b8a0e99bf3f2f6c80c57c0e474c47a5ee842b2c,To what other competitive baselines is this approach compared?,previous state - of - the - art models vhred ( attn ) and reranking - rl,"LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL",0.214255690574646,0.5406401753425598
4c50f75b1302f749c1351de0782f2d658d4bea70,How is quality of annotation measured?,"quality control and results to control the quality, we ensured that a single annotator annotates maximum 120 headlines",Annotators went through various phases to make sure their annotations did not deviate from the mean.,0.1429595649242401,0.49198195338249207
4c7ac51a66c15593082e248451e8f6896e476ffb,What is the performance proposed model achieved on AlgoList benchmark?,state - of - the - art,"Full Testing Set Accuracy: 84.02
Cleaned Testing Set Accuracy: 93.48",0.24758730828762054,0.13833871483802795
4c822bbb06141433d04bbc472f08c48bc8378865,How do they extract causality from text?,"text mining procedures bibref17, bibref18, bibref19 to better infer causal influence from data. in the cognitive sciences, the famous perception experiments of michotte et al. led to a long line of research exploring the cognitive biases that humans possess when attempting to link cause and effect bibref20, bibref21, bibref22. results we have collected approximately 1m causal statements made on twitter over the course of 2013, and for a control we gathered the same number of statements selected at random but controlling for time of year ( see methods ). we applied parts - of - speech ( pos ) and named entity ( ne ) taggers","They identify documents that contain the unigrams 'caused', 'causing', or 'causes'",0.13715912401676178,0.39409101009368896
4ca0d52f655bb9b4bc25310f3a76c5d744830043,How large is the first dataset?,"how large is the first dataset? datasets : : : augmentation nlp tasks require plenty of data. due to the relatively small number of samples in our datasets, we added more labeled data using a technique developed in bibref7 that was used by the winning team in kaggle ' s toxic comment classification challenge bibref8. the augmented datasets are similar to the original data files, but include additional machine - computed utterances for each original utterance. we created the additional utterances using the google translate api. each original utterance was first translated from english into three target languages ( german, french, and italian ), and then translated back into english. the resulting utterances were included together in the same object with the original utterance. these “ duplex translations ” can sometimes result in the original sentence, but many times variations are generated that convey the same emotions. table shows an example utterance ( labeled with “ joy ” ) after augmentation. datasets the two datasets used for the challenge are friends and emotionpush, part of the emotionlines corpus bibref4. the datasets contain english - language dialogues of varying lengths. for the competition, we provided 1, 000 labeled dialogues from each dataset for training, and 240 unlabeled dialogues from each dataset for evaluation. the friends dialogues are scripts taken from the american tv sitcom ( 1994 - 2004 ). the emotionpush dialogues are from facebook messenger chats by real users which have been anonymized to ensure user privacy. for both datasets, dialogue lengths range from 5 to 24 lines each. a breakdown of the lengths of the dialogues is shown in table. we employed workers using amazon mechanical turk ( aka amt or mturk ) to annotate the dialogues bibref5. each complete dialogue was offered as a single mturk human intelligence task ( hit ), within which each utterance was read and annotated for emotions by the worker. each hit was assigned to five workers. to ensure workers were qualified for the annotation task, we set up a number of requirements : workers had to be from an english - speaking country ( australia, canada, great britain, ireland, new zealand, or the us ), have a high hit approval rate ( at least 98 % ), and have already performed a minimum of 2, 000 hits. in the datasets, each utterance is accompanied by an annotation",1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,0.1058933436870575,0.5239979028701782
4d5e2a83b517e9c082421f11a68a604269642f29,how many domains did they experiment with?,two,2,0.07236120104789734,0.8621610999107361
4dad15fee1fe01c3eadce8f0914781ca0a6e3f23,How do they prevent the model complexity increasing with the increased number of slots?,sim has many fewer parameters than existing dialogue state tracking models,They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).,0.13739535212516785,0.4631739854812622
4e9684fd68a242cb354fa6961b0e3b5c35aae4b6,What is the results of multimodal compared to unimodal models?,,"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4 ",0.0656149759888649,0.08086389303207397
4ef2fd79d598accc54c084f0cca8ad7c1b3f892a,What is the size of their collected dataset?,30 hours,3347 unique utterances ,0.35069799423217773,0.1358451098203659
5067e5eb2cddbb34b71e8b74ab9210cd46bb09c5,Which matching features do they employ?,"bibref5, bibref9",Matching features from matching sentences from various perspectives.,0.812768280506134,0.1626676470041275
50716cc7f589b9b9f3aca806214228b063e9695b,What language technologies have been introduced in the past?,solar panels or new agricultural tools,"- Font & Keyboard
- Speech-to-Text
- Text-to-Speech
- Text Prediction
- Spell Checker
- Grammar Checker
- Text Search
- Machine Translation
- Voice to Text Search
- Voice to Speech Search",0.2547014057636261,0.14156579971313477
51aaec4c511d96ef5f5c8bae3d5d856d8bc288d3,What baselines did they compare their model with?,"what baselines did they compare their model with? quantitative evaluation table tabref28 shows the performance of the models considered in our evaluation on both test sets. the next two sections discuss the results in detail. first, we can observe that the final model “ ours with mask and ordered triplets ” outperforms the baseline and ablation models",the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search,0.05061540752649307,0.18684521317481995
51de39c8bad62d3cbfbec1deb74bd8a3ac5e69a8,Which modifications do they make to well-established Seq2seq architectures?,,"Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible",0.06367842108011246,0.06765782833099365
521a7042b6308e721a7c8046be5084bc5e8ca246,What is a confusion network or lattice?,lattices,"graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences",0.10014978796243668,0.225688636302948
52e8f79814736fea96fd9b642881b476243e1698,What systems are tested?,x1 and x2 systems,"BULATS i-vector/PLDA
BULATS x-vector/PLDA
VoxCeleb x-vector/PLDA
PLDA adaptation (X1)
 Extractor fine-tuning (X2) ",0.201009601354599,0.2946130633354187
52f7e42fe8f27d800d1189251dfec7446f0e1d3b,How much better is performance of proposed method than state-of-the-art methods in experiments?,significantly outperforms state - of - art results on accuracy for three datasets,"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.",0.1814316213130951,0.42174211144447327
53a0763eff99a8148585ac642705637874be69d4,How does the active learning model work?,automatically deciding which instances should be labeled by annotators to train a model as quickly and effectively as possible,"Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.",0.21449963748455048,0.524777352809906
53bf6238baa29a10f4ff91656c470609c16320e1,What is the source of the textual data? ,facebook users,Users' tweets,0.21656964719295502,0.509628415107727
540e9db5595009629b2af005e3c06610e1901b12,How was a quality control performed so that the text is noisy but the annotations are accurate?,,The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.,0.09077897667884827,0.05683046206831932
5455b3cdcf426f4d5fc40bc11644a432fa7a5c8f,What do they mean by answer styles?,,well-formed sentences vs concise answers,0.0623430572450161,0.11389269679784775
545ff2f76913866304bfacdb4cc10d31dbbd2f37,What data were they used to train the multilingual encoder?,,WMT 2014 En-Fr parallel corpus,0.036205220967531204,0.19491860270500183
54c7fc08598b8b91a8c0399f6ab018c45e259f79,How better is performance compared to competitive baselines?,significantly outperforms,"Proposed method vs best baseline result on Vecmap (Accuracy P@1):
EN-IT: 50 vs 50
IT-EN: 42.67 vs 42.67
EN-DE: 51.6 vs 51.47
DE-EN: 47.22 vs 46.96
EN-FI: 35.88 vs 36.24
FI-EN: 39.62 vs 39.57
EN-ES: 39.47 vs 39.30
ES-EN: 36.43 vs 36.06",0.33047598600387573,0.2603624165058136
54c9147ffd57f1f7238917b013444a9743f0deb8,Which are the sequence model architectures this method can be transferred across?,"lstm - based, the cnn - based, and the transformer - based",The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models,0.35988524556159973,0.5638922452926636
54fa5196d0e6d5e84955548f4ef51bfd9b707a32,"Are this techniques used in training multilingual models, on what languages?",mixing language pairs in a predetermined fashion,English to French and English to German,0.1621420681476593,0.44672784209251404
54fe8f05595f2d1d4a4fd77f4562eac519711fa6,How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?,"the exponential increase of interactions on the various social media platforms has generated the huge amount of data on social media platforms like facebook and twitter, etc. these interactions resulted not only positive effect but also negative effect over billions of people owing to the fact that there are lots of aggressive comments ( like hate, anger, and bullying ). these cause not only mental and psychological stress but also account deactivation and even suicidebibref1. in this paper we concentrate on problems related to aggressiveness. the fine - grained definition of the aggressiveness / aggression identification is provided by the organizers of trac - 2018 bibref0, bibref2. they have classified the aggressiveness into three labels ( overtly aggressive ( oag ), covertly aggressive ( cag ), non - aggressive ( nag ) ). the detailed description for each of the three labels is described as follows : the massive increase of the social media data rendered the manual methods of content moderation difficult and costly. machine learning and deep learning methods to identify such phenomena have attracted more attention to the research community in recent yearsbibref4. based on the current context, we can divide the problem into three sub - problems : ( a ) detection of aggression levels, ( b ) handling code - mixed data and ( c ) handling",Systems do not perform well both in Facebook and Twitter texts,0.023398596793413162,0.31904298067092896
551f77b58c48ee826d78b4bf622bb42b039eca8c,What are the weaknesses of their proposed interpretability quantification method?,"what are the weaknesses of their proposed interpretability quantification method?. an alternative approach was proposed in bibref26, where interpretability was quantified by the degree of clustering around embedding dimensions and orthogonal transformations were examined to increase interpretability while preserving the performance of the embedding. note, however, that it was shown in bibref26 that total interpretability of an embedding is constant under any orthogonal transformation and it can only be redistributed across the dimensions. with a similar motivation to bibref26, bibref27 proposed rotation algorithms based on exploratory factor analysis ( efa ) to preserve the expressive performance of the original word embeddings while improving their interpretability. in bibref27, interpretability was calculated using a distance ratio ( dr ) metric that is effectively proportional to the metric used in bibref26 our interpretability measurements are based on our proposed dataset semcat, which was designed to be a comprehensive dataset that contains a diverse set of word categories. yet, it is possible that the precise interpretability scores that are measured here are biased by the dataset used. in general, two main properties of the dataset can affect the results : category selection and within - category word selection. to examine the effects of these properties on interpretability evaluations, we create alternative datasets by varying both category selection and word selection for semcat. since semcat is comprehensive in terms of the words it contains for the categories, these datasets are created by subsampling the categories and words included in semcat. however, we claim that a dataset with a sufficiently large number of categories can still provide a good approximation to human judgements",can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories,0.024687178432941437,0.21511094272136688
55588ae77496e7753bff18763a21ca07d9f93240,What are the characteristics of the rural dialect?,"small, scattered population",It uses particular forms of a concept rather than all of them uniformly,0.5797726511955261,0.13506796956062317
564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee,Which languages do they validate on?,,"Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur",0.03681905195116997,0.16079886257648468
5712a0b1e33484ebc6d71c70ae222109c08dede2,What benchmark datasets they use?,,VQA and GeoQA,0.07594414800405502,0.1896006166934967
572458399a45fd392c3a4e07ce26dcff2ad5a07d,How much more accurate is the model than the baseline?,81 %,"For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. ",0.0951760858297348,0.11220578104257584
57388bf2693d71eb966d42fa58ab66d7f595e55f,How is morphology knowledge implemented in the method?,modified morfessor,A BPE model is applied to the stem after morpheme segmentation.,0.619890570640564,0.28952983021736145
579941de2838502027716bae88e33e79e69997a6,What is difference in peformance between proposed model and state-of-the art on other question types?,slightly eclipses the current state - of - the - art results,"For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.
For number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. 
For date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.",0.20603327453136444,0.2493448406457901
58a340c338e41002c8555202ef9adbf51ddbb7a1,Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?,sst - 2,SST-2 dataset,0.838322639465332,0.7494362592697144
58edc6ed7d6966715022179ab63137c782105eaf,Which one of the four proposed models performed best?,lft,the hybrid model MinAvgOut + RL,0.8083029985427856,0.1276373714208603
58f50397a075f128b45c6b824edb7a955ee8cba1,How many shared layers are in the system?,how many shared layers are in the system?,1,0.06813152134418488,0.2306663990020752
5908d7fb6c48f975c5dfc5b19bb0765581df2b25,How big is the dataset?,,Resulting dataset was 7934 messages for train and 700 messages for test.,0.046660229563713074,0.11209341883659363
593e307d9a9d7361eba49484099c7a8147d3dade,What are causal attribution networks?,separate efforts to map out the underlying or latent causal attribution network held collectively by humans,"networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans",0.6346065998077393,0.6620741486549377
5a0841cc0628e872fe473874694f4ab9411a1d10,By how much did they outperform the other methods?,"by how much did they outperform the other methods?. and we also get another observation that, although our proposed stc inlineform1 - le and stc inlineform2 - lpi outperform both bow based and recnn based approaches across all three datasets","on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI",0.14538179337978363,0.3610394299030304
5a23f436a7e0c33e4842425cf86d5fd8ba78ac92,How big is dataset used?,public financial news dataset released by bibref4,"553,451 documents",0.0576290488243103,0.1478557139635086
5a33ec23b4341584a8079db459d89a4e23420494,What is public dashboard?,leaderboard,"Public dashboard where competitors can see their results during competition, on part of the test set (public test set).",0.17865006625652313,0.3876805901527405
5b2480c6533696271ae6d91f2abe1e3a25c4ae73,Is the assumption that natural language is stationary and ergodic valid?,an assumption that we follow.,It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement.,0.09174603223800659,0.23611454665660858
5b6aec1b88c9832075cd343f59158078a91f3597,How does proposed word embeddings compare to Sindhi fastText word representations?,compared with recently revealed sdfasttext word representations,"Proposed SG model vs SINDHI FASTTEXT:
Average cosine similarity score: 0.650 vs 0.388
Average semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391",0.03720657527446747,0.44429194927215576
5bcc12680cf2eda2dd13ab763c42314a26f2d993,What evaluation metrics were used in the experiment?,what evaluation metrics were used in the experiment?,"For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy",0.04290130361914635,0.4029976725578308
5be94c7c54593144ba2ac79729d7545f27c79d37,What is the challenge for other language except English,,not researched as much as English,0.04845030978322029,0.11691761761903763
5c4c8e91d28935e1655a582568cc9d94149da2b2,Does DCA or GMM-based attention perform better in experiments?,dca,About the same performance,0.2544558346271515,0.06894585490226746
5c90e1ed208911dbcae7e760a553e912f8c237a5,How big are the datasets?,how big are the datasets?,"In-house dataset consists of  3716 documents 
ACE05 dataset consists of  1635 documents",0.1043030396103859,0.5264124870300293
5c95808cd3ee9585f05ef573b0d4a52e86d04c60,Which journal and conference are cited the most in recent years?,top - tier conference,CL Journal and EMNLP conference,0.07518205046653748,0.4635680615901947
5d9b088bb066750b60debfb0b9439049b5a5c0ce,what processing was done on the speeches before being parsed?,"removed all numbers, punctuation marks, and stop words",Remove numbers and interjections,0.21069121360778809,0.5037057995796204
5dfa59c116e0ceb428efd99bab19731aa3df4bbd,How many natural language explanations are human-written?,three,Totally 6980 validation and test image-sentence pairs have been corrected.,0.34138956665992737,0.1619306057691574
5e5460ea955d8bce89526647dd7c4f19b173ab34,How many of the utterances are transcribed?,5 minutes each,Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned),0.16989514231681824,0.19544647634029388
5e65bb0481f3f5826291c7cc3e30436ab4314c61,What discourse features are used?,"rst discourse features, we process non - empty discourse relations also through either local or global reading. in the local reading, we read all the discourse relations in a sentence ( a row ) then move on to the next sentence. in the global reading, we read in discourse relations for one entity at a time. this results in sequences of discourse relations for the input entries. further, we found an input - length threshold for the discourse features to help ( section secref26 ). not surprisingly, discourse does not contribute on shorter texts. many of the feature grids are empty for these shorter texts – either there are no coreference chains or they are not correctly resolved. currently we only have empirical results on short novel chunks and movie reviews, but believe the finding would generalize to twitter or blog posts. cnn2 - pv. this model ( figure figref10, left + center ) featurizes discourse information into a vector of relation probabilities. in order to derive the discourse features, an entity grid is constructed by feeding the document through an nlp pipeline to identify salient entities. two flavors of discourse features are created by populating the entity grid with either ( i ) grammatical relations ( gr ) or ( ii ) rst discourse relations ( rst )",Entity grid with grammatical relations and RST discourse relations.,0.12615631520748138,0.6861443519592285
5e9732ff8595b31f81740082333b241d0a5f7c9a,How much better were results of the proposed models than base LSTM-RNN model?,"how much better were results of the proposed models than base lstm - rnn model? we also present diversity - 32 graphs ( figure figref16 ) and report diversity - auc as well as distinct - 1 and - 2 for each model ( table tabref25 ). we can see that all our models have significantly better sentence - level diversity than vhred, let alone lstm. for unigram diversity, they are also better than lstm, though hard to distinguish from vhred. both bigram and trigram graphs reveal that all models are more diverse than lstm, except that rl shows lower diversity than the other models, which agree with our f1 results. note that since our models are only trained based on unigram output distributions, the bigram and trigram diversities are still far away from that of the ground - truth, which points to future direction. that said, the table does show that encouraging unigram diversity can already have positive influence on higher grams as well. also note that the hybrid model ( last row ) does not achieve the best result in terms of diversity another consideration of avgout is that theoretically we can have two choices. the first is to use the output distributions when we are teacher - forcing ( i. e., only feeding ground - truth tokens ) ; the other is to let the model use its own predictions during greedy / beam - search decoding or sampling. we reason that the former is a much better estimation of the model ' s bias, because the latter will result in a cascading enlargement of the model bias due to the auto - regressive nature of lstm - rnn models ( i. e., the tokens fed to the decoder are themselves also polluted by the model ' s bias ). our early experimental results also agreed with the above reasoning. three models to leverage avgout avgout can play at least three roles. first, it can be used to directly supervise output distribution during training ; second, it can be used as a prior in labeled sequence transduction methods to control diversity of the generated response ; and third, it can be used as a reward signal for reinforcement learning to encourage diverse sampled responses. in this section, we begin with a base vanilla seq2seq model, and next present our three models to diversify responses based on avgout. our base model lstm is identical to that proposed by bibref",on diversity 6.87 and on relevance 4.6 points higher,0.09263288974761963,0.4203576147556305
5f9bd99a598a4bbeb9d2ac46082bd3302e961a0f,What are the models evaluated on?,ability to understand and reason over natural language,They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA),0.10383595526218414,0.14026108384132385
5fb348b2d7b012123de93e79fd46a7182fd062bd,What datasets are used to evaluate the approach?,nell - one and wiki - one,"NELL-One, Wiki-One",0.4510897994041443,0.9762066602706909
5fb6a21d10adf4e81482bb5c1ec1787dc9de260d,How do they quantify moral relevance?,fitted slope,By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence,0.1897154301404953,0.07068285346031189
602396d1f5a3c172e60a10c7022bcfa08fa6cbc9,By how much do they outperform BiLSTMs in Sentiment Analysis?,rcrn,Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets.,0.26102977991104126,0.529885470867157
61fb982b2c67541725d6db76b9c710dd169b533d,Is infinite-length sequence generation a result of training with maximum likelihood?,this amounts to running the full training sequence through a recurrent model and zeroing the loss for the first tokens,There are is a strong conjecture that it might be the reason but it is not proven.,0.012033759616315365,-0.049672167748212814
63723c6b398100bba5dc21754451f503cb91c9b8,What is the state of the art?,"models to the english language, at least in the monolingual setting. even though multilingual models give remarkable results, they are often larger and their results still lag behind their monolingual counterparts bibref12 experiments : : : discussion camembert displays improved performance compared to prior work for the 4 downstream tasks considered. this confirms the hypothesis that pretrained language models can be effectively fine - tuned for various downstream tasks, as observed for english in previous work. moreover, our results also show that dedicated monolingual models still outperform multilingual ones. we explain this point in two ways. first, the scale of data is possibly essential to the performance of camembert. indeed, we use 138gb of uncompressed text vs. 57gb for mbert. second, with more data comes more diversity in the pretraining distribution. reaching state - of - the - art performances on 4 different tasks and 6 different datasets requires robust pretrained models","POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)",0.2371937334537506,0.3331330418586731
63850ac98a47ae49f0f49c1c1a6e45c6c447272c,What is the problem with existing metrics that they are trying to address?,achieve decent performance mainly by extraction rather than abstraction,"Answer with content missing: (whole introduction) However, recent
studies observe the limits of ROUGE and find in
some cases, it fails to reach consensus with human.
judgment (Paulus et al., 2017; Schluter, 2017).",0.037817731499671936,-0.028197718784213066
6389d5a152151fb05aae00b53b521c117d7b5e54,What is typical GAN architecture for each text-to-image synhesis group?,conditional gan,"Semantic Enhancement GANs: DC-GANs, MC-GAN
Resolution Enhancement GANs: StackGANs, AttnGAN, HDGAN
Diversity Enhancement GANs: AC-GAN, TAC-GAN etc.
Motion Enhancement GAGs: T2S, T2V, StoryGAN",0.23039604723453522,0.4498421549797058
63c0128935446e26eacc7418edbd9f50cba74455,What is the size of the released dataset?,,"440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples.",0.0442892462015152,0.14335870742797852
6412e97373e8e9ae3aa20aa17abef8326dc05450,What baseline model is used?,bibref10,Human evaluators,0.25923970341682434,0.15726426243782043
6472f9d0a385be81e0970be91795b1b97aa5a9cf,Do they train a different training method except from scheduled sampling?,,"Answer with content missing: (list missing) 
Scheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.

Yes.",0.023438695818185806,0.10334200412034988
657edbf39c500b2446edb9cca18de2912c628b7d,What was their perplexity score?,6 points,Perplexity score 142.84 on dev and 138.91 on test,0.21099041402339935,0.21377450227737427
675f28958c76623b09baa8ee3c040ff0cf277a5a,What is the size of the dataset?,,"300,000 sentences with 1.5 million single-quiz questions",0.08623455464839935,0.13390575349330902
67ee7a53aa57ce0d0bc1a20d41b64cb20303f4b7,How much training data is used?,,"163,110,000 utterances",0.056679051369428635,0.0654829815030098
68794289ed6078b49760dc5fdf88618290e94993,What are proof paths?,,A sequence of logical statements represented in a computational graph,0.06064863130450249,0.12765510380268097
68e3f3908687505cb63b538e521756390c321a1c,What is the performance difference of using a generated summary vs. a user-written one?,work consistently better,2.7 accuracy points,0.03317122533917427,0.30655670166015625
6a9eb407be6a459dc976ffeae17bdd8f71c8791c,What is the reward model for the reinforcement learning appraoch?,"with rl, each turn receives a measurement of goodness called a reward","reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail",0.0471353679895401,0.6033234596252441
6b91fe29175be8cd8f22abf27fb3460e43b9889a,what genres do they songs fall under?,"blues, rap, metal, folk, r & b, reggae, country, and religious","Gospel, Sertanejo, MPB, Forró, Pagode, Rock, Samba, Pop, Axé, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda",0.8439761400222778,0.565889835357666
6baf5d7739758bdd79326ce8f50731c785029802,Which four languages do they experiment with?,english and chinese,"German, English, Italian, Chinese",0.46857208013534546,0.7336955070495605
6ce057d3b88addf97a30cb188795806239491154,What models are included in baseline benchmarking results?,what models are included in baseline benchmarking results?,"BERT, XLNET RoBERTa, ALBERT, DistilBERT",0.09808335453271866,0.1015806645154953
6dcbe941a3b0d5193f950acbdc574f1cfb007845,What are the domains covered in the dataset?,,"Alarm
Bank
Bus
Calendar
Event
Flight
Home
Hotel
Media
Movie
Music
RentalCar
Restaurant
RideShare
Service
Travel
Weather",0.030662916600704193,0.1646762639284134
6e97c06f998f09256be752fa75c24ba853b0db24,How do the authors measure performance?,,Accuracy across six datasets,0.0415375754237175,0.12133104354143143
6f2118a0c64d5d2f49eee004d35b956cb330a10e,What datasets are used for training/testing models? ,table tabref11,"Microsoft Research dataset containing movie, taxi and restaurant domains.",0.1885295808315277,0.07066463679075241
6f2f304ef292d8bcd521936f93afeec917cbe28a,How much improvement is gained from the proposed approaches?,,It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.,0.04974751174449921,0.08293557167053223
707db46938d16647bf4b6407b2da84b5c7ab4a81,How much F1 was improved after adding skip connections?,,"Simple Skip improves F1 from 74.34 to 74.81
Transformer Skip improes F1 from 74.34 to 74.95 ",0.08372379094362259,0.06521934270858765
7182f6ed12fa990835317c57ad1ff486282594ee,How does the SCAN dataset evaluate compositional generalization?,strong compositional generalization performance,"it systematically holds out inputs in the training set containing basic primitive verb, ""jump"", and tests on sequences containing that verb.",0.0315767303109169,0.23022295534610748
71d59c36225b5ee80af11d3568bdad7425f17b0c,How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?,"88. 59 % f1 score. these dnn models are also the state - of - the - art models. data and model : : : model based on state - of - the - art methods for ner, blstm - cnn - crf is the end - to - end deep neural network model that achieves the best result on f - score bibref9. therefore, we decide to conduct the experiment on this model and analyze the errors. we run experiment with the ma and hovy ( 2016 ) model bibref8, source code provided by ( motoki sato ) and analysis the errors from this result. before we decide to analysis on this result, we have run some other methods, but this one with vietnamese pre - trained word embeddings provided by kyubyong park obtains the best result. other results are shown in the table 2. error - analysis method the results of our analysis experiments are reported in precision and recall over all labels ( name of person, location, organization and miscellaneous ). the process of analyzing errors has 2 steps : step 1 : we use two state - of - the - art models including blstm - cnn - crf and blstm - crf to train and test on vlsp ’ s ner corpus. in our experiments, we implement word embeddings as features to the two systems. step 2 : based on the best results ( blstm - cnn - crf ), error analysis is performed based on five types of errors ( no extraction, no annotation, wrong range, wrong tag, wrong range and tag ), in a way similar to bibref10, but we analyze on both gold",Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ,0.07869178801774979,0.5007474422454834
728a55c0f628f2133306b6bd88af00eb54017b12,What geometric properties do embeddings display?,,Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.,0.061915040016174316,0.08285593241453171
7348e781b2c3755b33df33f4f0cab4b94fcbeb9b,"Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",,Only automatic methods,0.09573221951723099,0.2092762142419815
7358a1ce2eae380af423d4feeaa67d2bd23ae9dd,How do their train their embeddings?,using the embeddings train set,"The embeddings are learned several times using the training set, then the average is taken.",0.23061048984527588,0.655295729637146
73633afbefa191b36cca594977204c6511f9dad4,"Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?",,"Not at the moment, but summaries can be additionaly extended with this annotations.",0.03857952356338501,0.046473775058984756
737397f66751624bcf4ef891a10b29cfc46b0520,Which datasets are used in the paper?,"probabilistic models and data used, followed by comprehensive evaluations of our methodology. a three - tier modelling framework : : : lexical data for moral sentiment to ground moral sentiment in text, we leverage the moral foundations dictionary bibref27","Google N-grams
COHA
Moral Foundations Dictionary (MFD)
",0.22554290294647217,0.46615877747535706
73bb8b7d7e98ccb88bb19ecd2215d91dd212f50d,What human evaluation method is proposed?,,comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant,0.1028340682387352,0.025901464745402336
74091e10f596428135b0ab06008608e09c051565,How is knowledge stored in the memory?,curated knowledge bases,entity memory and relational memory.,0.10638179630041122,0.32418736815452576
74261f410882551491657d76db1f0f2798ac680f,What are the six target languages?,,"Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI).",0.0974021777510643,-0.021153762936592102
74cd51a5528c6c8e0b634f3ad7a9ce366dfa5706,What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?,facilitates interpretability assessments and comparisons among separate embedding spaces,it is less expensive and quantifies interpretability using continuous values rather than binary evaluations,0.14605334401130676,0.4778343439102173
74db8301d42c7e7936eb09b2171cd857744c52eb,How is the performance on the task evaluated?,"how is the performance on the task evaluated? model : : : pretraining and finetuning pretraining and successive fine - tuning of neural network models is a common approach for handling of low - resource settings in nlp. the idea is that certain properties of language can be learned either from raw text, related tasks, or related languages. technically, pretraining consists of estimating some or all model parameters on examples which do not necessarily belong to the final target task. fine - tuning refers to continuing training of such a model on a target task, whose data is often limited. while the sizes of the pretrained model parameters usually remain the same between the two phases, the learning rate or other details of the training regime, e. g., dropout, might differ. pretraining can be seen as finding a suitable initialization of model parameters, before training on limited amounts of task - or language - specific examples. experimental design : : : hyperparameters and data we mostly use the default hyperparameters by sharma - katrapati - sharma : 2018 : k18 - 30. in particular, all rnns have one hidden layer of size 100, and all input and output embeddings are 300 - dimensional. for optimization, we use adam bibref21. pretraining on the source language is done for exactly 50 epochs. to obtain our final models, we then fine - tune different copies of each pretrained model for 300 additional epochs for each target language. we employ dropout bibref22 with a coefficient of 0. 3 for pretraining and, since that dataset is smaller, with a coefficient of 0. 5 for fine - tuning. we make use of the datasets from the conll – sigmorphon 2018 shared task bibref9. the organizers provided a low, medium, and high setting for each language, with 100, 1000, and 10000 examples, respectively. for all l1 languages, we train our models on the high - resource datasets with 10000 examples. for fine - tuning, we use the low - resource datasets. outside the scope of the shared tasks, kann - etal - 2017 - one investigated cross - lingual transfer for morphological inflection, but was limited to a quantitative analysis. furthermore, that work experimented with a standard sequence - to - sequence model bibref12 in a multi - task training fashion bibref24, while we pre",Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors,0.10173123329877853,0.41887834668159485
753990d0b621d390ed58f20c4d9e4f065f0dc672,What is the seed lexicon?,negative words,a vocabulary of positive and negative predicates that helps determine the polarity score of an event,0.70823734998703,0.4626038372516632
75b69eef4a38ec16df63d60be9708a3c44a79c56,How much better peformance is achieved in human evaluation when model is trained considering proposed metric?,out of 4 ),"Pearson correlation to human judgement - proposed vs next best metric
Sample level comparison:
- Story generation: 0.387 vs 0.148
- Dialogue: 0.472 vs 0.341
Model level comparison:
- Story generation:  0.631 vs 0.302
- Dialogue: 0.783 vs 0.553",0.029765810817480087,0.09930514544248581
76377e5bb7d0a374b0aefc54697ac9cd89d2eba8,How do they obtain word lattices from words?,various word segmentations with different strategies,By considering words as vertices and generating directed edges between neighboring words within a sentence,0.19989612698554993,0.5484428405761719
78577fd1c09c0766f6e7d625196adcc72ddc8438,What dataset is used for train/test of this method?,training,Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset.,0.22776071727275848,0.3326919674873352
785eb3c7c5a5c27db14006ac357299ed1216313a,What they formulate the question generation as?,"basic question dataset ( bqd ), generated by our basic question generation algorithm",LASSO optimization problem,0.032842472195625305,0.09074000269174576
78c010db6413202b4063dc3fb6e3cc59ec16e7e3,What is different in the improved annotation protocol?,substantially more reliable performance evaluation of qa - srl parsers,a trained worker consolidates existing annotations ,0.18167532980442047,0.23653411865234375
7920f228de6ef4c685f478bac4c7776443f19f39,What language is the Twitter content in?,python,English,0.3935326039791107,0.35183781385421753
7994b4001925798dfb381f9aa5c0545cdbd77220,How do they perform data augmentation?,,They randomly sample sentences from Wikipedia that contains an object RC and add them to training data,0.10368919372558594,0.18438874185085297
7997b9971f864a504014110a708f215c84815941,"What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?",unavailability of suitable datasets and lexicons,"Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text",0.25377747416496277,0.37283360958099365
79a28839fee776d2fed01e4ac39f6fedd6c6a143,What is the main contribution of the paper? ,motivation,"Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance",0.09364690631628036,-0.024933064356446266
79bb1a1b71a1149e33e8b51ffdb83124c18f3e9c,What accuracy does CNN model achieve?,,Combined per-pixel accuracy for character line segments is 74.79,0.10707923769950867,0.13967624306678772
79ed71a3505cf6f5e8bf121fd7ec1518cab55cae,How do they damage different neural modules?,weights are corrupted,"Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.",0.135129913687706,0.3935976028442383
7a53668cf2da4557735aec0ecf5f29868584ebcf,What kind of instructional videos are in the dataset?,tutorial videos,tutorial videos for a photo-editing software,0.21077823638916016,0.7219469547271729
7a7e279170e7a2f3bc953c37ee393de8ea7bd82f,What two types the Chinese reading comprehension dataset consists of?,automatically generated training set and human - annotated validation and test set,cloze-style reading comprehension and user query reading comprehension questions,0.4769825041294098,0.06077830120921135
7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed,Is ROUGE their only baseline?,our first baseline is rouge - l bibref1,"No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.",0.18476589024066925,0.4789309501647949
7af01e2580c332e2b5e8094908df4e43a29c8792,How was lexical diversity measured?,aggregated all worker responses to a particular question into a single list corresponding to that question,By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions,0.32999029755592346,0.6140822768211365
7b44bee49b7cb39cb7d5eec79af5773178c27d4d,How is the data in RAFAEL labelled?,"polish text - based, open - domain, factoid question answering. it means that provided questions, knowledge base and returned answers are expressed in polish and may belong to any domain","Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner",0.06409524381160736,0.18997684121131897
7b89515d731d04dd5cbfe9c2ace2eb905c119cbc,Which is the baseline model?,i - vector model,"The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. ",0.38976049423217773,0.3774372339248657
7cf726db952c12b1534cd6c29d8e7dfa78215f9e,What is a word confusion network?,cnets,It is a network used to encode speech lattices to maintain a rich hypothesis space.,0.8202440142631531,0.22832736372947693
7d3c036ec514d9c09c612a214498fc99bf163752,What is the source of the dataset?,online sites that were tagged as fake news sites by the non - profit independent media fact - checking organization verafiles and the national union of journalists in the philippines ( nujp ). real articles were sourced from mainstream news websites in the philippines,"Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera",0.039961766451597214,0.8341991305351257
7d59374d9301a0c09ea5d023a22ceb6ce07fb490,How do they measure the diversity of inferences?,experiments on the event2mind and atomic dataset show that our proposed approach outperforms baseline methods,by number of distinct n-grams,0.12767471373081207,0.05036493390798569
7f9bc06cfa81a4e3f7df4c69a1afef146ed5a1cf,What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?,significant drop in bert ’ s performance,"10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%
50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%",0.06398016214370728,0.28864461183547974
8051927f914d730dfc61b2dc7a8580707b462e56,What baseline algorithms were presented?,baseline1 : sentence - level prediction,"a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm",0.07359594851732254,0.6654350757598877
81064bbd0a0d72a82d8677c32fb71b06501830a0,By how much is precission increased?,as it approaches 1,"ROUGE-1 increases by 0.05, ROUGE-2 by 0.06 and ROUGE-L by 0.09",0.34155967831611633,0.23489966988563538
81cee2fc6edd9b7bc65bbf6b4aa35782339e6cff,What type of documents are supported by the annotation platform?,regulatory filings and property lease agreements,"Variety of formats supported (PDF, Word...), user can define content elements of document",0.8492704033851624,0.2346615195274353
81e8d42dad08a58fe27eea838f060ec8f314465e,What is the state-of-the art?,we have demonstrated the effectiveness of our approach in the context of summarization,neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder,0.03679043427109718,0.06712552905082703
8255f74cae1352e5acb2144fb857758dda69be02,How do they measure grammaticality?,corpora following cda,by calculating log ratio of grammatical phrase over ungrammatical phrase,0.11322502046823502,0.21854367852210999
82a28c1ed7988513d5984f6dcacecb7e90f64792,How big are negative effects of proposed techniques on high-resource tasks?,,The negative effects were insignificant.,0.039992839097976685,0.08074992150068283
83f14af3ccca4ab9deb4c6d208f624d1e79dc7eb,Which of the two ensembles yields the best performance?,avg,Answer with content missing: (Table 2) CONCAT ensemble,0.1190938875079155,0.19941112399101257
8427988488b5ecdbe4b57b3813b3f981b07f53a5,On which task does do model do best?,author profiling,Variety prediction task,0.18141920864582062,0.20068566501140594
8434974090491a3c00eed4f22a878f0b70970713,How big is their model?,,Proposed model has 1.16 million parameters and 11.04 MB.,0.07058655470609665,0.028011785820126534
8568c82078495ab421ecbae38ddd692c867eac09,How many layers of self-attention does the model have?,,"1, 4, 8, 16, 32, 64",0.039972420781850815,0.10326885432004929
85e45b37408bb353c6068ba62c18e516d4f67fe9,What is the baseline?,multi - task architecture,The baseline is a multi-task architecture inspired by another paper.,0.2653377652168274,0.6520823240280151
8602160e98e4b2c9c702440da395df5261f55b1f,What are the three datasets used in the paper?,"age, dialect, and gender",Data released for APDA shared task contains 3 datasets.,0.9520152807235718,0.07798224687576294
863d5c6305e5bb4b14882b85b6216fa11bcbf053,What are the 12 AV approaches which are examined?,,"MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD",0.03938879072666168,0.1881200224161148
86cd1228374721db67c0653f2052b1ada6009641,What domain does the dataset fall into?,,YouTube videos,0.05243552103638649,0.3190540075302124
880a76678e92970791f7c1aad301b5adfc41704f,What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?,context classifiers,"ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.
NLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing ""retweets"", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation.",0.4107630252838135,0.26310285925865173
894c086a2cbfe64aa094c1edabbb1932a3d7c38a,What are the state-of-the-art systems?,semeval 2016 task 6 and the systems of bibref15,"For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN",0.22998613119125366,0.17225472629070282
8951fde01b1643fcb4b91e51f84e074ce3b69743,How they evaluate their approach?,"how they evaluate their approach? experiments we evaluate all models in five low - resource ner settings across different languages. although the evaluation is performed for ner labeling, the proposed models are not restricted to the task of ner and can potentially be used for other tasks. the training for all models was performed with labels in the io format. the predicted labels for the test data were converted and evaluated in iob2 with the official conll evaluation script","They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise",0.024921637028455734,0.5621676445007324
8958465d1eaf81c8b781ba4d764a4f5329f026aa,What are the three measures of bias which are reduced in experiments?,"bibref0. on the other hand, the most popular metric for measuring bias is the weat statistic bibref1, which compares the cosine similarities between groups of words. however, weat has been recently shown to overestimate bias as a result of implicitly relying on similar frequencies for the target words bibref4, and bibref5","RIPA, Neighborhood Metric, WEAT",0.0822506695985794,0.302057147026062
8985ead714236458a7496075bc15054df0e3234e,What is the performance of the models on the tasks?,,"Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)",0.05135181546211243,0.05707668885588646
89d1687270654979c53d0d0e6a845cdc89414c67,How do they obtain human judgements?,crowdsourced,Using crowdsourcing ,0.7750070095062256,0.8893714547157288
8a0a51382d186e8d92bf7e78277a1d48958758da,How better is gCAS approach compared to other approaches?,close to other methods on critical - slots,"For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52",0.050312306731939316,0.16385969519615173
8a1d4ed00d31c1f1cb05bc9d5e4f05fe87b0e5a4,Who manually annotated the semantic roles for the set of learner texts?,two senior students majoring in applied linguistics,Authors,0.5024721026420593,0.17393091320991516
8a5254ca726a2914214a4c0b6b42811a007ecfc6,How much transcribed data is available for for Ainu language?,most of it is not transcribed and fully studied yet,Transcribed data is available for duration of 38h 54m 38s for 8 speakers.,0.2191963642835617,0.474945604801178
8a871b136ccef78391922377f89491c923a77730,What are the baseline state of the art models?,,"Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention",0.10232947766780853,0.09906734526157379
8ad815b29cc32c1861b77de938c7269c9259a064,What languages are represented in the dataset?,0. 002 % or more of the twitter corpus,"EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO",0.30894935131073,0.302043616771698
8c0a0747a970f6ea607ff9b18cfeb738502d9a95,What was the performance of both approaches on their dataset?,the performance on cn - celeb ( e ) is much worse,ERR of 19.05 with i-vectors and 15.52 with x-vectors,0.057071562856435776,0.1367744356393814
8c8a32592184c88f61fac1eef12c7d233dbec9dc,Are this models usually semi/supervised or unsupervised?,unsupervised,"Both supervised and unsupervised, depending on the task that needs to be solved.",0.9447557330131531,0.536259651184082
8d793bda51a53a4605c1c33e7fd20ba35581a518,what bottlenecks were identified?,what bottlenecks were identified?. actual use of this analysis was carried out for a speech solution developed for indian railway inquiry system to identify bottlenecks in the system before its actual launch,Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System.,0.09345687925815582,0.6052178144454956
8e2b125426d1220691cceaeaf1875f76a6049cbd,By how much do they improve the accuracy of inferences over state-of-the-art methods?,outperforms baseline methods,"ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.
On Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.",0.10770925879478455,0.2673041820526123
8e9de181fa7d96df9686d0eb2a5c43841e6400fa,"Is CRWIZ already used for data collection, what are the results?",,"Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.",0.024960942566394806,0.045397620648145676
8ea4bd4c1d8a466da386d16e4844ea932c44a412,What dataset do they use?,,A parallel corpus where the source is an English expression of code and the target is Python code.,0.05550391227006912,0.17730507254600525
8f87215f4709ee1eb9ddcc7900c6c054c970160b,how is quality measured?,how is quality measured? results in table tabref13 we compare the quality of unisent with the baseline - lexicon as well as with the gold standard lexicon for general domain data,Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.,0.02220364846289158,0.42216718196868896
90159e143487505ddc026f879ecd864b7f4f479e,How much of the ASR grapheme set is shared between languages?,,Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script.,0.07457786798477173,-0.008302777074277401
90bc60320584ebba11af980ed92a309f0c1b5507,How do they enrich the positional embedding with length information,"in the second approach, inspired by recent work in text summarization bibref11, we enrich the position encoding used by the transformer model with information representing the position of words with respect to the end of the target string",They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).,0.14175446331501007,0.25747618079185486
9299fe72f19c1974564ea60278e03a423eb335dc,What was the weakness in Hassan et al's evaluation design?,what was the weakness in hassan et al ' s evaluation design?,"MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set
",0.09527026116847992,0.33045533299446106
9349acbfce95cb5d6b4d09ac626b55a9cb90e55e,What are the citation intent labels in the datasets?,figure eight,"Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.",0.34230783581733704,0.09018038958311081
93b299acfb6fad104b9ebf4d0585d42de4047051,Which datasets are used?,,"ABSA SemEval 2014-2016 datasets
Yelp Academic Dataset
Wikipedia dumps",0.04510168358683586,0.12463133782148361
9447ec36e397853c04dcb8f67492ca9f944dbd4b,What is the dataset used as input to the Word2Vec algorithm?,one - hot vectors representing words,Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words,0.6869555115699768,0.2588328421115875
944d5dbe0cfc64bf41ea36c11b1d378c408d40b8,Which of the two speech recognition models works better overall on CN-Celeb?,,x-vector,0.02688436023890972,0.2782458961009979
94bee0c58976b58b4fef9e0adf6856fe917232e5,How much bigger is Switchboard-2000 than Switchboard-300 database?,how much bigger is switchboard - 2000 than switchboard - 300 database?,Switchboard-2000 contains 1700 more hours of speech data.,0.0471869595348835,0.643793523311615
94e0cf44345800ef46a8c7d52902f074a1139e1a,What web and user-generated NER datasets are used for the analysis?,,"MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC",0.09039980918169022,0.16093087196350098
954c4756e293fd5c26dc50dc74f505cc94b3f8cc,What are dilated convolutions?,skip some input values,Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.,0.21036982536315918,0.37231406569480896
9555aa8de322396a16a07a5423e6a79dcd76816a,By how much does their model outperform both the state-of-the-art systems?,by how much does their model outperform both the state - of - the - art systems?,w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%,0.05048391595482826,0.5618622303009033
957bda6b421ef7d2839c3cec083404ac77721f14,What stylistic features are used to detect drunk texts?,"capitalisation, spelling errors","LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) 
 and Sentiment Ratio",0.679267168045044,0.20066668093204498
96be67b1729c3a91ddf0ec7d6a80f2aa75e30a30,What language do the agents talk in?,natural,English,0.5288458466529846,0.27007296681404114
96c09ece36a992762860cde4c110f1653c110d96,What was the result of the highest performing system?,,"For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.
For task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2",0.10057360678911209,0.06734392046928406
973f6284664675654cc9881745880a0e88f3280e,What proficiency indicators are used to the score the utterances?,,"6 indicators:
- lexical richness
- pronunciation and fluency
- syntactical correctness
- fulfillment of delivery
- coherence and cohesion
- communicative, descriptive, narrative skills",0.10318955034017563,0.07964528352022171
98515bd97e4fae6bfce2d164659cd75e87a9fc89,What is the source of the user interaction data? ,,Sociability from ego-network on Twitter,0.07161270827054977,0.2284589558839798
98785bf06e60fcf0a6fe8921edab6190d0c2cec1,How do they determine which words are informative?,favors rare or informative words,Informative are those that will not be suppressed by regularization performed.,0.08591925352811813,0.3445032238960266
98eb245c727c0bd050d7686d133fa7cd9d25a0fb,Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?,,BLEU scores,0.04867255315184593,0.18872986733913422
993b896771c31f3478f28112a7335e7be9d03f21,What novel class of recurrent-like networks is proposed?,,"A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state.",0.02794932946562767,0.0906667411327362
999b20dc14cb3d389d9e3ba5466bc3869d2d6190,What is the latest paper covered by this survey?,corpus and evaluation metrics before examining specific neural models,Kim et al. (2019),0.3969184160232544,0.028403107076883316
9a596bd3a1b504601d49c2bec92d1592d7635042,What is the performance of their model?,,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220.,0.046573206782341,0.05482763797044754
9a65cfff4d99e4f9546c72dece2520cae6231810,What is the performance of proposed model on entire DROP dataset?,what is the performance of proposed model on entire drop dataset?,"The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev",0.0821046382188797,0.30016177892684937
9aa52b898d029af615b95b18b79078e9bed3d766,How faster is training and decoding compared to former models?,much faster,"Proposed vs best baseline:
Decoding: 8541 vs 8532 tokens/sec
Training: 8h vs 8h",0.38310471177101135,0.23930004239082336
9ab43f941c11a4b09a0e4aea61b4a5b4612e7933,What approach did previous models use for multi-span questions?,tag - based,Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span,0.23990504443645477,0.22710567712783813
9adcc8c4a10fa0d58f235b740d8d495ee622d596,How many additional task-specific layers are introduced?,"how many additional task - specific layers are introduced? overall, previous approaches to joint ner and re have experimented little with deep task - specificity, with the exception of those models that include additional layers for the re task. to our knowledge, no work has considered including additional ner - specific layers beyond scoring and / or output layers. this may reflect a residual influence of the pipeline approach in which the ner task must be solved first before additional layers are used to solve the re task. however, there is no a priori reason to think that the re task would benefit more from additional task - specific layers than the ner task. we also note that while previous work has tackled joint ner and re in variety of textual domains, in all cases the number of shared and task - specific parameters is held constant across these domains. the fact that the optimal number of task - specific layers differed between the two datasets demonstrates the value of taking the number of shared and task - specific layers to be a hyperparameter of our model architecture. as shown in table tabref17, the final hyperparameters used for the conll04 dataset included an additional re - specific birnn layer than did the final hyperparameters used for the ade dataset. we suspect that this is due to the limited number of relations and entities in the ade dataset. for most examples in this dataset, it is sufficient to correctly identify a single drug entity, a single adverse - effect entity, and an adverse - effect relation between the two entities. thus, the ner and re tasks for this dataset are more closely related than they are in the case of the conll04 dataset. intuitively, cases in which the ner and re problems can be solved by relying on more shared information should require fewer task - specific layers. the above approaches allow for very little task - specificity, since both the ner task and the re task are coerced into a single task. other approaches incorporate greater task - specificity in one of two ways. first, several models share the majority of model parameters between the ner and re tasks, but also have separate scoring and / or output layers used to produce separate outputs for each task. for example, katiyar and cardie katiyar - cardie - 2017 - going and bekoulis et al. bekoulis2018joint propose models in which token representations first pass through one or more shared bils",2 for the ADE dataset and 3 for the CoNLL04 dataset,0.08208580315113068,0.32839006185531616
9ae084e76095194135cd602b2cdb5fb53f2935c1,What metrics are used for evaluation?,,word error rate,0.0807962492108345,0.09845484048128128
9bcc1df7ad103c7a21d69761c452ad3cd2951bda,On which task does do model do worst?,author profiling,Gender prediction task,0.3218648433685303,0.29520004987716675
9c4a4dfa7b0b977173e76e2d2f08fa984af86f0e,How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?,"outperforms all existing models on both the full test set and the cleaned test set. ablation experiments with tp2lstm and lstm2tp show that, for this task, the tp - n2f decoder is more helpful than tp - n2f encoder. this may be because lisp codes rely more heavily on structure representations.. secref20 of the appendix ), we report both execution accuracy ( of the final multi - choice answer after running the execution engine ) and operation sequence accuracy ( where the generated operation sequence must match the ground truth sequence exactly ). tp - n2f is compared to a baseline provided by the seq2prog model in bibref16, an lstm - based seq2seq model with attention. our model outperforms both the original seq2prog, designated seq2prog - orig, and the best reimplemented seq2prog after an extensive hyperparameter search, designated seq2prog - best. table tabref16 presents the results. to verify the importance of the tp - n2f encoder and decoder, we conducted experiments to replace either the encoder with a standard lstm ( denoted lstm2tp ) or the decoder with a standard attentional lstm ( denoted tp2lstm ). we observe that both the tpr components of tp - n2f are important for achieving the observed performance gain relative to the baseline. tp - n2f model : : : inference and the learning strategy of the tp - n2f model during inference time, natural language questions are","Full Testing Set accuracy: 84.02
Cleaned Testing Set accuracy: 93.48",0.04622379317879677,0.2619258165359497
9c68d6d5451395199ca08757157fbfea27f00f69,Which OpenIE systems were used?,openie 4 bibref5,OpenIE4 and MiniIE,0.4947088360786438,0.6003108024597168
9d578ddccc27dd849244d632dd0f6bf27348ad81,What are the results?,the difference is that the first term makes the scores of the two events distant from each other,"Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. 
Using a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.",0.2189994603395462,0.12317472696304321
9d5df9022cc9eb04b9f5c5a9d8308a332ebdf50c,What is the new labeling strategy?,partition texts into those with pure and mixed sentiment orientations,They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations,0.47437745332717896,0.5974348187446594
9d9b11f86a96c6d3dd862453bf240d6e018e75af,How does counterfactual data augmentation aim to tackle bias?,"augmenting our dataset with a copy of every dialogue with gendered words swapped using the gendered word pair list provided by bibref21. for example, all instances of grandmother are swapped with grandfather. methodology : mitigating bias in generative dialogue : : : positive - bias data collection to create a more gender - balanced dataset, we collect additional data using a positive - bias data collection ( pos. data ) strategy. methodology : mitigating bias in generative dialogue we explore both data augmentation and algorithmic methods to mitigate bias in generative transformer dialogue models. we describe first our modeling setting and then the three proposed techniques for mitigating bias. using ( i ) counterfactual data augmentation bibref25 to swap gendered words and ( ii ) additional data collection with crowdworkers, we create a gender - balanced dataset. further, ( iii ) we describe a controllable generation method which moderates the male and female gendered words it produces. we use the dialogues in light because we find that it is highly imbalanced with respect to gender : there are over 60 % more male - gendered characters than female. we primarily address the discrepancy in the representation of male and female genders, although there are many characters that are gender neutral ( like “ trees "" ) or for which the gender could not be determined. we did not find any explicitly identified non - binary characters. we note that this is a bias in and of itself, and should be addressed in future work. we show that training on gender biased data leads existing generative dialogue models to amplify gender bias further. to offset this, we collect additional in - domain personas and dialogues to balance gender and increase the diversity of personas in the dataset. next, we combine this approach with counterfactual data augmentation and methods for controllable text generation to mitigate the bias in dialogue generation",The training dataset is augmented by swapping all gendered words by their other gender counterparts,0.0539279580116272,0.6465265154838562
9e04730907ad728d62049f49ac828acb4e0a1a2a,What were their performance results?,,"On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%",0.0926438719034195,0.17076972126960754
9ebb2adf92a0f8db99efddcade02a20a219ca7d9,How is the proficiency score calculated?,,"They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.",0.028073759749531746,0.07447483390569687
9ef182b61461d0d8b6feb1d6174796ccde290a15,Do they annotate their own dataset or use an existing one?,,Use an existing one,0.09898387640714645,0.11096769571304321
9fe4a2a5b9e5cf29310ab428922cc8e7b2fc1d11,What are state of the art methods MMM is compared to?,baselines,"FTLM++, BERT-large, XLNet",0.05021636188030243,0.11436669528484344
a02696d4ab728ddd591f84a352df9375faf7d1b4,How large is the Dialog State Tracking Dataset?,http : / / fb. ai / babi. we also give results on a proprietary dataset extracted from an online restaurant reservation concierge service with anonymized users,"1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs",0.013636692427098751,0.10086487233638763
a09633584df1e4b9577876f35e38b37fdd83fa63,"How is human evaluation performed, what was the criteria?",,Through Amazon MTurk annotators to determine plausibility and content richness of the response,0.05183841660618782,0.07122219353914261
a1064307a19cd7add32163a70b6623278a557946,How many uniue words are in the dataset?,,908456 unique words are available in collected corpus.,0.09949149191379547,0.0648551657795906
a24a7a460fd5e60d71a7e787401c68caa4702df6,What monolingual word representations are used?,cross - lingual,"AraVec for Arabic, FastText for French, and Word2vec Google News for English.",0.5337136387825012,0.3182448446750641
a2be2bd84e5ae85de2ab9968147b3d49c84dfb7f,What genres are covered?,"entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement","genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement",0.7971483469009399,0.9654855728149414
a379c380ac9f67f824506951444c873713405eed,What are the baselines?,,"CNN, LSTM, BERT",0.08282420784235,0.18584656715393066
a381ba83a08148ce0324b48b8ff35128e66f580a,what models did they compare to?,,"High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM ",0.0802430659532547,0.1252431720495224
a3d83c2a1b98060d609e7ff63e00112d36ce2607,How many sentence transformations on average are available per unique sentence in dataset?,,27.41 transformation on average of single seed sentence is available in dataset.,0.06672826409339905,0.16167424619197845
a3f108f60143d13fe38d911b1cc3b17bdffde3bd,what was their system's f1 performance?,what was their system ' s f1 performance?,"Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.",0.09655652195215225,0.43710851669311523
a48c6d968707bd79469527493a72bfb4ef217007,Which training dataset allowed for the best generalization to benchmark sets?,snli,MultiNLI,0.28096744418144226,0.5927703976631165
a4d8fdcaa8adf99bdd1d7224f1a85c610659a9d3,"When they say ""comparable performance"", how much of a performance drop do these new embeddings result in?",,"Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.",0.027769993990659714,0.054006971418857574
a4fe5d182ddee24e5bbf222d6d6996b3925060c8,which datasets were used in evaluation?,table tabref10,"CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0",0.05678341165184975,0.1650172770023346
a4ff1b91643e0c8a0d4cc1502d25ca85995cf428,Which two datasets does the resource come from?,two different surveys,two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor,0.4151241183280945,0.41692665219306946
a516b37ad9d977cb9d4da3897f942c1c494405fe,Which models do they try out?,,"DocQA, SAN, QANet, ASReader, LM, Random Guess",0.041247595101594925,0.15772229433059692
a56fbe90d5d349336f94ef034ba0d46450525d19,What DCGs are used?,definite clause grammars,Author's own DCG rules are defined from scratch.,0.9666784405708313,0.08791011571884155
a5b67470a1c4779877f0d8b7724879bbb0a3b313,what metrics are used in evaluation?,micro - averaged inlineform0,micro-averaged F1,0.9097594022750854,0.6234808564186096
a71ebd8dc907d470f6bd3829fa949b15b29a0631,how did they ask if a tweet was racist?,how did they ask if a tweet was racist?,"if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.",0.058033861219882965,0.37970873713493347
a7adb63db5066d39fdf2882d8a7ffefbb6b622f0,what was the baseline?,"what was the baseline? hamming loss is different in the sense that it is a loss and it is defined as the fraction of wrong labels to the total number of labels. hamming loss can be a good measurement when it comes to evaluating multi - label classifiers. the hamming loss is expressed as displayform0 where inlineform0 is number of documents, inlineform1 number of labels, inlineform2 is the target value and inlineform3 is predicted value. bibref7 for evaluation the inlineform0 and inlineform1 was calculated as defined in section secref15 for both the mlp model and the knn model. for precision and recall formulas eqref20 and eqref21 were used because of their advantage in multi - label classification. the distribution of predicted genres was also shown in a histogram and compared to the target distribution of genres. furthermore the ratio of reviews that got zero genres predicted was also calculated and can be expressed as displayform0 evaluation when evaluating classifiers it is common to use accuracy, precision and recall as well as hamming loss. accuracy, precision and recall are defined by the the four terms true positive ( inlineform0 ), true negative ( inlineform1 ), false positive ( inlineform2 ) and false negative ( inlineform3 ) which can be seen in table tabref16. accuracy is a measurement of how correct a model ' s predictions are and is defined as displayform0. precision is a ratio of how often positive predictions actually are positve and is defined as displayform0. recall is a measurement of how good the model is to find all true positives and is defined as displayform0. bibref5 it has been shown that when calculating precision and recall on multi - label classifiers, it can be advantageous to use micro averaged precision and recall bibref6. the formulas for micro averaged precision are expressed as displayform0 displayform1 where inlineform0 is label index and inlineform1 is number of labels. discussion when looking at the results it is apparent that knn is better than mlp in these experiments. in particular, the inlineform0 stands out between knn and mlp where knn got inlineform1 and mlp got inlineform2 which is considered a significant difference. given that the inlineform3 was relatively high for both models, this result hints that the models only predicted genres when the confidence was high, which resulted in fewer genres being predicted than the target",There is no baseline.,0.10574054718017578,0.2762850821018219
a81941f933907e4eb848f8aa896c78c1157bff20,"Can the model add new relations to the knowledge graph, or just new entities?","can the model add new relations to the knowledge graph, or just new entities?",The model does not add new relations to the knowledge graph.,0.06564521789550781,0.731140673160553
a891039441e008f1fd0a227dbed003f76c140737,What MC abbreviate for?,,machine comprehension,0.10484129190444946,0.28316786885261536
a978a1ee73547ff3a80c66e6db3e6c3d3b6512f4,How much improvement does their method get over the fine tuning baseline?,,"0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.",0.03803931921720505,0.07904966920614243
aa54e12ff71c25b7cff1e44783d07806e89f8e54,What is an example of a health-related tweet?,long short - term memory,"The health benefits of alcohol consumption are more limited than previously thought, researchers say",0.47631943225860596,0.0356687568128109
aacb0b97aed6fc6a8b471b8c2e5c4ddb60988bf5,How many attention layers are there in their model?,,one,0.029522066935896873,0.4467759430408478
aaed6e30cf16727df0075b364873df2a4ec7605b,What is WNGT 2019 shared task?,neural generation of text,efficiency task aimed  at reducing the number of parameters while minimizing drop in performance,0.21103882789611816,0.10227490216493607
ac148fb921cce9c8e7b559bba36e54b63ef86350,What dataset they use for evaluation?,"what dataset they use for evaluation? introduction machine summarization systems have made significant progress in recent years, especially in the domain of news text. this has been made possible among other things by the popularization of the neural sequence - to - sequence ( seq2seq ) paradigm bibref0, bibref1, bibref2, the development of methods which combine the strengths of extractive and abstractive approaches to summarization bibref3, bibref4, and the availability of large training datasets for the task, such as gigaword or the cnn - daily mail corpus which comprise of over 3. 8m shorter and 300k longer articles and aligned summaries respectively. unfortunately, the lack of datasets of similar scale for other text genres remains a limiting factor when attempting to take full advantage of these modeling advances using supervised training algorithms. experiments : : : data and model choices we validate our approach on the gigaword corpus, which comprises of a training set of 3. 8m article headlines ( considered to be the full text ) and titles ( summaries ), along with 200k validation pairs, and we report test performance on the same 2k set used in bibref7",The same 2K set from Gigaword used in BIBREF7,0.05192629620432854,0.26180776953697205
acc8d9918d19c212ec256181e51292f2957b37d7,What are the differences with previous applications of neural networks for this task?,it will not be able to efficiently model dependencies and interactions between words that are a few steps apart,This approach considers related images,0.09864058345556259,0.12942495942115784
ace60950ccd6076bf13e12ee2717e50bc038a175,How are the two different models trained?,pre - trained,They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.,0.3885323405265808,0.5163794755935669
ad0a7fe75db5553652cd25555c6980f497e08113,How does the model compute the likelihood of executing to the correction semantic denotation?,ranker,By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x.,0.5465608835220337,0.11224941164255142
ad1f230f10235413d1fe501e414358245b415476,Which models were compared?,which models were compared? the most surprising result was that the accuracy of all models drops significantly even when the models were trained on multinli and tested on snli,"BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT",0.0409235917031765,0.21743254363536835
ad5898fa0063c8a943452f79df2f55a5531035c7,Which embeddings do they detect biases in?,"which embeddings do they detect biases in? beside this kind of magical power, however, embeddings have been shown to carry worrying biases present in our society and thus encoded in language. recent studies bibref1, bibref2",Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset,0.03089264966547489,0.4161456823348999
ad67ca844c63bf8ac9fdd0fa5f58c5a438f16211,Which unlabeled data do they pretrain with?,audio,1000 hours of WSJ audio data,0.10486125200986862,0.47507956624031067
aeda22ae760de7f5c0212dad048e4984cd613162,What annotations are available in the dataset?,,"For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)",0.08394189178943634,0.09146202355623245
af75ad21dda25ec72311c2be4589efed9df2f482,How much does this system outperform prior work?,,"The system outperforms by 27.7% the LSTM model, 38.5% the RL-SPINN model and 41.6% the Gumbel Tree-LSTM",0.06842011958360672,0.06880839914083481
b13d0e463d5eb6028cdaa0c36ac7de3b76b5e933,What datasets are used to evaluate the model?,irns,WN18 and FB15k,0.737038254737854,0.22516082227230072
b2254f9dd0e416ee37b577cef75ffa36cbcb8293,How many domains of ontologies do they gather data from?,5,"5 domains: software, stuff, african wildlife, healthcare, datatypes",0.15032243728637695,0.2535806894302368
b27f7993b1fe7804c5660d1a33655e424cea8d10,What is the source of the visual data? ,,Profile pictures from the Twitter users' profiles.,0.04565905034542084,0.18862409889698029
b3857a590fd667ecc282f66d771e5b2773ce9632,What is a string kernel?,"histogram intersection string kernel bibref19. for two strings over an alphabet inlineform0, inlineform1, the intersection string kernel is formally defined as follows : displayform0 where inlineform0 is the number of occurrences of n - gram inlineform1 as a substring in inlineform2, and inlineform3 is the length of inlineform4. the spectrum string kernel or the presence bits string kernel can be defined in a similar fashion bibref19. in recent years, methods based on string kernels have demonstrated remarkable performance in various text classification tasks bibref35, bibref36, bibref22, bibref19, bibref10, bibref17, bibref26. string kernels represent a way of using information at the character level by measuring the similarity of strings through character n - grams",String kernel is a technique that uses character n-grams to measure the similarity of strings,0.08409332484006882,0.5901079177856445
b39f2249a1489a2cef74155496511cc5d1b2a73d,What is the accuracy reported by state-of-the-art methods?,93. 7 %,"Answer with content missing: (Table 1)
Previous state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)",0.1387687623500824,0.21564343571662903
b3a09d2e3156c51bd5fdc110a2a00a67bb8c0e42,what are the off-the-shelf systems discussed in the paper?,"ots ). the role of the program implementing his method was to route documents to mt systems, and beesley ' s paper more clearly describes what has later come to be known as a character model","Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier.",0.09925053268671036,0.1671903133392334
b3fcab006a9e51a0178a1f64d1d084a895bd8d5c,what are the state of the art methods?,"bibref10, bibref11, bibref12, bibref13, bibref14","S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al.",0.15574872493743896,0.34200039505958557
b43fa27270eeba3e80ff2a03754628b5459875d6,What domains are present in the data?,20,"Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather",0.08363837003707886,0.018167011439800262
b540cd4fe9dc4394f64d5b76b0eaa4d9e30fb728,Could you tell me more about the metrics used for performance evaluation?,,"BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy",0.08562897890806198,0.13781271874904633
b5a2b03cfc5a64ad4542773d38372fffc6d3eac7,How are EAC evaluated?,qualitative and quantitative assessment,"Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement.",0.7901467084884644,0.6327623128890991
b5bfa6effdeae8ee864d7d11bc5f3e1766171c2d,Which regions of the United States do they consider?,,all regions except those that are colored black,0.08737493306398392,0.10865360498428345
b5e883b15e63029eb07d6ff42df703a64613a18a,How were topics of interest about DDEO identified?,as subtopics of other ddeo topics,using topic modeling model Latent Dirichlet Allocation (LDA),0.08137060701847076,0.3594754934310913
b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1,What were the non-neural baselines used for the task?,,The Lemming model in BIBREF17,0.08943416178226471,0.1647406816482544
b6f5860fc4a9a763ddc5edaf6d8df0eb52125c9e,Which 5 languages appear most frequently in AA paper titles?,"chinese, arabic, korean, japanese, and hindi","English, Chinese, French, Japanese and Arabic",0.19420549273490906,0.9293455481529236
b6fb72437e3779b0e523b9710e36b966c23a2a40,How many rules had to be defined?,how many rules had to be defined?,"WikiSQL - 2 rules (SELECT, WHERE)
SimpleQuestions - 1 rule
SequentialQA - 3 rules (SELECT, WHERE, COPY)",0.08754682540893555,0.3869170546531677
b7708cbb50085eb41e306bd2248f1515a5ebada8,How do they get the formal languages?,"by analyzing the cell states and the activations of the gates in their lstm model, they further demonstrated that the network learns how to count up and down at certain places in the sample sequences",These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.,0.04408521577715874,0.31955471634864807
b8dea4a98b4da4ef1b9c98a211210e31d6630cf3,What is specific to gCAS cell?,three sequentially connected units,"It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner.",0.08625990152359009,0.5790574550628662
b8f711179a468fec9a0d8a961fb0f51894af4b31,What kind of neural network architecture do they use?,two - stream,CNN,0.5811521410942078,0.29421812295913696
b9025c39838ccc2a79c545bec4a676f7cc4600eb,Why do they think this task is hard?  What is the baseline performance?,there may be situations where more than one action is reasonable,"1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.
2. Macro F1 = 14.6 (MLR, length 96 snippet)
Weighted F1 = 31.1 (LSTM, length 128 snippet)",0.10151887685060501,0.5361126065254211
b9f2a30f5ef664ff845d860cf4bfc2afb0a46e5a,How do they gather human judgements for similarity between relations?,developing a framework,By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4,0.22103042900562286,0.08273666352033615
ba1da61db264599963e340010b777a1723ffeb4c,What does recurrent deep stacking network do?,stacks and concatenates the outputs of previous frames into the input features of the current frame,Stacks and joins outputs of previous frames with inputs of the current frame,0.6413410902023315,0.8857191205024719
ba56afe426906c4cfc414bca4c66ceb4a0a68121,What are the datasets used for the task?,"split randomly by percentages 80 ( training ), 10 ( development ), and 10 ( test )","Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)",0.03930776193737984,0.07888886332511902
bab8c69e183bae6e30fc362009db9b46e720225e,What are two strong baseline methods authors refer to?,"dep & rel or dep & relpath ), three methods achieve similar performance. overall, relawe achieves best results given enough syntactic knowledge. experiment : : : results on the english data we also conduct several experiments on the english dataset to validate the effectiveness of our approaches on other languages than chinese and the results are in table tabref49. although both configurations are not exactly the same as their original papers, we tried our best to reproduce their methods on the conll2009 dataset for our comparison. overall, the results are consistent with the chinese experiments, while the improvement is not as large as the chinese counterparts. the relawe model with deppath & relpath still achieves the best performance. applying our syntax - enhanced model to more languages will be an interesting research direction to work on in the future. [ 10 ] we reimplement lisa in bibref15 as lisa ( dep ), and bibref9 ' s best deppath approach as input ( deppath ). therefore, we can compare with their work as fairly as possible. other settings are the best configurations for their corresponding methods. closer observation reveals two additional interesting phenomena. firstly, srl performance improvement is not proportionate to the improvement of dependency quality. when switching syntactic dependency trees from auto to biaffine, srl performance improves 0. 5 %, although syntactic dependency improves about 8 %. in contrast, the difference between biaffine and biaffinebert",Marcheggiani and Titov (2017) and Cai et al. (2018),0.1606176346540451,0.22163866460323334
bb4de896c0fa4bf3c8c43137255a4895f52abeef,What is the baseline model?,tacotron model bibref32,a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model,0.1800420880317688,0.4048100709915161
bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2,How do the various social phenomena examined manifest in different types of communities?,systematically across communities,"Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
",0.23377163708209991,0.46215131878852844
bbdb2942dc6de3d384e3a1b705af996a5341031b,What type of model are the ELMo representations used in?,deep learning,A bi-LSTM with max-pooling on top of it,0.6003316044807434,0.28258296847343445
bc9c31b3ce8126d1d148b1025c66f270581fde10,What datasets are used to evaluate this approach?,," Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ",0.06979946047067642,0.06583467125892639
bd1a3c651ca2b27f283d3f36df507ed4eb24c2b0,Is it a neural model? How is it trained?,"deep neural networks, similar to those used for image captioning, are capable of producing these types of questions after extensive training","No, it is a probabilistic model trained by finding feature weights through gradient ascent",0.17351210117340088,0.2530716359615326
bd5379047c2cf090bea838c67b6ed44773bcd56f,Which experiments are perfomed?,which experiments are perfomed?,They used BERT-based models to detect subjective language in the WNC corpus,0.06906097382307053,0.1314331740140915
bd99aba3309da96e96eab3e0f4c4c8c70b51980a,Which existing models does this approach outperform?,"which existing models does this approach outperform? however, the aforementioned method tries to regularize the output word distribution based on what it has already learned. the relative order of the output words is kept. the self - dependency may not be desirable for regularization. it may be better if more correspondence that is spurious can be identified. in this paper, we further propose to obtain the soft target from a different view of the model, so that different knowledge of the dataset can be used to mitigate the overfitting problem. an additional output layer is introduced to generate the soft target. the two output layers share the same hidden representation but have independent parameters. they could learn different knowledge of the data. we refer to this approach as dual - train. for clarity, the original output layer is denoted by inlineform0 and the new output layer inlineform1. their outputs are denoted by inlineform2 and inlineform3, respectively. the output layer inlineform4 acts as the original output layer. a similar work to model distillation is the soft - target regularization method bibref20 for image classification. instead of using the outputs of other instances, it used an exponential average of the past label distributions of the current instance as the soft target distribution. the proposed method is different compared with the existing model distillation methods, in that the proposed method does not require additional models or additional space to record the past soft label distributions. the existing methods are not suitable for text summarization tasks, because the training of an additional model is costly, and the additional space is huge due to the massive number of data. the proposed method uses its current state as the soft target distribution and eliminates the need to train additional models or to store the history information. the new output layer inlineform0 is trained normally using the originally hard target. this output layer is not used in the prediction, and its only purpose is to generate the soft target to facilitate the soft training of inlineform1. suppose the correct label is inlineform2. the target of the output inlineform3 includes only the one - hot distribution : displayform0 because of the random initialization of the parameters in the output layers, inlineform0 and inlineform1","RNN-context, SRB, CopyNet, RNN-distract, DRGD",0.04979310557246208,0.24889206886291504
bdc93ac1b8643617c966e91d09c01766f7503872,What is the size of the second dataset?,5 to 24 lines each,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,0.08750911802053452,0.2525247633457184
bdd8368debcb1bdad14c454aaf96695ac5186b09,How is the intensity of the PTSD established?,based on the weekly survey results of all three clinical survey tools,"Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.",0.24437890946865082,0.20543064177036285
bde6fa2057fa21b38a91eeb2bb6a3ae7fb3a2c62,what state of the accuracy did they obtain?,lower accuracy,51.5,0.24169915914535522,0.21577894687652588
bfce2afe7a4b71f9127d4f9ef479a0bfb16eaf76,What human evaluation metrics were used in the paper?,,rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context,0.06973034143447876,0.04477272927761078
c000a43aff3cb0ad1cee5379f9388531b5521e9a,how are the bidirectional lms obtained?,news articles,"They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs.",0.7785095572471619,0.09174497425556183
c0035fb1c2b3de15146a7ce186ccd2e366fb4da2,By how much of MGNC-CNN out perform the baselines?,,"In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. 
In case of Irony the difference is about 2.0. 
",0.046100862324237823,0.05091525986790657
c029deb7f99756d2669abad0a349d917428e9c12,How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?,,3%,0.10005136579275131,0.3261537253856659
c034f38a570d40360c3551a6469486044585c63c,How better is proposed method than baselines perpexity wise?,,Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.,0.08754461258649826,0.11197148263454437
c13fe4064df0cfebd0538f29cb13e917fc5c3be0,What is the network architecture?,sogaard2016deep,"The network architecture has a multi-task Bi-Directional Recurrent Neural Network, with an unsupervised sequence labeling task and a low-dimensional embedding layer between tasks. There is a hidden layer after each successive task with skip connections to the senior supervised layers.",0.15322859585285187,0.07577774673700333
c165ea43256d7ee1b1fb6f5c0c8af5f7b585e60d,How much is proposed model better than baselines in performed experiments?,,"most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)
GDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)
GDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)",0.014481520280241966,0.019032977521419525
c1c611409b5659a1fd4a870b6cc41f042e2e9889,What evaluations did the authors use on their system?,what evaluations did the authors use on their system?,"BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.",0.10087040066719055,0.3860599398612976
c1f4d632da78714308dc502fe4e7b16ea6f76f81,Which language-pair had the better performance?,english - french,French-English,0.34908321499824524,0.9838618636131287
c348a8c06e20d5dee07443e962b763073f490079,What two components are included in their proposed framework?,single and ensemble model,evidence extraction and answer synthesis,0.38350802659988403,0.14814472198486328
c359ab8ebef6f60c5a38f5244e8c18d85e92761d,How many paraphrases are generated per question?,100,"10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans",0.09325719624757767,0.1889621913433075
c45feda62f23245f53e855706e2d8ea733b7fd03,Which translation system do they use to translate to English?,pre - trained translation model,Attention-based translation model with convolution sequence to sequence model,0.5678220391273499,0.6625715494155884
c47e87efab11f661993a14cf2d7506be641375e4,How does new evaluation metric considers critical informative entities?,"critical information completeness ( cic ). formally, cic is a recall of semantic slot information between a candidate summary and a reference summary",Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities,0.10581426322460175,0.6670595407485962
c4b5cc2988a2b91534394a3a0665b0c769b598bb,How do they define local variance?,the reciprocal of its variance,The reciprocal of the variance of the attention distribution,0.35617226362228394,0.6953896284103394
c4c9c7900a0480743acc7599efb359bc81cf3a4d,How much is pre-training loss increased in Low/Medium/Hard level of pruning?,medium,"The increase is linearly from lowest on average 2.0 , medium around 3.5, and the largest is 6.0",0.346153199672699,0.31122729182243347
c515269b37cc186f6f82ab9ada5d9ca176335ded,What evidence do they present that the model attends to shallow context clues?,verb semantics,Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues,0.272539883852005,0.3551334738731384
c554a453b6b99d8b59e4ef1511b1b506ff6e5aa4,how was the dataset built?,having annotators answer pre - existing questions,"Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable"" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is “yes"" or “no""",0.2545951306819916,0.5623540282249451
c69f4df4943a2ca4c10933683a02b179a5e76f64,What approach performs better in experiments global latent or sequence of fine-grained latent variables?,sequential latent,"PPL: SVT
Diversity: GVT
Embeddings Similarity: SVT
Human Evaluation: SVT",0.27250412106513977,0.08128689229488373
c77d6061d260f627f2a29a63718243bab5a6ed5a,How different is the dataset size of source and target?,,the training dataset is large while the target dataset is usually much smaller,0.06714420020580292,0.14053788781166077
c7d99e66c4ab555fe3d616b15a5048f3fe1f3f0e,What is an example of a computational social science NLP task?,statistical machine translation,Visualization of State of the union addresses,0.16837458312511444,0.16029447317123413
c82e945b43b2e61c8ea567727e239662309e9508,What additional features are proposed for future work?,deeper analysis of clinical narratives in ehrs,distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort,0.261275053024292,0.3760411739349365
ca26cfcc755f9d0641db0e4d88b4109b903dbb26,How better are results compared to baseline models?,significantly better,F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.,0.2878701090812683,0.20937906205654144
ca7e71131219252d1fab69865804b8f89a2c0a8f,How does this compare to traditional calibration methods like Platt Scaling?,,No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.,0.10176850855350494,-0.1638273149728775
cacb83e15e160d700db93c3f67c79a11281d20c5,Does this paper propose a new task that others can try to improve performance on?,improvement in clause level representation,"No, there has been previous work on recognizing social norm violation.",0.1757335662841797,-0.06662407517433167
caf9819be516d2c5a7bfafc80882b07517752dfa,Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?,,They evaluate quantitatively.,0.10864195227622986,0.1334201842546463
cb78e280e3340b786e81636431834b75824568c3,How many emotions do they look at?,8,9,0.05661914870142937,0.8417977094650269
cb8a6f5c29715619a137e21b54b29e9dd48dad7d,"What is an ""answer style""?",,well-formed sentences vs concise answers,0.07868596911430359,0.11389269679784775
cbbcafffda7107358fa5bf02409a01e17ee56bfd,Was any variation in results observed based on language typology?,only account for at most more of the part - of - speech tag entropy than a control.,It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information.,0.07249250262975693,0.22935569286346436
cc5d3903913fa2e841f900372ec74b0efd5e0c71,Which sentiment analysis tasks are addressed?,"b d, b e, b k, d b, d e, d k, e b, e d, e k, k b, k d, k e",12 binary-class classification and multi-class classification of reviews based on rating,0.8664204478263855,0.15555597841739655
cc5d8e12f6aecf6a5f305e2f8b3a0c67f49801a9,What percentage fewer errors did professional translations make?,,36%,0.10574303567409515,0.291284441947937
cd8de03eac49fd79b9d4c07b1b41a165197e1adb,How are multimodal representations combined?,,The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.,0.06728678196668625,0.10750570893287659
cdf1bf4b202576c39e063921f6b63dc9e4d6b1ff,What metric is used to measure performance?,"what metric is used to measure performance? evaluation tasks we use a standard set of supervised as well as unsupervised benchmark tasks from the literature to evaluate our trained models, following bibref16. the breadth of tasks allows to fairly measure generalization to a wide area of different domains, testing the general - purpose quality ( universality ) of all competing sentence embeddings. for downstream supervised evaluations, sentence embeddings are combined with logistic regression to predict target labels. in the unsupervised evaluation for sentence similarity, correlation of the cosine similarity between two embeddings is compared to human annotators.. for example, the trec task is a poor measure of how one predicts the content of the sentence ( the question ) but a good measure of how the next sentence in the sequence ( the answer ) is predicted. unsupervised similarity evaluation results. in table tabref19, we see that our sent2vec models are state - of - the - art on the majority of tasks when comparing to all the unsupervised models trained on the toronto corpus, and clearly achieve the best averaged","Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks",0.06672757863998413,0.47370976209640503
ce807a42370bfca10fa322d6fa772e4a58a8dca1,What are the four forums the data comes from?,"darkode, hack forums, blackhat, and nulled","Darkode,  Hack Forums, Blackhat and Nulled.",0.09938163310289383,0.9695959687232971
cf93a209c8001ffb4ef505d306b6ced5936c6b63,From when are many VQA datasets collected?,between 2014 – 2016,late 2014,0.2535325884819031,0.6920700073242188
cfbccb51f0f8f8f125b40168ed66384e2a09762b,How are discourse embeddings analyzed?,general analysis,They perform t-SNE clustering to analyze discourse embeddings,0.5229695439338684,0.08497896790504456
cfffc94518d64cb3c8789395707e4336676e0345,What approaches without reinforcement learning have been tried?,"classification, regression","classification, regression, neural methods",0.11191126704216003,0.7948311567306519
d028dcef22cdf0e86f62455d083581d025db1955,What are the strong baselines you have?,what are the strong baselines you have? we optimized our single - task baseline,optimize single task with no synthetic data,0.02182527258992195,0.3917081952095032
d0bc782961567dc1dd7e074b621a6d6be44bb5b4,How big is seed lexicon used for training?,very small,30 words,0.11476901918649673,0.25819411873817444
d0c79f4a5d5c45fe673d9fcb3cd0b7dd65df7636,What are new best results on standard benchmark?,,"New best results of accuracy (P@1) on Vecmap:
Ours-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43",0.09215059131383896,0.1100260317325592
d0f831c97d345a5b8149a9d51bf321f844518434,What labels are in the dataset?,,binary label of stress or not stress,0.024682605639100075,0.11766333132982254
d2fbf34cf4b5b1fd82394124728b03003884409c,Who was the top-scoring team?,friends and emotionpush,IDEA,0.3261657655239105,0.18417727947235107
d3092f78bdbe7e741932e3ddf997e8db42fa044c,What experimental evaluation is used?,procedure and results,root mean square error between the actual and the predicted price of Bitcoin for every minute,0.6975166201591492,0.050608519464731216
d3bcfcea00dec99fa26283cdd74ba565bc907632,How big is dataset for this challenge?,,"133,287 images",0.07990837842226028,0.038113344460725784
d484a71e23d128f146182dccc30001df35cdf93f,How much is proposed model better in perplexity and BLEU score than typical UMT models?,,"Perplexity of the best model is 65.58 compared to best baseline 105.79.
Bleu of the best model is 6.57 compared to best baseline 5.50.",0.06068319082260132,0.09051623940467834
d5256d684b5f1b1ec648d996c358e66fe51f4904,what is the practical application for this paper?,"using corpus based approaches ; however, we would like to quantify the complexity not only by the type and token distributions over a corpus, but also by taking into account other important dimension : the predictability of a morph sequence bibref7. this is a preliminary work that takes as a case of study the distant languages otomi, nahuatl and spanish. the general idea is to use parallel corpora, type - token relationship and some nlp strategies for measuring the predictability in statistical language models",Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools.,0.05653534084558487,0.33754491806030273
d5498d16e8350c9785782b57b1e5a82212dbdaad,How accurate is model trained on text exclusively?,,Relative error is less than 5%,0.04504106193780899,0.034191954880952835
d5fa26a2b7506733f3fa0973e2fe3fc1bbd1a12d,"How are possible sentence transformations represented in dataset, as new sentences?",annotationinstructions,"Yes, as new sentences.",0.5433337688446045,0.22088971734046936
d60a3887a0d434abc0861637bbcd9ad0c596caf4,What semantic rules are proposed?,,rules that compute polarity of words after POS tagging or parsing steps,0.07327546924352646,0.07258015125989914
d653d994ef914d76c7d4011c0eb7873610ad795f,How were breast cancer related posts compiled from the Twitter streaming API?,how were breast cancer related posts compiled from the twitter streaming api?,"By using  keywords `breast' AND `cancer' in tweet collecting process. 
",0.0542004257440567,0.6678837537765503
d6e2b276390bdc957dfa7e878de80cee1f41fbca,What models other than standalone BERT is new model compared to?,attentive mimicking,Only Bert base and Bert large are compared to proposed approach.,0.7263931632041931,0.13754859566688538
d6e8b32048ff83c052e978ff3b8f1cb097377786,"How are customer satisfaction, customer frustration and overall problem resolution data collected?",binary classification experiments,By annotators on Amazon Mechanical Turk.,0.4160501956939697,0.29505622386932373
d70ba6053e245ee4179c26a5dabcad37561c6af0,Which datasets did they experiment on?,question - answer datasets,ConciergeQA and AmazonQA,0.3054898679256439,0.17917735874652863
d71cb7f3aa585e256ca14eebdc358edfc3a9539c,Which models achieve state-of-the-art performances?,sequential neural network model,"CELEX (Dutch and English) - SVM-HMM
Festival, E-Hitz and OpenLexique - Liang hyphenation
IIT-Guwahat - Entropy CRF",0.4387352466583252,0.07750765979290009
d77c9ede2727c28e0b5a240b2521fd49a19442e0,What's the input representation of OpenIE tuples into the model?,the concatenation of word embedding and another embedding indicating whether this word is predicate,word embeddings,0.23047639429569244,0.5698093771934509
d79d897f94e666d5a6fcda3b0c7e807c8fad109e,What result from experiments suggest that natural language based agents are more robust?,task - nuisances,Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances,0.2995709180831909,0.4833783507347107
d7d611f622552142723e064f330d071f985e805c,How many utterances are in the corpus?,how many utterances are in the corpus?,Total number of utterances available is: 70607 (37344 ENG + 33263 GER),0.10295532643795013,0.6748735904693604
d824f837d8bc17f399e9b8ce8b30795944df0d51,How do they show their model discovers underlying syntactic structure?,use the hidden states of space ( separator ) tokens to summarize previous information,By visualizing syntactic distance estimated by the parsing network,0.04555601626634598,0.32537567615509033
d8de12f5eff64d0e9c9e88f6ebdabc4cdf042c22,How much gain does the model achieve with pretraining MVCNN?,,0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj,0.08541504293680191,0.19518901407718658
da8bda963f179f5517a864943dc0ee71249ee1ce,How many layers does their system have?,,4 layers,0.08833318203687668,0.21970930695533752
dac2591f19f5bbac3d4a7fa038ff7aa09f6f0d96,what are the three methods presented in the paper?,,"Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.",0.04110059142112732,0.15074361860752106
dafa760e1466e9eaa73ad8cb39b229abd5babbda,How large is the dataset they generate?,,4.756 million sentences,0.10975611209869385,0.176084965467453
dbdf13cb4faa1785bdee90734f6c16380459520b,What cluster identification method is used in this paper?,,"A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18",0.08174002915620804,0.17099112272262573
dbfce07613e6d0d7412165e14438d5f92ad4b004,What affective-based features are used?,"dal activation, anew dominance, emolex negative, emolex fear, liwc assent, liwc cause, liwc certain and liwc sad. therefore, we have only 17 columns of features in the best performing system covering structural, conversational, affective and dialogue - act features. affective based features the idea to use affective features in the context of our task was inspired by recent works on fake news detection, focusing on emotional responses to true and false rumors bibref5, and by the work in bibref6 reflecting on the role of affect in dialogue acts bibref6. multi - faceted affective features have been already proven to be effective in some related tasks bibref9, including the stance detection task proposed at semeval - 2016 ( task 6 ). we used the following affective resources relying on different emotion models. emolex : it contains 14, 182 words associated with eight primary emotion based on the plutchik model bibref10, bibref11. emosenticnet ( emosn ) : it is an enriched version of senticnet bibref12 including 13, 189 words labeled by six ekman ' s basic emotion bibref13, bibref14. dictionary of affect in language ( dal ) : includes 8, 742 english words labeled by three scores representing three dimensions : pleasantness, activation and imagery bibref15. proposed method we developed a new model by exploiting several stylistic and structural features characterizing twitter language. in addition, we propose to utilize conversational - based features by exploiting the peculiar tree structure of the dataset. we also explored the use of affective based feature by extracting information from several affective resources including dialogue - act inspired features","affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count",0.3089276850223541,0.6815651059150696
dc69256bdfe76fa30ce4404b697f1bedfd6125fe,What are the languages used to test the model?,"hindi, english, and german","Hindi, English and German (German task won)",0.9181264042854309,0.8123501539230347
dcb18516369c3cf9838e83168357aed6643ae1b8,Which retrieval system was used for baselines?,deep neural networks,The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.,0.1177414134144783,0.09977439790964127
dd155f01f6f4a14f9d25afc97504aefdc6d29c13,What aspects have been compared between various language models?,,"Quality measures using perplexity and recall, and performance measured using latency and energy usage. ",0.059126585721969604,0.07356737554073334
de12e059088e4800d7d89e4214a3997994dbc0d9,What are the baseline systems that are compared against?,,"The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM",0.10968926548957825,0.08449655026197433
df2839dbd68ed9d5d186e6c148fa42fce60de64f,How big is the provided treebank?,,"1448 sentences more than the dataset from Bhat et al., 2017",0.10571133345365524,0.1930265575647354
df79d04cc10a01d433bb558d5f8a51bfad29f46b,Which languages do they test on?,which languages do they test on?,"Answer with content missing: (Applications section) We use Wikipedia articles
in five languages
(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams
et al. (2017).
Select:
Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English",0.08832496404647827,0.35178622603416443
dfbab3cd991f86d998223726617d61113caa6193,"For the purposes of this paper, how is something determined to be domain specific knowledge?",filters effectively propagate sentiment level knowledge from domain specific terms as well,reviews under distinct product categories are considered specific domain knowledge,0.0807984247803688,0.536327600479126
e051d68a7932f700e6c3f48da57d3e2519936c6d,Which pre-trained English NER model do they use?,"bilstm - crf model, which could obtain transferred knowledge from a pre - trained english ner system. first, we translate other languages into english. since the proposed models of bibref3 and bibref4, the performance of attention - based machine translation systems is close to the human level. the attention mechanism can make the translation results more accurate. furthermore, this mechanism has another useful property : the attention weights can represent the alignment information. after translating the low - resource language into english, we utilize the pre - trained english ner model to predict the sentences and record the output states of bilstm in this model. the states contain the semantic and task - specific information of the sentences. by using soft alignment attention weights as a transformation matrix, we manage to transfer the knowledge of high resource language — english to other languages analysis our proposed approach is the first to leverage hidden states of ner model from another language to improve monolingual ner performance. the training time with or without ban is almost the same due to the translation module and the english ner module are pre - trained. back attention knowledge transfer the sentences in low - resource languages are used as input to the model. given a input sentence inlineform0 in low - resource language, we use pre - trained translation model to translate inlineform1 into english and the output is inlineform2. simultaneously, we record the average of values for all inlineform3 attention layers : displayform0 after that, we use the pre - trained english ner model to predict the translated sentence inlineform0",Bidirectional LSTM based NER model of Flair,0.0369064100086689,0.38801029324531555
e09e89b3945b756609278dcffb5f89d8a52a02cd,How many speeches are in the dataset?,1201,5575 speeches,0.39183369278907776,0.29811975359916687
e0b7acf4292b71725b140f089c6850aebf2828d2,How is annotation projection done when languages have different word order?,word alignments,"Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments.",0.04175476357340813,0.6841623187065125
e111925a82bad50f8e83da274988b9bea8b90005,How do they collect the control corpus?,"how do they collect the control corpus? ( control ) corpus. crucially, the tf - idf filtering",Randomly from Twitter,0.04133062809705734,0.2571399509906769
e1b36927114969f3b759cba056cfb3756de474e4,By how much does using phonetic feedback improve state-of-the-art systems?,,Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9,0.034427572041749954,0.07517825812101364
e2427f182d7cda24eb7197f7998a02bc80550f15,How is the architecture fault-tolerant?,"streaming and increasing volume of data in a fault tolerant way. figure figref2 gives an overview of the architecture design. central to this design is apache spark which acts as an in - memory data store and allows us to perform computations in a scalable manner. this data is the input to our machine learning model for making predictions. to bootstrap our model, we first gather a few days of data and store that in apache spark rdds. next, we perform computations to construct features from the raw data. all these computations are performed on data that is distributed across multiple spark clusters and therefore will scale as the data grows continuously.. these challenges call for learning platforms based on big data architectures that can not only handle heterogeneous volumes of data but also be fault tolerant and persistent in real time. kryptooracle has been built in the apache ecosystem and uses apache spark. data structures in spark are based on resilient distributed datasets ( rdd ), a read only multi - set of data which can be distributed over a cluster of machines",By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault,0.09418550878763199,0.5372705459594727
e28019afcb55c01516998554503bc1b56f923995,"Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",,Personal thought of the annotator.,0.032944779843091965,0.1860625445842743
e292676c8c75dd3711efd0e008423c11077938b1,Which soft-selection approaches are evaluated?,bert - original and bert - soft,LSTM and BERT ,0.11737680435180664,0.5963283181190491
e2f269997f5a01949733c2ec8169f126dabd7571,Which data sources do they use?,,"- En-Fr (WMT14)
- En-De (WMT15)
- Skipthought (BookCorpus)
- AllNLI (SNLI + MultiNLI)
- Parsing (PTB + 1-billion word)",0.03779872506856918,0.14677444100379944
e3981a11d3d6a8ab31e1b0aa2de96f253653cfb2,For which languages most of the existing MRC datasets are created?,english,English,0.20900540053844452,1.0000001192092896
e414d819f10c443cbefa8bdb9bd486ffc6d1fc6a,What is the average length of the recordings?,30 to 50 minutes,40 minutes,0.3639255464076996,0.7880274057388306
e4a19b91b57c006a9086ae07f2d6d6471a8cf0ce,How are the main international development topics that states raise identified?,structural topic model bibref4," They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.",0.617446780204773,0.4983551800251007
e4cc2e73c90e568791737c97d77acef83588185f,How long is the dataset?,"how long is the dataset? string kernels. kernel functions bibref38 capture the intuitive notion of similarity between objects in a specific domain. for example, in text mining, string kernels can be used to measure the pairwise similarity between text samples, simply based on character n - grams. various string kernel functions have been proposed to date bibref35, bibref38, bibref19. perhaps one of the most recently introduced string kernels is the histogram intersection string kernel bibref19. for two strings over an alphabet inlineform0, inlineform1, the intersection string kernel is formally defined as follows : displayform0 where inlineform0 is the number of occurrences of n - gram inlineform1 as a substring in inlineform2, and inlineform3 is the length of inlineform4. the spectrum string kernel or the presence bits string kernel can be defined in a similar fashion bibref19. general transfer learning approaches. long et al. bibref12 proposed a novel transfer learning framework to model distribution adaptation and label propagation in a unified way, based on the structural risk minimization principle and the regularization theory. shu et al. bibref5 proposed a method that bridges the distribution gap between the source domain and the target domain through affinity learning, by exploiting the existence of a subset of data points in the target domain that are distributed similarly to the data points in the source domain. in bibref7, deep learning is employed to jointly optimize the representation, the cross - domain transformation and the target label inference in an end - to - end fashion. more recently, sun et al. bibref8 proposed an unsupervised domain adaptation method that minimizes the domain shift by aligning the second - order statistics of source and target distributions, without requiring any target labels. chang et al notations. we use the following notations in the algorithm. sets, arrays and matrices are written in capital letters. all collection types are considered to be indexed starting from position 1. the elements of a set inlineform0 are denoted by inlineform1, the elements of an array inlineform2 are alternatively denoted by inlineform3 or inlineform4, and the elements of a matrix inlineform5 are denoted by inlineform6 or inlineform7 when convenient. the sequence inlineform8 is denoted by inlineform9. we use sequences to index arrays or matrices as well. for example, for an array inlineform",8000,0.10884924978017807,0.05702803656458855
e51d0c2c336f255e342b5f6c3cf2a13231789fed,Which Twitter corpus was used to train the word vectors?,word2vec,They collected tweets in Russian language using a heuristic query specific to Russian,0.9623475670814514,0.15323027968406677
e5a965e7a109ae17a42dd22eddbf167be47fca75,What are the problems related to ambiguity in PICO sentence prediction tasks?,integration of training data from different tasks and sources and assessing our model ' s capacity for transfer learning and domain adaptation,Some sentences are associated to ambiguous dimensions in the hidden state output,0.3184625208377838,0.16092146933078766
e63bde5c7b154fbe990c3185e2626d13a1bad171,What is the performance achieved on NarrativeQA?,"what is the performance achieved on narrativeqa? results table 2 shows that our ensemble model, controlled with the nlg and q & a styles, achieved state - of - the - art performance on the nlg and q & a tasks in terms of rouge - l. in particular, for the nlg task, our single model outperformed competing models in terms of both rouge - l and bleu - 1. the capability of creating abstractive summaries from the question and passages contributed to its improvements over the state - of - the - art extractive approaches bibref6, bibref7. table 3 shows the results of the ablation test for our model ( controlled with the nlg style ) on the well - formed answers of the wfa dev. set. our model, which was trained with the all set consisting of the two styles, outperformed the model trained with the wfa set consisting of the single style. multi - style learning allowed our model to improve nlg performance by also using non - sentence answers. conclusion we believe our study makes two contributions to the study of multi - passage rc with nlg. our model enables 1 ) multi - source abstractive summarization based rc and 2 ) style - controllable rc. the key strength of our model is its high accuracy of generating abstractive summaries from the question and passages ; our model achieved state - of - the - art performance in terms of rouge - l on the q & a and nlg tasks of ms marco 2. 1 that have different answer styles bibref5. the styles considered in this paper are only related to the context of the question in the answer sentence ; our model will be promising for controlling other styles such as length and speaking styles. future work will involve exploring the potential of hybrid models combining extractive and abstractive approaches and improving the passage re - ranking and answerable question identification. table 4 shows the passage re - ranking performance for the ten given passages on the ans dev. set. our ranker improved the initial ranking provided by bing by a significant margin. also, the ranker shares the question - passages reader with the answer decoder, and this sharing contributed to the improvements over the ranker trained without the answer decoder. this result is similar to those reported in bibref33. moreover, the joint learning with the answer possibility classifier and multiple answer styles, which enables our model to learn from a larger number of data, improved the re - ranking.","Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87",0.10744431614875793,0.23872749507427216
e7329c403af26b7e6eef8b60ba6fefbe40ccf8ce,How much better does this baseline neural model do?,,"The model outperforms at every point in the
implicit-tuples PR curve reaching almost 0.8 in recall",0.024975374341011047,0.0672527402639389
e76139c63da0f861c097466983fbe0c94d1d9810,Is the model presented in the paper state of the art?,,"No, supervised models perform better for this task.",0.10215013474225998,0.09520569443702698
e829f008d62312357e0354a9ed3b0827c91c9401,Which psycholinguistic and basic linguistic features are used?,nlp features,"Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features",0.6680535674095154,0.5298578143119812
e84ba95c9a188fda4563f45e53fbc8728d8b5dab,Do they build one model per topic or on all topics?,,One model per topic.,0.10610907524824142,0.166476771235466
e8fcfb1412c3b30da6cbc0766152b6e11e17196c,What improvement does the MOE model make over the SOTA on language modelling?,significantly beats the multilingual gnmt model on 11 of the 12 language pairs,Perpexity is improved from 34.7 to 28.0.,0.06248125806450844,0.17106834053993225
e91692136033bbc3f19743d0ee5784365746a820,"It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?",using question - answer pairs as a supervision signal,using multiple pivot sentences,0.489662766456604,0.2598663568496704
e9ccc74b1f1b172224cf9f01e66b1fa9e34d2593,What metadata is included?,"speaker, category, tags and linked entities. we do not encode ` reason ' as it gives away the label, and do not include ` checker ' as there are too many unique checkers for this information to be relevant. the claim publication date is potentially relevant, but it does not make sense to merely model this as a one - hot feature, so we leave incorporating temporal information to future work. since all metadata consists of individual words and phrases, a sequence encoder is not necessary, and we opt for a cnn followed by a max pooling operation as used in bibref3 to encode metadata for fact checking. the max - pooled metadata representations, denoted inlineform0, are then concatenated with the instance representations, e. g. for the most elaborate model, crawled _ ranked, these would be concatenated with inlineform1. we perform an ablation analysis of how metadata impacts results, shown in table tabref35. out of the different types of metadata, topic tags on their own contribute the most. this is likely because they offer highly complementary information to the claim text of evidence pages. only using all metadata together achieves a higher macro f1 at similar micro f1 than using no metadata at all. to further investigate this, we split the test set into those instances for which no metadata is available vs. those for which metadata is available. we find that encoding metadata within the model hurts performance for domains where no metadata is available, but improves performance where it is. in practice, an ensemble of both types of models would be sensible, as well as exploring more involved methods of encoding metadata. claim veracity prediction we train several models to predict the veracity of claims. those fall into two categories : those that only consider the claims themselves, and those that encode evidence pages as well. in addition, claim metadata ( speaker, checker, linked entities","besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date",0.14844270050525665,0.42513608932495117
e9d9bb87a5c4faa965ceddd98d8b80d4b99e339e,How much do they outperform previous state-of-the-art?,,"On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.
On subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.",0.0812264233827591,0.09080573916435242
ea6764a362bac95fb99969e9f8c773a61afd8f39,What is the highest accuracy score achieved?,,82.0%,0.06954512745141983,0.20423682034015656
ead5dc1f3994b2031a1852ecc4f97ac5760ea977,How many category tags are considered?,"how many category tags are considered? in contrast, we build our model to utilize video frames, raw audio signal, and the speech content in the caption generation process. to this end, we deploy automatic speech recognition ( asr ) system bibref11 to extract time - aligned captions of what is being said ( similar to subtitles ) and employ it alongside with video and audio representations in the transformer model. the proposed model is assessed using the challenging activitynet captions bibref2 benchmark dataset, where we obtain competitive results to the current state - of - the - art. the subsequent ablation studies indicate a substantial contribution from audio and speech signals. moreover, we retrieve and perform breakdown analysis by utilizing previously unused video category tags provided with the original youtube videos bibref12",14 categories,0.04405362159013748,0.264293909072876
eb5ed1dd26fd9adb587d29225c7951a476c6ec28,What are the results of the experiment?,,"They were able to create a language model from the dataset, but did not test.",0.09490890800952911,-0.03286360949277878
eb95af36347ed0e0808e19963fe4d058e2ce3c9f,What is the accuracy of the proposed technique?,"what is the accuracy of the proposed technique? error analysis we describe four classes of failures that we observed, and the future work they suggest. missing important words : which material will spread out to completely fill a larger container? ( a ) air ( b ) ice ( c ) sand ( d ) water in this question, we have tuples that support water will spread out and fill a larger container but miss the critical word “ completely ”. an approach capable of detecting salient question words could help avoid that. lossy ie : which action is the best method to separate a mixture of salt and water?... the ir solver correctly answers this question by using the sentence : separate the salt and water mixture by evaporating the water. however, tupleinf is not able to answer this question as open ie is unable to extract tuples from this imperative sentence. while the additional structure from open ie is useful for more robust matching, converting sentences to open ie tuples may lose important bits of information. experiments comparing our method with two state - of - the - art systems for 4th and 8th grade science exams, we demonstrate that ( a ) tupleinf with only automatically extracted tuples significantly outperforms tableilp with its original curated knowledge as well as with additional tuples, and ( b ) tupleinf ' s complementary approach to ir leads to an improved ensemble. numbers in bold indicate statistical significance based on the binomial exact test bibref20 at. we consider two question sets. ( 1 ) 4th grade set ( 1220 train, 1304 test ) is a 10x larger superset of the ny regents questions bibref6, and includes professionally written licensed questions. ( 2 ) 8th grade set ( 293 train, 282 test ) contains 8th grade questions from various states. conclusion we presented a new qa system, tupleinf, that can reason over a large, potentially noisy tuple kb to answer complex questions. our results show that tupleinf is a new state - of - the - art structured solver for elementary - level science that does not rely on curated knowledge and generalizes to higher grades. errors due to lossy ie and misalignments suggest future work in incorporating context and distributional measures. appendix : ilp model details to build the ilp model, we first need to get the questions terms ( qterm ) from the question by chunking the question using an in - house chunker based on the postagger from",51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge,0.0876167044043541,0.2811063230037689
ec2b8c43f14227cf74f9b49573cceb137dd336e7,How is the speech recognition system evaluated?,"how is the speech recognition system evaluated? speakers were assigned either to training or evaluation sets, with proportions of and, respectively ; then training and evaluation lists were built, accordingly. table reports statistics from the spoken data set. the id all identifies the whole data set, while clean defines the subset in which sentences containing background voices, incomprehensible speech and word fragments were excluded. relation to prior work. scientific literature is rich in approaches for automated assessment of spoken language proficiency. performance is directly dependent on asr accuracy",Speech recognition system is evaluated using WER metric.,0.06221992149949074,0.6427969336509705
ed11b4ff7ca72dd80a792a6028e16ba20fccff66,How do they obtain region descriptions and object annotations?,visual genome dataset,they are available in the Visual Genome dataset,0.059762295335531235,0.8327642679214478
ed522090941f61e97ec3a39f52d7599b573492dd,What is triangulation?,. we outline a simple triangulation method with which we extend the muse dictionaries to an additional 2352 lexicons,"Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.",0.03719984367489815,0.3208865821361542
ed7985e733066cd067b399c36a3f5b09e532c844,What is different in BERT-gen from standard BERT?,,"They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens.",0.019814131781458855,0.17234212160110474
ed7a3e7fc1672f85a768613e7d1b419475950ab4,Does this approach perform better in the multi-domain or single-domain setting?,,single-domain setting,0.20943200588226318,0.11087652295827866
edb068df4ffbd73b379590762125990fcd317862,which benchmark tasks did they experiment on?,stanford sentiment treebank bibref7 and the ag english news corpus bibref3, They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task.,0.7208027839660645,0.6701173186302185
edb2d24d6d10af13931b3a47a6543bd469752f0c,How did the select the 300 Reddit communities for comparison?,"using our framework, we map almost 300 reddit communities onto the landscape defined by the two axes of our typology",They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.,0.18855692446231842,0.38261517882347107
eddabb24bc6de6451bcdaa7940f708e925010912,How are the EAU text spans annotated?,tree - based sentiment annotations are sentiment scores assigned to nodes in constituency parse trees,Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.,0.12136752903461456,0.3471303880214691
ee9b95d773e060dced08705db8d79a0a6ef353da,How are content clusters used to improve the prediction of incident severity?,how are content clusters used to improve the prediction of incident severity?,they are used as additional features in a supervised classification task,0.022021835669875145,0.32302480936050415
ef7b62a705f887326b7ebacbd62567ee1f2129b3,What were the baselines?,siamese neural network,"Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations",0.21933069825172424,0.7264688611030579
ef872807cb0c9974d18bbb886a7836e793727c3d,What contextual features are used?,"bibref16, bibref1",The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.,0.08293212205171585,0.11042718589305878
efb3a87845460655c53bd7365bcb8393c99358ec,What were their results on the three datasets?,,"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR",0.07982371747493744,0.059065841138362885
efc65e5032588da4a134d121fe50d49fe8fe5e8c,What supplemental tasks are used for multitask learning?,,"Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question",0.034819960594177246,0.20423980057239532
f10325d022e3f95223f79ab00f8b42e3bb7ca040,How are discourse features incorporated into the model?,an entity grid is constructed by feeding the document through an nlp pipeline to identify salient entities,They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.,0.10862081497907639,0.5319370031356812
f1831b2e96ff8ef65b8fde8b4c2ee3e04b7ac4bf,How close do clusters match to ground truth tone categories?,closely match the ground truth in figure figref2,"NMI between cluster assignments and ground truth tones for all sylables is:
Mandarin: 0.641
Cantonese: 0.464",0.2173757553100586,0.24179641902446747
f1bd66bb354e3dabf5dc4a71e6f08b17d472ecc9,How do slot binary classifiers improve performance?,greatly resolves the issues of tscp that uses a single decoder with each step having unconstrained vocabulary - size choices,by adding extra supervision to generate the slots that will be present in the response,0.0516890324652195,0.18891441822052002
f2c5da398e601e53f9f545947f61de5f40ede1ee,How do their interpret the coefficients?,"map to the “ dummy "" space",The coefficients are projected back to the dummy variable space.,0.23175406455993652,0.3290608823299408
f398587b9a0008628278a5ea858e01d3f5559f65,By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?,large margin,"SPNet vs best baseline:
ROUGE-1: 90.97 vs 90.68
CIC: 70.45 vs 70.25",0.3752100467681885,0.152869313955307
f4238f558d6ddf3849497a130b3a6ad866ff38b3,How is moral bias measured?,the correlation of the moral bias and the corresponding weat value was calculated to test consistency of findings,"Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q) − cos(b, q)
Bias is calculated as substraction of cosine similarities of question and some answer for two opposite answers.",0.05036243051290512,0.339555948972702
f42d470384ca63a8e106c7caf1cb59c7b92dbc27,What evaluation metrics are used?,we only consider the behaviors present in the route,"exact match, f1 score, edit distance and goal match",0.17083659768104553,0.054080504924058914
f463db61de40ae86cf5ddd445783bb34f5f8ab67,what are the baselines?,what are the baselines?,Perceptron model using the local features.,0.10158884525299072,0.08555854111909866
f4e17b14318b9f67d60a8a2dad1f6b506a10ab36,How is the generative model evaluated?,quantitative,Comparing BLEU score of model with and without attention,0.8383451700210571,0.20166487991809845
f513e27db363c28d19a29e01f758437d7477eb24,what are the baselines?,attention sum reader,"AS Reader, GA Reader, CAS Reader",0.17647618055343628,0.44708433747291565
f52b2ca49d98a37a6949288ec5f281a3217e5ae8,How do they condition the output to a given target-source class?,augment the source side with a token representing a specific length - ratio class,"They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.",0.1542205512523651,0.42390176653862
f5707610dc8ae2a3dc23aec63d4afa4b40b7ec1e,What are resolution model variables?,indicates in which mode the mention should be resolved,"Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.",0.08274427801370621,0.699726939201355
f5e571207d9f4701b4d01199ef7d0bfcfa2c0316,What are the linguistic differences between each class?,sarcastic and the not - sarcastic,"Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes",0.07966932654380798,0.5891869068145752
f62c78be58983ef1d77049738785ec7ab9f2a3ee,what datasets did the authors use?,the corpus of annotated news comments available from the yahoo webscope program,"Kaggle
Subversive Kaggle
Wikipedia
Subversive Wikipedia
Reddit
Subversive Reddit ",0.04592388868331909,0.3191930651664734
f6380c60e2eb32cb3a9d3bca17cf4dc5ae584eca,Why is big data not appropriate for this task?,perceived reliance on big data,Training embeddings from small-corpora can increase the performance of some tasks,0.3072434961795807,0.1388680338859558
f68bd65b5251f86e1ed89f0c858a8bb2a02b233a,how do they collect the comparable corpus?,distributions of labmt scores over all unigrams,Randomly from a Twitter dump,0.2959742546081543,0.13846363127231598
f7817b949605fb04b1e4fec9dd9ca8804fb92ae9,Why does not the approach from English work on other languages?,english does not mark grammatical gender,"Because, unlike other languages, English does not mark grammatical genders",0.22694160044193268,0.8688221573829651
f7c34b128f8919e658ba4d5f1f3fc604fb7ff793,What are all the input modalities considered in prior work in question generation?,"text summarization bibref24, image captioning bibref25 and table - to - text generation bibref26","Textual inputs, knowledge bases, and images.",0.453217089176178,0.3692351281642914
fa2a384a23f5d0fe114ef6a39dced139bddac20e,How big is the dataset?,"how big is the dataset? that being said, the presented dataset is currently the only available resource of its kind focusing on the czech court decisions that is freely available to research teams. this significantly reduces the costs necessary to conduct these types of studies involving network analysis, and the similar techniques requiring a large amount of citation data. that being said, it is still difficult to create sufficiently large citation datasets to allow a complex research. in the case of the czech republic, it was difficult to obtain a relevant dataset of the court decisions of the apex courts ( supreme court, supreme administrative court and constitutional court ). due to its size, it is nearly impossible to extract the references manually. one has to reach out for an automation of such task. however, study of court decisions displayed many different ways that courts use to cite even decisions of their own, not to mention the decisions of other courts. the great diversity in citations led us to the use of means of the natural language processing for the recognition and the extraction of the citation data from court decisions of the czech apex courts. methodology : : : dataset and models : : : czcdc 1. 0 dataset novotna and harasta bibref19 prepared a dataset of the court decisions of the czech supreme court, the supreme administrative court and the constitutional court. the dataset contains 237, 723 decisions published between 1st january 1993 and the 30th september 2018",903019 references,0.10643252730369568,0.2629753053188324
fa2ffc6b4b046e17bc41e199855c4941673e2caf,What parallel corpus did they use?,seame phase ii ( south east asia mandarin - english ),Parallel monolingual corpus in English and Mandarin,0.03673553839325905,0.529775857925415
fa3312ae4bbed11a5bebd77caf15d651962e0b26,What was the performance on the self-collected corpus?,,F1 scores of 86.16 on slot filling and 94.56 on intent detection,0.10627856850624084,0.15243394672870636
fb5ce11bfd74e9d7c322444b006a27f2ff32a0cf,What is task success rate achieved? ,when the cube is withing the boundaries of the targeted bowl,96-97.6% using the objects color or shape and 79% using shape alone,0.12577871978282928,0.20940294861793518
fb76e994e2e3fa129f1e94f1b043b274af8fb84c,How does the context-aware variational autoencoder learn event background information?,by using the context - aware latent variable," CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.",0.31233978271484375,0.4330398440361023
fbe5e513745d723aad711ceb91ce0c3c2ceb669e,What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?,"what data / studies do the authors provide to support the assertion that the majority of aggressive conversations contain code - mixed languages? conclusion and future work in this paper, we have briefly described the approach we have taken to solve the aggressive identification on online social media texts which is very challenging since the dataset is noisy and code - mixed. we presented an ensemble of deep learning models which outperform previous approaches by sufficient margin while having the ability to generalize across domains. in future, we will explore other methods to increase the understanding of deep learning models on group targeted text, although the categories are well defined we will look after if we further fine - tune the categories with more data. in the future, we are planning to pay attention on a generalized language model for code - mixed texts which can also handle hindi - code - mixed and other multi - lingual code - mixed datasets ( i. e., trying to reduce the dependencies on language - specific code - mixed resources ). the informal setting / environment of social media often encourage multilingual speakers to switch back and forth between languages when speaking or writing. these all resulted in code - mixing and code - switching. code - mixing refers to the use of linguistic units from different languages in a single utterance or sentence, whereas code - switching refers to the co - occurrence of speech extracts belonging to two different grammatical systemsbibref3. this language interchange makes the grammar more complex and thus it becomes tough to handle it by traditional algorithms. thus the presence of high percentage of code - mixed content in social media text has increased the complexity of the aggression detection task. for example, the dataset provided by the organizers of trac - 2018 bibref0, bibref2 is actually a code - mixed dataset. experiment and evaluation : : : dataset description we have used two datasets in our experimental evaluations : ( 1 ) trac 2018 dataset and ( 2 ) kaggle dataset. trac 2018 dataset : we have used the english code - mixed dataset provided by trac 2018. this dataset contains three labels, ( a ) non - aggressive ( nag ), ( b ) overtly - aggressive ( oag ) and ( c ) covertly - aggressive ( cag ). the distribution of training, validation and test sets are described in table tabref56. kaggle dataset : this dataset contains 20001 tweets which are manually labeled. the labels are divided into two categories ( indicating",None,0.10843390971422195,0.15216046571731567
fbee81a9d90ff23603ee4f5986f9e8c0eb035b52,What are their initial results on this task?,,"Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.",0.021425483748316765,0.16056881844997406
fc3f0eb297b2308b99eb4661a510c9cdbb6ffba2,What is the size of the dataset?,,3029,0.05269911512732506,0.1715158224105835
fc8bc6a3c837a9d1c869b7ee90cf4e3c39bcd102,Is the baseline a non-heirarchical model like BERT?,is the baseline a non - heirarchical model like bert?,There were hierarchical and non-hierarchical baselines; BERT was one of those baselines,0.03919975459575653,0.7751882672309875
fd7f13b63f6ba674f5d5447b6114a201fe3137cb,What language is the experiment done in?,natural,english language,0.6908100843429565,0.2322908639907837
fd8b6723ad5f52770bec9009e45f860f4a8c4321,What QA models were used?,what qa models were used?,"A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.",0.0848155990242958,0.1990305334329605
fe90eec1e3cdaa41d2da55864c86f6b6f042a56c,What are the sources of the data?,,"User reviews written in Chinese collected online for hotel, mobile phone, and travel domains",0.05570404604077339,0.10382489114999771
fee5aef7ae521ccd1562764a91edefecec34624d,How does explicit constraint on the KL divergence term that authors propose looks like?,non - zero positive constraint,"Answer with content missing: (Formula 2) Formula 2 is an answer: 
\big \langle\! \log p_\theta({x}|{z}) \big \rangle_{q_\phi({z}|{x})}  -  \beta |D_{KL}\big(q_\phi({z}|{x}) || p({z})\big)-C|",0.18705925345420837,0.0998462587594986
ff28d34d1aaa57e7ad553dba09fc924dc21dd728,What are their correlation results?,what are their correlation results? the correlation of sum - qe with human judgments is high or very high bibref23 for all s in all datasets,High correlation results range from 0.472 to 0.936,0.029269538819789886,0.5718621015548706
ff2bcf2d8ffee586751ce91cf15176301267b779,What are the characteristics of the city dialect?,,Lexicon of the cities tend to use most forms of a particular concept,0.0715726912021637,0.07833392173051834
