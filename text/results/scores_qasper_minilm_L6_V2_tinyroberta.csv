question_id,question,model_answer,ground_truth,confidence,cosine_sim
0038b073b7cca847033177024f9719c971692042,How is the input triple translated to a slot-filling task?,,"The relation R(x,y) is mapped onto a question q whose answer is y",0.9964858889579773,0.016371265053749084
00bcdffff7e055f99aaf1b05cf41c98e2748e948,What is the baseline method for the task?,,"For the emotion recognition from text they use described neural network as baseline.
For audio and face there is no baseline.",0.9690582156181335,0.023402012884616852
00ef9cc1d1d60f875969094bb246be529373cb1d,What methodology is used to compensate for limited labelled data?,,Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.,0.989429235458374,0.03608883172273636
01123a39574bdc4684aafa59c52d956b532d2e53,By how much does their method outperform state-of-the-art OOD detection?, more than 17 points in precision @ k on average,"AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average",0.43511828780174255,0.15938924252986908
01dc6893fc2f49b732449dfe1907505e747440b0,What debate topics are included in the dataset?,,"Ethics, Gender, Human rights, Sports, Freedom of Speech, Society, Religion, Philosophy, Health, Culture, World, Politics, Environment, Education, Digital Freedom, Economy, Science and Law",0.9938430786132812,0.005993416998535395
02348ab62957cb82067c589769c14d798b1ceec7,What simpler models do they look at?,,"BiGRUs with attention, ROUGE, Language model, and next sentence prediction ",0.970452606678009,0.13931655883789062
02417455c05f09d89c2658f39705ac1df1daa0cd,How much does it minimally cost to fine-tune some model according to benchmarking framework?,,"$1,728",0.9741413593292236,0.06557254493236542
02e4bf719b1a504e385c35c6186742e720bcb281,How are relations used to propagate polarity?,,"based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ",0.8999172449111938,0.02543461136519909
03ce42ff53aa3f1775bc57e50012f6eb1998c480,What 6 language pairs is experimented on?,,"EN<->ES
EN<->DE
EN<->IT
EN<->EO
EN<->MS
EN<->FI",0.9914387464523315,0.07846681773662567
04012650a45d56c0013cf45fd9792f43916eaf83,How much is performance hurt when using too small amount of layers in encoder?,,"comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ",0.9566992521286011,0.009332703426480293
0457242fb2ec33446799de229ff37eaad9932f2a,Which elements of the platform are modular?,,"handling large volume incoming data, sentiment analysis on tweets and predictive online learning",0.9573174118995667,0.07209131121635437
04b43deab0fd753e3419ed8741c10f652b893f02,What are the two decoding functions?, a linear projection,"a linear projection and a bijective function with continuous transformation though  ‘affine coupling layer’ of (Dinh et al.,2016). ",0.18055880069732666,0.6594761610031128
04f72eddb1fc73dd11135a80ca1cf31e9db75578,How much more coverage is in the new dataset?,,278 more annotations,0.9917253851890564,0.1528910994529724
05671d068679be259493df638d27c106e7dd36d0,What is the performance proposed model achieved on MathQA?,,"Operation accuracy: 71.89
Execution accuracy: 55.95",0.4595896601676941,0.0959615632891655
056fc821d1ec1e8ca5dc958d14ea389857b1a299,How many feature maps are generated for a given triple?,,3 feature maps for a given tuple,0.9015812277793884,0.13143290579319
06095a4dee77e9a570837b35fc38e77228664f91,"Is all text in this dataset a question, or are there unrelated sentences in between questions?",,the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ,0.9960423707962036,-0.018355248495936394
068dbcc117c93fa84c002d3424bafb071575f431,How was quality measured?, ua metric with 5 generators per predicate,"Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.",0.2277170568704605,0.061744824051856995
07580f78b04554eea9bb6d3a1fc7ca0d37d5c612,Can the approach be generalized to other technical domains as well? ,,"There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.",0.7629944086074829,0.024134356528520584
07c59824f5e7c5399d15491da3543905cfa5f751,How big is dataset used for training/testing?,,"4,261  days for France and 4,748 for the UK",0.4765796959400177,0.013602903112769127
0828cfcf0e9e02834cc5f279a98e277d9138ffd9,How was the dataset collected?,,extracted text from Sorani Kurdish books of primary school and randomly created sentences,0.010143604129552841,0.038688771426677704
08333e4dd1da7d6b5e9b645d40ec9d502823f5d7,How much performance gap between their approach and the strong handcrafted method?,,"0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C",0.9950871467590332,-0.04872046783566475
085147cd32153d46dd9901ab0f9195bfdbff6a85,What are the baseline models?,,"MC-CNN
MVCNN
CNN",0.7642494440078735,0.15150848031044006
093039f974805952636c19c12af3549aa422ec43,Is this library implemented into Torch or is framework agnostic?, agnostic,It uses deep learning framework (pytorch),0.9518540501594543,0.006341117434203625
097ab15f58cb1fce5b5ffb5082b8d7bbee720659,Which language has the lowest error rate reduction?,,thai,0.9944130778312683,0.24923747777938843
0a3a8d1b0cbac559f7de845d845ebbfefb91135e,What accuracy is achieved by the speech recognition system?,,"Accuracy not available: WER results are reported 42.6 German, 35.9 English",0.9957307577133179,0.06139286980032921
0a7ac8eccbc286e0ab55bc5949f3f8d2ea2d1a60,How many improvements on the French-German translation benchmark?,,one,0.9977605938911438,0.44960346817970276
0b31eb5bb111770a3aaf8a3931d8613e578e07a8,"What are the selection criteria for ""causal statements""?",,"Presence of only the exact unigrams 'caused', 'causing', or 'causes'",0.982245683670044,0.08478128910064697
0b411f942c6e2e34e3d81cc855332f815b6bc123,What's the method used here?, advantage actor critic bibref29,Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8.,0.24417448043823242,0.2992345690727234
0bd864f83626a0c60f5e96b73fb269607afc7c09,How are sentence embeddings incorporated into the speech recognition system?,,BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer.,0.31703829765319824,0.06288012862205505
0c09a0e8f9c5bdb678563be49f912ab6e3f97619,How many roles are proposed?,,12,0.7279389500617981,0.33374133706092834
0c7823b27326b3f5dff51f32f45fc69c91a4e06d,In which setting they achieve the state of the art?,,in open-ended task esp. for counting-type questions ,0.7115077376365662,0.0379527248442173
0d7de323fd191a793858386d7eb8692cc924b432,What writing styles are present in the corpus?,,"current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.",0.877587616443634,0.18690253794193268
0da6cfbc8cb134dc3d247e91262f5050a2200664,What topic clusters are identified by LDA?,,"Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club",0.44911518692970276,0.071526899933815
0fcac64544842dd06d14151df8c72fc6de5d695c,What previous methods is the proposed method compared against?,,"BLSTM+Attention+BLSTM
Hierarchical BLSTM-CRF
CRF-ASN
Hierarchical CNN (window 4)
mLSTM-RNN
DRLM-Conditional
LSTM-Softmax
RCNN
CNN
CRF
LSTM
BERT",0.9930582046508789,0.07516014575958252
0fd678d24c86122b9ab27b73ef20216bbd9847d1,What evaluation metrics are used?,,Accuracy on each dataset and the average accuracy on all datasets.,0.986407458782196,0.04160941764712334
10ddc5caf36fe9d7438eb5a3936e24580c4ffe6a,Which competitive relational classification models do they test?,,For relation prediction they test TransE and for relation extraction they test position aware neural sequence model,0.8783299326896667,-0.024442564696073532
1165fb0b400ec1c521c1aef7a4e590f76fee1279,How do they model travel behavior?,,The data from collected travel surveys is used to model travel behavior.,0.8513704538345337,0.02133302018046379
11d2f0d913d6e5f5695f8febe2b03c6c125b667c,How is performance of this system measured?,,using the BLEU score as a quantitative metric and human evaluation for quality,0.4074975550174713,0.05520669370889664
126e8112e26ebf8c19ca7ff3dd06691732118e90,What are simulated datasets collected?,,There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents,0.986555814743042,0.05654282867908478
12ac76b77f22ed3bcb6430bcd0b909441d79751b,What are the competing models?,,"TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.",0.9896736741065979,0.03179952874779701
12cfbaace49f9363fcc10989cf92a50dfe0a55ea,what results do they achieve?, 96. 37 mean inlineform0,91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task,0.312817245721817,0.27670353651046753
12eaaf3b6ebc51846448c6e1ad210dbef7d25a96,How many convolutional layers does their model have?,,wav2vec has 12 convolutional layers,0.9044033885002136,0.011975185945630074
13d92cbc2c77134626e26166c64ca5c00aec0bf5,What baseline approaches do they compare against?,,"HotspotQA: Yang, Ding, Muppet
Fever: Hanselowski, Yoneda, Nie",0.3940016031265259,0.14853693544864655
13e87f6d68f7217fd14f4f9a008a65dd2a0ba91c,What is the performance of the model?,,"Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates
Experiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)",0.5963937640190125,0.011384350247681141
14634943d96ea036725898ab2e652c2948bd33eb,What is the accuracy of the model for the six languages tested?,,"Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)",0.983745813369751,0.024534782394766808
14e259a312e653f8fc0d52ca5325b43c3bdfb968,"Is any data-to-text generation model trained on this new corpus, what are the results?"," an explosion of neural data - to - text generation models, which depend on large training data. these are typically trained on one of the few parallel corpora publicly available","Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.",0.09277180582284927,0.06703859567642212
14eb2b89ba39e56c52954058b6b799a49d1b74bf,How are their changes evaluated?,,The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset,0.9667109251022339,0.038432296365499496
157b9f6f8fb5d370fa23df31de24ae7efb75d6f3,How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?,,"They achieved best result in the PAN 2017 shared task with accuracy for Variety prediction task 0.0013 more than the 2nd best baseline, accuracy for Gender prediction task 0.0029 more than 2nd best baseline and accuracy for Joint prediction task 0.0101 more than the 2nd best baseline",0.9189431667327881,0.06430839747190475
160e6d2fc6e04bb0b4ee8d59c06715355dec4a17,What accuracy score do they obtain?," f1 - score as accuracy is often misleading for dataset with unbalanced class distribution. however, for completeness sake, all measures are reported. proposed model and experimentation : : : evaluation metrics we employed the standard metrics that are widely adapted in the literature for measuring multi - class classification performance. these metrics are accuracy, precision, recall, and f1 - score",the best performing model obtained an accuracy of 0.86,0.1865568906068802,0.27158084511756897
16af38f7c4774637cf8e04d4b239d6d72f0b0a3a,How large is the dataset?,,over 104k documents,0.9897977709770203,0.10633863508701324
16f71391335a5d574f01235a9c37631893cd3bb0, What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?,,"Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)",0.9829906225204468,0.05938595160841942
171ebfdc9b3a98e4cdee8f8715003285caeb2f39,How better is accuracy of new model compared to previously reported models?,,"Average accuracy of proposed model vs best prevous result:
Single-task Training: 57.57 vs 55.06
Multi-task Training: 50.17 vs 50.59",0.74949711561203,0.07158612459897995
1771a55236823ed44d3ee537de2e85465bf03eaf,What is the difference in recall score between the systems?,,"Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.",0.24802808463573456,0.06175465136766434
18288c7b0f8bd7839ae92f9c293e7fb85c7e146a,How strong was the correlation between exercise and diabetes?,,weak correlation with p-value of 0.08,0.9904986619949341,-0.04861513152718544
182b6d77b51fa83102719a81862891f49c23a025,What limitations are mentioned?,,"deciding publisher partisanship, risk annotator bias because of short description text provided to annotators",0.7419387698173523,0.07394100725650787
1959e0ebc21fafdf1dd20c6ea054161ba7446f61,How is the clinical text structuring task defined?, a final problem,CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.,0.6072037220001221,0.05334346368908882
19c9cfbc4f29104200393e848b7b9be41913a7ac,How many questions are in the dataset?,,"2,714 ",0.8699456453323364,-0.06923511624336243
1a69696034f70fb76cd7bb30494b2f5ab97e134d,By how much does their model outperform existing methods?,,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result.,0.9927737712860107,0.010613314807415009
1a8b7d3d126935c09306cacca7ddb4b953ef68ab,What were their results?,,Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set,0.9266588687896729,0.032418400049209595
1adbdb5f08d67d8b05328ccc86d297ac01bf076c,What languages do they use?,,"Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.",0.9807804822921753,-0.040995415300130844
1b9119813ea637974d21862a8ace83bc1acbab8e,What dataset do they use?,,They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).,0.9694969058036804,-0.026819121092557907
1baf87437b70cc0375b8b7dc2cfc2830279bc8b5,"What is the source of the ""control"" corpus?",,"Randomly selected from a Twitter dump, temporally matched to causal documents",0.5106299519538879,0.08076354116201401
1c4cd22d6eaefffd47b93c2124f6779a06d2d9e1,How big is their dataset?,,"3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing",0.9130274057388306,0.1078658252954483
1c68d18b4b65c4d75dc199d2043079490f6310f8,What are the two PharmaCoNER subtasks?,,Entity identification with offset mapping and concept indexing,0.9959348440170288,-0.0006707286229357123
1cbca15405632a2e9d0a7061855642d661e3b3a7,How much improvement do they get?, seven times,Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.,0.2404981106519699,0.01916745863854885
1d74fd1d38a5532d20ffae4abbadaeda225b6932,What is their f1 score and recall?,,"F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.",0.9905470013618469,0.03425204008817673
1ed6acb88954f31b78d2821bb230b722374792ed,What is private dashboard?,,Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).,0.93975830078125,0.12387262284755707
1f63ccc379f01ecdccaa02ed0912970610c84b72,How much is the gap between using the proposed objective and using only cross-entropy objective?,,The mixed objective improves EM by 2.5% and F1 by 2.2%,0.9852727651596069,0.08144988119602203
1fb73176394ef59adfaa8fc7827395525f9a5af7,Where did they get training data?,,AmazonQA and ConciergeQA datasets,0.9865076541900635,0.08691820502281189
20ec88c45c1d633adfd7bff7bbf3336d01fb6f37,what are the evaluation metrics?,,"Precision, Recall, F1",0.9334774017333984,0.13076847791671753
2122bd05c03dde098aa17e36773e1ac7b6011969,What task do they evaluate on?,,Fill-in-the-blank natural language questions,0.9872581958770752,0.09702499210834503
21663d2744a28e0d3087fbff913c036686abbb9a,How does their model differ from BERT?,,Their model does not differ from BERT.,0.6840518712997437,0.020247135311365128
2210178facc0e7b3b6341eec665f3c098abef5ac,What type of recurrent layers does the model use?,,GRU,0.9724510908126831,0.39389267563819885
22b8836cb00472c9780226483b29771ae3ebdc87,What is the new initialization method proposed in this paper?, skip - gram with negative - sampling ( sgns ) algorithm by mikolov2013distributed that simultaneously trains both word and entity vectors,They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.,0.13445845246315002,0.5474939346313477
22c802872b556996dd7d09eb1e15989d003f30c0,How do they correlate NED with emotional bond levels?,,They compute Pearson’s correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating,0.31513842940330505,-0.008697917684912682
234ccc1afcae4890e618ff2a7b06fc1e513ea640,How big is performance improvement proposed methods are used?,,"Data augmentation (es)  improved Adv es by 20% comparing to baseline 
Data augmentation (cs) improved Adv cs by 16.5% comparing to baseline
Data augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline 
All models show improvements over adversarial sets  
",0.952073335647583,0.11762218922376633
2376c170c343e2305dac08ba5f5bda47c370357f,How was the dataset collected?, in a machine - to - machine fashion,"They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ",0.4481680989265442,0.30788370966911316
238ec3c1e1093ce2f5122ee60209b969f7669fae,How is the fluctuation in the sense of the word and its neighbors measured?,,"Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.
4) Computing the mean m and the sample variance σ for the uniformities of ai .
5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.",0.32138895988464355,0.06254834681749344
23d32666dfc29ed124f3aa4109e2527efa225fbc,Do the use word embeddings alone or they replace some previous features of the model with word embeddings?,,They use it as addition to previous model - they add new edge between words if word embeddings are similar.,0.9924875497817993,0.0612463541328907
25b2ae2d86b74ea69b09c140a41593c00c47a82b,How were the navigation instructions collected?,,using Amazon Mechanical Turk using simulated environments with topological maps,0.8098015785217285,0.10208760201931
25fd61bb20f71051fe2bd866d221f87367e81027,What baselines have been used in this work?,What baselines have been used in this work?the main contributions of this work are preprocessing and hyper - parameters,"NDM, LIDM, KVRN, and TSCP/RL",0.37239348888397217,0.21023233234882355
26c290584c97e22b25035f5458625944db181552,What is the size of their dataset?,,"10,001 utterances",0.9865851998329163,0.1054321825504303
26faad6f42b6d628f341c8d4ce5a08a591eea8c2,How many documents are in the Indiscapes dataset?,,508,0.9963993430137634,0.2773895859718323
2815bac42db32d8f988b380fed997af31601f129,What is improvement in accuracy for short Jokes in relation other types of jokes?, 8 percent,It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%,0.37405925989151,0.29741597175598145
281cd4e78b27a62713ec43249df5000812522a89,What is the average length of the claims?,,Average claim length is 8.9 tokens.,0.9923304915428162,0.03911704197525978
2869d19e54fb554fcf1d6888e526135803bb7d75,What performance did they obtain on the SemEval dataset?,,F1 score of 82.10%,0.9747471213340759,0.08331742882728577
28b2a20779a78a34fb228333dc4b93fd572fda15,Does proposed end-to-end approach learn in reinforcement or supervised learning manner?,,supervised learning,0.8519627451896667,0.14695055782794952
29d917cc38a56a179395d0f3a2416fca41a01659,How are the potentially relevant text fragments identified?,," Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5–10 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.",0.9930824041366577,0.08886004239320755
2a46db1b91de4b583d4a5302b2784c091f9478cc,How many examples do they have in the target domain?,,"Around 388k examples, 194k from tst2013 (in-domain) and 194k from newstest2014 (out-of-domain)",0.9741762280464172,0.10446146130561829
2a6469f8f6bf16577b590732d30266fd2486a72e,What is novel in author's approach?,,"They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data.",0.9955050945281982,0.03546101972460747
2cf8825639164a842c3172af039ff079a8448592,How is the data annotated?,,The data are self-reported by Twitter users and then verified by two human experts.,0.9918689131736755,0.031225062906742096
2d274c93901c193cf7ad227ab28b1436c5f410af,What are the baselines that Masque is compared against?,,"BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D",0.9957181215286255,0.13630357384681702
2d307b43746be9cedf897adac06d524419b0720b,How long are the datasets?,,"Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses",0.9736809730529785,-0.017003260552883148
2d4d0735c50749aa8087d1502ab7499faa2f0dd8,By how much do they outperform previous state-of-the-art models?,,"Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)",0.9854938387870789,0.05684233829379082
2d536961c6e1aec9f8491e41e383dc0aac700e0a,What are all 15 types of modifications ilustrated in the dataset?,,"- paraphrase 1
- paraphrase 2
- different meaning
- opposite meaning
- nonsense
- minimal change
- generalization
- gossip
- formal sentence
- non-standard sentence
- simple sentence
- possibility
- ban
- future
- past",0.9960078597068787,0.09196223318576813
2df2f6e4efd19023434c84f5b4f29a2f00bfc9fb,What other non-neural baselines do the authors compare to? ,,"bag of words, tf-idf, bag-of-means",0.9856113791465759,0.1418227255344391
2e1660405bde64fb6c211e8753e52299e269998f,How long is the dataset?,,"645, 600000",0.35136497020721436,0.09842823445796967
2e1ededb7c8460169cf3c38e6cde6de402c1e720,What is the prediction accuracy of the model?, 0. 32 %,"mean prediction accuracy 0.99582651
S&P 500 Accuracy 0.99582651",0.3378223180770874,0.3078581392765045
2ec97cf890b537e393c2ce4c2b3bd05dfe46f683,How do they measure correlation between the prediction and explanation quality?,,They look at the performance accuracy of explanation and the prediction performance,0.7297347784042358,0.11329583823680878
3070d6d6a52aa070f0c0a7b4de8abddd3da4f056,How do they measure performance of language model tasks?,,"BPC, Perplexity",0.6951897144317627,0.07400587946176529
30803eefd7cdeb721f47c9ca72a5b1d750b8e03b,How well does their system perform on the development set of SRE?,,"EER 16.04, Cmindet 0.6012, Cdet 0.6107",0.919003427028656,0.0718645304441452
30b5e5293001f65d2fb9e4d1fdf4dc230e8cf320,What text classification task is considered?,,To classify a text as belonging to one of the ten possible classes.,0.9866246581077576,0.03143945708870888
30dad5d9b4a03e56fa31f932c879aa56e11ed15b,What is the 12 class bilingual text?,,"Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant",0.9979529976844788,0.2018318772315979
3103502cf07726d3eeda34f31c0bdf1fc0ae964e,How do Zipf and Herdan-Heap's laws differ?,,"Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)",0.9133966565132141,0.012361547909677029
311a7fa62721e82265f4e0689b4adc05f6b74215,How do they define upward and downward reasoning?,,"Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.",0.8316835761070251,0.05925959721207619
3213529b6405339dfd0c1d2a0f15719cdff0fa93,What is the baseline model used?, vanilla bert model,"The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data",0.26891303062438965,0.46027871966362
327e06e2ce09cf4c6cc521101d0aecfc745b1738,What evaluation metrics did they look at?,,accuracy with standard deviation,0.9409334063529968,-0.03496139124035835
327e6c6609fbd4c6ae76284ca639951f03eb4a4c,By how much is performance on CN-Celeb inferior to performance on VoxCeleb?,,"For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb",0.8517646193504333,0.014275512658059597
32e8eda2183bcafbd79b22f757f8f55895a0b7b2,How many categories of offensive language were there?,,3,0.9045805335044861,0.49110352993011475
334f90bb715d8950ead1be0742d46a3b889744e7,What semantic features help in detecting whether a piece of text is genuine or generated? of ," factually correct, in addition to its statistical properties","No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.",0.12123309820890427,0.30186858773231506
33d864153822bd378a98a732ace720e2c06a6bc6,What is new state-of-the-art performance on CoNLL-2009 dataset?,,In closed setting 84.22 F1 and in open 87.35 F1.,0.9768631458282471,0.07172469049692154
33f72c8da22dd7d1378d004cbd8d2dcd814a5291,What is the metric that is measures in this paper?, eq. 7,error rate in a minimal pair ABX discrimination task,0.11139578372240067,0.08762503415346146
34af2c512ec38483754e94e1ea814aa76552d60a,What benchmarks are created?,,Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples,0.8926027417182922,0.01260537188500166
34fab25d9ceb9c5942daf4ebdab6c5dd4ff9d3db,What dataset did they use?, chinese nlp,"weibo-100k, Ontonotes, LCQMC and XNLI",0.5636122822761536,0.28296512365341187
36a9230fadf997d3b0c5fc8af8d89bd48bf04f12,Which model architecture do they for sentence encoding?,,"Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs
- RNN",0.3468652069568634,0.10041474550962448
36b25021464a9574bf449e52ae50810c4ac7b642,Where does the information on individual-level demographics come from?, passively sensed social data,From Twitter profile descriptions of the users.,0.7022359371185303,0.22171777486801147
36feaac9d9dee5ae09aaebc2019b014e57f61fbf,How do they measure model size?,,By the number of parameters.,0.8507716655731201,0.014869076199829578
3703433d434f1913307ceb6a8cfb9a07842667dd,What learning paradigms do they cover in this survey?,,"Considering ""What"" and ""How"" separately versus jointly optimizing for both.",0.9283406138420105,0.009681389667093754
3748787379b3a7d222c3a6254def3f5bfb93a60e,What linguistic quality aspects are addressed?,,"Grammaticality, non-redundancy, referential clarity, focus, structure & coherence",0.9869438409805298,0.03923463076353073
37bc8763eb604c14871af71cba904b7b77b6e089,How is module that analyzes behavioral state trained?,"How is module that analyzes behavioral state trained?we propose to augment rnn language models with a behavior model that provides information relating to a speaker ' s psychological state. this behavioral information is combined with hidden layers of the rnnlm through a gating mechanism prior to output prediction of the next word. in contrast to typical language models, we propose to model  where  for an rnn function . the behavior model is implemented with a multi - layered rnn over the input sequence of words. the first recurrent layer of the behavior model is initialized with pre - trained weights from the model described in section secref3 and fixed during language modeling training. an overview of the proposed behavior gated language model is shown in figure figref6 experimental setup : : : behavior model the behavior model was implemented using an rnn with lstm units and trained with the couples therapy corpus. out of the 33 behavioral codes included in the corpus we applied the behaviors acceptance, blame, negativity, positivity, and sadness to train our models. this is motivated from previous works which showed good separability in these behaviors as well as being easy to interpret. the behavior model is pre - trained to identify the presence of each behavior from a sequence of words using a multi - label classification scheme",pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus,0.4177264869213104,0.5269547700881958
37c7c62c9216d6cf3d0858cf1deab6db4b815384,how was annotation done?, by distributing it over dozens of people,Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations,0.15931451320648193,0.13415977358818054
384d571e4017628ebb72f3debb2846efaf0cb0cb,On what dataset is Aristo system trained?,,"Aristo Corpus
Regents 4th
Regents 8th
Regents `12th
ARC-Easy
ARC-challenge ",0.9900720715522766,0.12523241341114044
38a5cc790f66a7362f91d338f2f1d78f48c1e252,What baseline is used?,What baseline is used?svm baseline : the baseline classifier uses a linear support vector machine bibref7,SVM,0.1513315588235855,0.54767906665802
38c74ab8292a94fc5a82999400ee9c06be19f791,How large is the corpus?,,"It contains 106,350 documents",0.8913106918334961,0.03832109645009041
39a450ac15688199575798e72a2cc016ef4316b5,How much performance improvements they achieve on SQuAD?,,Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ,0.9751389026641846,0.020151013508439064
39f8db10d949c6b477fa4b51e7c184016505884f,How does their model learn using mostly raw data?,,by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity,0.5787076354026794,0.09038527309894562
3a3a65c65cebc2b8c267c334e154517d208adc7d,What extraction model did they use?,,"Multi-Encoder, Constrained-Decoder model",0.9802825450897217,0.08379200845956802
3aa7173612995223a904cc0f8eef4ff203cbb860,What baseline models do they compare against?,,"SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)",0.9878787398338318,0.07376785576343536
3aee5c856e0ee608a7664289ffdd11455d153234,What was the performance of their model?, increasing accuracy by 35 % and 25 %,"For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81",0.10516304522752762,0.21463538706302643
3b391cd58cf6a61fe8c8eff2095e33794e80f0e3,What is the dataset used in the paper?,,"historical S&P 500 component stocks
 306242 news articles",0.9806880354881287,0.10149140655994415
3b745f086fb5849e7ce7ce2c02ccbde7cfdedda5,By how much do they outperform other models in the sentiment in intent classification tasks?,,In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average,0.5241329073905945,0.05422335863113403
3b995a7358cefb271b986e8fc6efe807f25d60dc,What types of word representations are they evaluating?,,GloVE; SGNS,0.9837194085121155,0.20303867757320404
3bfdbf2d4d68e01bef39dc3371960e25489e510e,how do they measure discussion quality?," stepwise regressions, we found that the best model of discussion quality ( r - squared of inlineform0","Measuring three aspects: argumentation, specificity and knowledge domain.",0.09030834585428238,0.38597628474235535
3d49b678ff6b125ffe7fb614af3e187da65c6f65,"What does ""explicitly leverages their probabilistic correlation to guide the training process of both models"" mean?",,"The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization.",0.9355036020278931,0.07725462317466736
3d6015d722de6e6297ba7bfe7cb0f8a67f660636,What are the 12 categories devised?,,"Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study",0.9963887333869934,0.025937819853425026
3e839783d8a4f2fe50ece4a9b476546f0842b193,What was their result on Stance Sentiment Emotion Corpus?,,F1 score of 66.66%,0.9707321524620056,0.06734000146389008
3f326c003be29c8eac76b24d6bba9608c75aa7ea,What evaluation metric is used?,,F1 and Weighted-F1,0.97235107421875,0.08578142523765564
3f3c09c1fd542c1d9acf197957c66b79ea1baf6e,How many annotators participated?,,1,0.2504497766494751,0.4767308831214905
3f5f74c39a560b5d916496e05641783c58af2c5d,How are the synthetic examples generated?,,"Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out",0.9514512419700623,0.11667656898498535
3fb4334e5a4702acd44bd24eb1831bb7e9b98d31,How big are the datasets used?, under 25k,"Evaluation datasets used:
CMRC 2018 - 18939 questions, 10 answers
DRCD - 33953 questions, 5 answers
NIST MT02/03/04/05/06/08 Chinese-English - Not specified

Source language train data:
SQuAD - Not specified",0.5999203324317932,0.10761282593011856
405964517f372629cda4326d8efadde0206b7751,How is performance measured?,,they use ROC curves and cross-validation,0.5675510764122009,0.05436629056930542
40c2bab4a6bf3c0628079fcf19e8b52f27f51d98,How do they generate the synthetic dataset?,,using generative process,0.5140483379364014,0.04574396088719368
415f35adb0ef746883fb9c33aa53b79cc4e723c3,"In the targeted data collection approach, what type of data is targetted?",,Gendered characters in the dataset,0.31858527660369873,0.04887740686535835
41d3ab045ef8e52e4bbe5418096551a22c5e9c43,what datasets were used?," iwslt14 german - english, turkish - english, and wmt14 english - german","IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German",0.7429635524749756,0.96685791015625
41e300acec35252e23f239772cecadc0ea986071,What neural machine translation models can learn in terms of transfer learning?,,Multilingual Neural Machine Translation Models,0.786014974117279,0.03737089782953262
4226a1830266ed5bde1b349205effafe7a0e2337,What meta-information is being transferred?,,"high-order representation of a relation, loss gradient of relation meta",0.5927014350891113,0.08824369311332703
42394c54a950bae8cebecda9de68ee78de69dc0d,What is the source of external knowledge?,,counts of predicate-argument tuples from English Wikipedia,0.8097068071365356,0.08337201178073883
427252648173c3ba78c211b86fa89fc9f4406653,What domains are detected in this paper?,,"Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW),
“Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). Exceptionally, GEN does
not have a natural root category.",0.8147255182266235,0.0902826115489006
43f86cd8aafe930ebb35ca919ada33b74b36c7dd,In what way is the input restructured?,,"In four entity-centric ways - entity-first, entity-last, document-level and sentence-level",0.8722316026687622,0.06600499153137207
445e792ce7e699e960e2cb4fe217aeacdd88d392,How do this framework facilitate demographic inference from social media?," by developing a multimodal framework and employing statistical techniques for fusing heterogeneous sets of features obtained by processing visual, textual and user interaction data",Demographic information is predicted using weighted lexicon of terms.,0.08590329438447952,0.32787325978279114
44c4bd6decc86f1091b5fc0728873d9324cdde4e,How big is the Japanese data?,,"7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus",0.9926273822784424,0.06273573637008667
4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb,what amounts of size were used on german-english?,,"Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development",0.6960489153862,0.04316597804427147
45893f31ef07f0cca5783bd39c4e60630d6b93b3,How do they select monotonicity facts?,,They derive it from Wordnet,0.9459538459777832,0.09467889368534088
45a2ce68b4a9fd4f04738085865fbefa36dd0727,what dataset was used?,,The dataset from a joint ADAPT-Microsoft project,0.7403320670127869,0.02388242445886135
4640793d82aa7db30ad7b88c0bf0a1030e636558,what previous systems were compared to?,,"Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Søgaard and Goldberg (2016) ",0.9943826794624329,0.061092574149370193
4688534a07a3cbd8afa738eea02cc6981a4fd285,How do they combine MonaLog with BERT?,,They use Monalog for data-augmentation to fine-tune BERT on this task,0.9207434058189392,0.060814887285232544
4704cbb35762d0172f5ac6c26b67550921567a65,By how much does transfer learning improve performance on this task?,,"In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%",0.7309502363204956,-0.026606010273098946
471d624498ab48549ce492ada9e6129da05debac,What context modelling methods are evaluated?, 13,"Concat
Turn
Gate
Action Copy
Tree Copy
SQL Attn
Concat + Action Copy
Concat + Tree Copy
Concat + SQL Attn
Turn + Action Copy
Turn + Tree Copy
Turn + SQL Attn
Turn + SQL Attn + Action Copy",0.3469145596027374,0.07875534892082214
496b4ae3c0e26ec95ff6ded5e6790f24c35f0f5b,How do they incorporate human advice?,,by converting human advice to first-order logic format and use as an input to calculate gradient,0.7096962928771973,0.03161289170384407
4a4616e1a9807f32cca9b92ab05e65b05c2a1bf5,What were the sizes of the test sets?,,Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences,0.9915037751197815,0.019265050068497658
4a61260d6edfb0f93100d92e01cf655812243724,Which 3 NLP areas are cited the most?,,"machine translation, statistical machine, sentiment analysis",0.9933216571807861,0.06192133575677872
4b41f399b193d259fd6e24f3c6e95dc5cae926dd,How big dataset is used for training this system?,,"For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.",0.8930801153182983,0.15735186636447906
4b8257cdd9a60087fa901da1f4250e7d910896df,How do the authors define or exemplify 'incorrect words'?,,typos in spellings or ungrammatical words,0.9869639873504639,0.1186121329665184
4b8a0e99bf3f2f6c80c57c0e474c47a5ee842b2c,To what other competitive baselines is this approach compared?, previous state - of - the - art models vhred ( attn ) and reranking - rl,"LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL",0.25815579295158386,0.5561475157737732
4c50f75b1302f749c1351de0782f2d658d4bea70,How is quality of annotation measured?,,Annotators went through various phases to make sure their annotations did not deviate from the mean.,0.6840599775314331,0.045442208647727966
4c7ac51a66c15593082e248451e8f6896e476ffb,What is the performance proposed model achieved on AlgoList benchmark?,,"Full Testing Set Accuracy: 84.02
Cleaned Testing Set Accuracy: 93.48",0.9902205467224121,0.0890677347779274
4c822bbb06141433d04bbc472f08c48bc8378865,How do they extract causality from text?,,"They identify documents that contain the unigrams 'caused', 'causing', or 'causes'",0.9895622730255127,0.03498562425374985
4ca0d52f655bb9b4bc25310f3a76c5d744830043,How large is the first dataset?,,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,0.9946039319038391,0.17945662140846252
4d5e2a83b517e9c082421f11a68a604269642f29,how many domains did they experiment with?,,2,0.9729438424110413,0.45951545238494873
4dad15fee1fe01c3eadce8f0914781ca0a6e3f23,How do they prevent the model complexity increasing with the increased number of slots?, sim has many fewer parameters,They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).,0.2079256922006607,0.19813886284828186
4e9684fd68a242cb354fa6961b0e3b5c35aae4b6,What is the results of multimodal compared to unimodal models?,,"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4 ",0.8674248456954956,0.05292574688792229
4ef2fd79d598accc54c084f0cca8ad7c1b3f892a,What is the size of their collected dataset?,,3347 unique utterances ,0.9715434312820435,0.10848522931337357
5067e5eb2cddbb34b71e8b74ab9210cd46bb09c5,Which matching features do they employ?," bibref5, bibref9",Matching features from matching sentences from various perspectives.,0.8411799669265747,0.15603341162204742
50716cc7f589b9b9f3aca806214228b063e9695b,What language technologies have been introduced in the past?,,"- Font & Keyboard
- Speech-to-Text
- Text-to-Speech
- Text Prediction
- Spell Checker
- Grammar Checker
- Text Search
- Machine Translation
- Voice to Text Search
- Voice to Speech Search",0.9907125234603882,0.09037508815526962
51aaec4c511d96ef5f5c8bae3d5d856d8bc288d3,What baselines did they compare their model with?,,the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search,0.5491905212402344,0.05970754846930504
51de39c8bad62d3cbfbec1deb74bd8a3ac5e69a8,Which modifications do they make to well-established Seq2seq architectures?,,"Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible",0.32264065742492676,0.09610212594270706
521a7042b6308e721a7c8046be5084bc5e8ca246,What is a confusion network or lattice?, lattices using consensus decoding bibref7. the word error rates of the 1 - best sequences are 39. 9 % for lattices and 38. 5 % for confusion networks,"graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences",0.08345267921686172,0.4266700744628906
52e8f79814736fea96fd9b642881b476243e1698,What systems are tested?,,"BULATS i-vector/PLDA
BULATS x-vector/PLDA
VoxCeleb x-vector/PLDA
PLDA adaptation (X1)
 Extractor fine-tuning (X2) ",0.9480866193771362,0.044450268149375916
52f7e42fe8f27d800d1189251dfec7446f0e1d3b,How much better is performance of proposed method than state-of-the-art methods in experiments?, significantly outperforms,"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.",0.4757750332355499,0.22655053436756134
53a0763eff99a8148585ac642705637874be69d4,How does the active learning model work?, mainly aims to ease the data collection process,"Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.",0.0637182742357254,0.19888481497764587
53bf6238baa29a10f4ff91656c470609c16320e1,What is the source of the textual data? ,,Users' tweets,0.9254401326179504,0.18912279605865479
540e9db5595009629b2af005e3c06610e1901b12,How was a quality control performed so that the text is noisy but the annotations are accurate?,,The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.,0.9816752672195435,0.01474820077419281
5455b3cdcf426f4d5fc40bc11644a432fa7a5c8f,What do they mean by answer styles?,,well-formed sentences vs concise answers,0.9906079769134521,0.07681803405284882
545ff2f76913866304bfacdb4cc10d31dbbd2f37,What data were they used to train the multilingual encoder?,,WMT 2014 En-Fr parallel corpus,0.9663125276565552,0.09717956930398941
54c7fc08598b8b91a8c0399f6ab018c45e259f79,How better is performance compared to competitive baselines?, significantly outperforms,"Proposed method vs best baseline result on Vecmap (Accuracy P@1):
EN-IT: 50 vs 50
IT-EN: 42.67 vs 42.67
EN-DE: 51.6 vs 51.47
DE-EN: 47.22 vs 46.96
EN-FI: 35.88 vs 36.24
FI-EN: 39.62 vs 39.57
EN-ES: 39.47 vs 39.30
ES-EN: 36.43 vs 36.06",0.33692291378974915,0.25991761684417725
54c9147ffd57f1f7238917b013444a9743f0deb8,Which are the sequence model architectures this method can be transferred across?," lstm - based, the cnn - based, and the transformer - based",The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models,0.1119360700249672,0.5568974018096924
54fa5196d0e6d5e84955548f4ef51bfd9b707a32,"Are this techniques used in training multilingual models, on what languages?", multilingual nmt models are generally trained by mixing language pairs in a predetermined fashion,English to French and English to German,0.17361415922641754,0.3036167621612549
54fe8f05595f2d1d4a4fd77f4562eac519711fa6,How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?,,Systems do not perform well both in Facebook and Twitter texts,0.9901034832000732,0.06410849839448929
551f77b58c48ee826d78b4bf622bb42b039eca8c,What are the weaknesses of their proposed interpretability quantification method?,,can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories,0.9784655570983887,0.04426484555006027
55588ae77496e7753bff18763a21ca07d9f93240,What are the characteristics of the rural dialect?,,It uses particular forms of a concept rather than all of them uniformly,0.9878008365631104,-0.021870635449886322
564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee,Which languages do they validate on?,,"Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur",0.9967476725578308,0.1535741537809372
5712a0b1e33484ebc6d71c70ae222109c08dede2,What benchmark datasets they use?,,VQA and GeoQA,0.9845202565193176,0.15177124738693237
572458399a45fd392c3a4e07ce26dcff2ad5a07d,How much more accurate is the model than the baseline?,,"For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. ",0.9221318960189819,0.0649459958076477
57388bf2693d71eb966d42fa58ab66d7f595e55f,How is morphology knowledge implemented in the method?,,A BPE model is applied to the stem after morpheme segmentation.,0.8828337788581848,0.038143910467624664
579941de2838502027716bae88e33e79e69997a6,What is difference in peformance between proposed model and state-of-the art on other question types?, does not hinder performance,"For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.
For number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. 
For date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.",0.13221198320388794,0.24736061692237854
58a340c338e41002c8555202ef9adbf51ddbb7a1,Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?,,SST-2 dataset,0.9872943162918091,-0.025608379393815994
58edc6ed7d6966715022179ab63137c782105eaf,Which one of the four proposed models performed best?, lft,the hybrid model MinAvgOut + RL,0.6425978541374207,0.13475769758224487
58f50397a075f128b45c6b824edb7a955ee8cba1,How many shared layers are in the system?,,1,0.27162688970565796,0.4767308831214905
5908d7fb6c48f975c5dfc5b19bb0765581df2b25,How big is the dataset?,,Resulting dataset was 7934 messages for train and 700 messages for test.,0.4119674265384674,0.07370645552873611
593e307d9a9d7361eba49484099c7a8147d3dade,What are causal attribution networks?, separate efforts to map out the underlying or latent causal attribution network held collectively by humans,"networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans",0.7712296843528748,0.6442950963973999
5a0841cc0628e872fe473874694f4ab9411a1d10,By how much did they outperform the other methods?,,"on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI",0.9693323969841003,-0.02936437539756298
5a23f436a7e0c33e4842425cf86d5fd8ba78ac92,How big is dataset used?,,"553,451 documents",0.871188759803772,0.038268230855464935
5a33ec23b4341584a8079db459d89a4e23420494,What is public dashboard?,,"Public dashboard where competitors can see their results during competition, on part of the test set (public test set).",0.9839590787887573,0.17781710624694824
5b2480c6533696271ae6d91f2abe1e3a25c4ae73,Is the assumption that natural language is stationary and ergodic valid?,,It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement.,0.03786046802997589,0.01765899918973446
5b6aec1b88c9832075cd343f59158078a91f3597,How does proposed word embeddings compare to Sindhi fastText word representations?,,"Proposed SG model vs SINDHI FASTTEXT:
Average cosine similarity score: 0.650 vs 0.388
Average semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391",0.9876139760017395,-0.0035120914690196514
5bcc12680cf2eda2dd13ab763c42314a26f2d993,What evaluation metrics were used in the experiment?,,"For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy",0.9293447732925415,0.031024768948554993
5be94c7c54593144ba2ac79729d7545f27c79d37,What is the challenge for other language except English,,not researched as much as English,0.9882737994194031,-0.020838646218180656
5c4c8e91d28935e1655a582568cc9d94149da2b2,Does DCA or GMM-based attention perform better in experiments?,,About the same performance,0.22542741894721985,0.1352376639842987
5c90e1ed208911dbcae7e760a553e912f8c237a5,How big are the datasets?,,"In-house dataset consists of  3716 documents 
ACE05 dataset consists of  1635 documents",0.931675136089325,0.044528450816869736
5c95808cd3ee9585f05ef573b0d4a52e86d04c60,Which journal and conference are cited the most in recent years?,,CL Journal and EMNLP conference,0.9195166230201721,0.12670908868312836
5d9b088bb066750b60debfb0b9439049b5a5c0ce,what processing was done on the speeches before being parsed?," we have removed all numbers, punctuation marks, and stop words",Remove numbers and interjections,0.588870108127594,0.46759092807769775
5dfa59c116e0ceb428efd99bab19731aa3df4bbd,How many natural language explanations are human-written?,,Totally 6980 validation and test image-sentence pairs have been corrected.,0.9816908240318298,0.1266629695892334
5e5460ea955d8bce89526647dd7c4f19b173ab34,How many of the utterances are transcribed?,,Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned),0.8644178509712219,0.07028631120920181
5e65bb0481f3f5826291c7cc3e30436ab4314c61,What discourse features are used?, rst,Entity grid with grammatical relations and RST discourse relations.,0.6657810211181641,0.4544301927089691
5e9732ff8595b31f81740082333b241d0a5f7c9a,How much better were results of the proposed models than base LSTM-RNN model?,,on diversity 6.87 and on relevance 4.6 points higher,0.9881962537765503,0.1338462233543396
5f9bd99a598a4bbeb9d2ac46082bd3302e961a0f,What are the models evaluated on?,,They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA),0.4067796766757965,0.05842084810137749
5fb348b2d7b012123de93e79fd46a7182fd062bd,What datasets are used to evaluate the approach?, nell - one and wiki,"NELL-One, Wiki-One",0.4717330038547516,0.9726645946502686
5fb6a21d10adf4e81482bb5c1ec1787dc9de260d,How do they quantify moral relevance?, fitted slope,By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence,0.239520862698555,0.033763337880373
602396d1f5a3c172e60a10c7022bcfa08fa6cbc9,By how much do they outperform BiLSTMs in Sentiment Analysis?,,Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets.,0.9917631149291992,0.036682263016700745
61fb982b2c67541725d6db76b9c710dd169b533d,Is infinite-length sequence generation a result of training with maximum likelihood?,,There are is a strong conjecture that it might be the reason but it is not proven.,0.9163400530815125,0.027995936572551727
63723c6b398100bba5dc21754451f503cb91c9b8,What is the state of the art?,,"POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)",0.9934213757514954,0.07064908742904663
63850ac98a47ae49f0f49c1c1a6e45c6c447272c,What is the problem with existing metrics that they are trying to address?,,"Answer with content missing: (whole introduction) However, recent
studies observe the limits of ROUGE and find in
some cases, it fails to reach consensus with human.
judgment (Paulus et al., 2017; Schluter, 2017).",0.9844136834144592,-0.005096813663840294
6389d5a152151fb05aae00b53b521c117d7b5e54,What is typical GAN architecture for each text-to-image synhesis group?,,"Semantic Enhancement GANs: DC-GANs, MC-GAN
Resolution Enhancement GANs: StackGANs, AttnGAN, HDGAN
Diversity Enhancement GANs: AC-GAN, TAC-GAN etc.
Motion Enhancement GAGs: T2S, T2V, StoryGAN",0.9203304648399353,0.09428365528583527
63c0128935446e26eacc7418edbd9f50cba74455,What is the size of the released dataset?,,"440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples.",0.9908133149147034,0.09132218360900879
6412e97373e8e9ae3aa20aa17abef8326dc05450,What baseline model is used?,,Human evaluators,0.9077181220054626,0.20726266503334045
6472f9d0a385be81e0970be91795b1b97aa5a9cf,Do they train a different training method except from scheduled sampling?,,"Answer with content missing: (list missing) 
Scheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.

Yes.",0.9780055284500122,0.09287497401237488
657edbf39c500b2446edb9cca18de2912c628b7d,What was their perplexity score?,,Perplexity score 142.84 on dev and 138.91 on test,0.8041366934776306,0.015680944547057152
675f28958c76623b09baa8ee3c040ff0cf277a5a,What is the size of the dataset?,,"300,000 sentences with 1.5 million single-quiz questions",0.9910405874252319,0.11905725300312042
67ee7a53aa57ce0d0bc1a20d41b64cb20303f4b7,How much training data is used?,,"163,110,000 utterances",0.9813321828842163,0.06356222182512283
68794289ed6078b49760dc5fdf88618290e94993,What are proof paths?,"What are proof paths?this continuous relaxation is at the crux of ntp ' inability to scale to large datasets. during both training and inference, ntp need to compute all possible proof trees needed for proving a query, relying on the continuous unification of the query with all the rules and facts in the kb. this procedure quickly becomes infeasible for large datasets, as the number of nodes of the resulting computation graph grows exponentially. our insight is that we can radically reduce the computational complexity of inference and learning by generating only the most promising proof paths. in particular, we show that the problem of finding the facts in the kb that best explain a query can be reduced to a  - nearest neighbour problem, for which efficient exact and approximate solutions exist bibref18. this enables us to apply ntp to previously unreachable real - world datasets, such as wordnet. in this paper, we focus on the ntp model proposed by bibref0. akin to recent neural - symbolic models, ntp rely on a continuous relaxation of a discrete algorithm, operating over the sub - symbolic representations. in this case, the algorithm is an analogue to prolog ' s backward chaining with a relaxed unification operator. the backward chaining algorithm constructs neural networks, which model continuously relaxed proof paths using sub - symbolic representations",A sequence of logical statements represented in a computational graph,0.2855220437049866,0.5551453232765198
68e3f3908687505cb63b538e521756390c321a1c,What is the performance difference of using a generated summary vs. a user-written one?,,2.7 accuracy points,0.04100361093878746,0.12178143113851547
6a9eb407be6a459dc976ffeae17bdd8f71c8791c,What is the reward model for the reinforcement learning appraoch?, return,"reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail",0.37135034799575806,0.30362749099731445
6b91fe29175be8cd8f22abf27fb3460e43b9889a,what genres do they songs fall under?," blues, rap, metal, folk, r & b, reggae, country, and religious","Gospel, Sertanejo, MPB, Forró, Pagode, Rock, Samba, Pop, Axé, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda",0.6753943562507629,0.5708436965942383
6baf5d7739758bdd79326ce8f50731c785029802,Which four languages do they experiment with?,,"German, English, Italian, Chinese",0.9965382814407349,0.06053651124238968
6ce057d3b88addf97a30cb188795806239491154,What models are included in baseline benchmarking results?,,"BERT, XLNET RoBERTa, ALBERT, DistilBERT",0.9914864301681519,0.14777721464633942
6dcbe941a3b0d5193f950acbdc574f1cfb007845,What are the domains covered in the dataset?,,"Alarm
Bank
Bus
Calendar
Event
Flight
Home
Hotel
Media
Movie
Music
RentalCar
Restaurant
RideShare
Service
Travel
Weather",0.956666886806488,0.08866327255964279
6e97c06f998f09256be752fa75c24ba853b0db24,How do the authors measure performance?,,Accuracy across six datasets,0.9445410966873169,0.025626713410019875
6f2118a0c64d5d2f49eee004d35b956cb330a10e,What datasets are used for training/testing models? ,,"Microsoft Research dataset containing movie, taxi and restaurant domains.",0.9790873527526855,0.02647404745221138
6f2f304ef292d8bcd521936f93afeec917cbe28a,How much improvement is gained from the proposed approaches?,,It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.,0.9639664888381958,0.026285899803042412
707db46938d16647bf4b6407b2da84b5c7ab4a81,How much F1 was improved after adding skip connections?,,"Simple Skip improves F1 from 74.34 to 74.81
Transformer Skip improes F1 from 74.34 to 74.95 ",0.994168758392334,0.0890725702047348
7182f6ed12fa990835317c57ad1ff486282594ee,How does the SCAN dataset evaluate compositional generalization?,,"it systematically holds out inputs in the training set containing basic primitive verb, ""jump"", and tests on sequences containing that verb.",0.9312519431114197,0.07839086651802063
71d59c36225b5ee80af11d3568bdad7425f17b0c,How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?,,Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ,0.992185115814209,0.12442846596240997
728a55c0f628f2133306b6bd88af00eb54017b12,What geometric properties do embeddings display?,,Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.,0.9922520518302917,0.01202599797397852
7348e781b2c3755b33df33f4f0cab4b94fcbeb9b,"Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",,Only automatic methods,0.9766838550567627,0.07815616577863693
7358a1ce2eae380af423d4feeaa67d2bd23ae9dd,How do their train their embeddings?,"How do their train their embeddings?for each variable in encoding set, learn the new embeddings using the embeddings train set. this should be done simultaneously","The embeddings are learned several times using the training set, then the average is taken.",0.09318039566278458,0.6452684998512268
73633afbefa191b36cca594977204c6511f9dad4,"Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?",,"Not at the moment, but summaries can be additionaly extended with this annotations.",0.7912213802337646,-0.004252973478287458
737397f66751624bcf4ef891a10b29cfc46b0520,Which datasets are used in the paper?,,"Google N-grams
COHA
Moral Foundations Dictionary (MFD)
",0.9242644309997559,0.04111900553107262
73bb8b7d7e98ccb88bb19ecd2215d91dd212f50d,What human evaluation method is proposed?,,comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant,0.9454165697097778,0.01783180609345436
74091e10f596428135b0ab06008608e09c051565,How is knowledge stored in the memory?, curated knowledge bases,entity memory and relational memory.,0.7623905539512634,0.3383750021457672
74261f410882551491657d76db1f0f2798ac680f,What are the six target languages?,,"Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI).",0.5032030940055847,0.0354846827685833
74cd51a5528c6c8e0b634f3ad7a9ce366dfa5706,What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?, free of human effort,it is less expensive and quantifies interpretability using continuous values rather than binary evaluations,0.37196221947669983,0.1414591372013092
74db8301d42c7e7936eb09b2171cd857744c52eb,How is the performance on the task evaluated?,,Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors,0.9855840802192688,0.033587805926799774
753990d0b621d390ed58f20c4d9e4f065f0dc672,What is the seed lexicon?,,a vocabulary of positive and negative predicates that helps determine the polarity score of an event,0.7828058004379272,0.0941796600818634
75b69eef4a38ec16df63d60be9708a3c44a79c56,How much better peformance is achieved in human evaluation when model is trained considering proposed metric?,"How much better peformance is achieved in human evaluation when model is trained considering proposed metric?the experimental results are summarized in table 1. we can see that the proposed comparative evaluator correlates far better with human judgment than bleu and perplexity. when compared with recently proposed parameterized metrics including adversarial evaluator and adem, our model consistently outperforms them by a large margin","Pearson correlation to human judgement - proposed vs next best metric
Sample level comparison:
- Story generation: 0.387 vs 0.148
- Dialogue: 0.472 vs 0.341
Model level comparison:
- Story generation:  0.631 vs 0.302
- Dialogue: 0.783 vs 0.553",0.27860307693481445,0.36745890974998474
76377e5bb7d0a374b0aefc54697ac9cd89d2eba8,How do they obtain word lattices from words?, via an existing lookup vocabulary,By considering words as vertices and generating directed edges between neighboring words within a sentence,0.292699933052063,0.35728535056114197
78577fd1c09c0766f6e7d625196adcc72ddc8438,What dataset is used for train/test of this method?, tts system dataset,Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset.,0.24044986069202423,0.6350498199462891
785eb3c7c5a5c27db14006ac357299ed1216313a,What they formulate the question generation as?,,LASSO optimization problem,0.9972986578941345,0.13385173678398132
78c010db6413202b4063dc3fb6e3cc59ec16e7e3,What is different in the improved annotation protocol?,,a trained worker consolidates existing annotations ,0.9939141273498535,0.08323082327842712
7920f228de6ef4c685f478bac4c7776443f19f39,What language is the Twitter content in?,,English,0.9774746298789978,0.28494971990585327
7994b4001925798dfb381f9aa5c0545cdbd77220,How do they perform data augmentation?,,They randomly sample sentences from Wikipedia that contains an object RC and add them to training data,0.9740238785743713,0.04913242161273956
7997b9971f864a504014110a708f215c84815941,"What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?", unavailability of suitable datasets and lexicons,"Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text",0.4001762866973877,0.37551936507225037
79a28839fee776d2fed01e4ac39f6fedd6c6a143,What is the main contribution of the paper? , organization,"Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance",0.3876056671142578,-0.007133087143301964
79bb1a1b71a1149e33e8b51ffdb83124c18f3e9c,What accuracy does CNN model achieve?, faster convergence and stabler training,Combined per-pixel accuracy for character line segments is 74.79,0.6764910817146301,0.1403377652168274
79ed71a3505cf6f5e8bf121fd7ec1518cab55cae,How do they damage different neural modules?,,"Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.",0.9911367893218994,0.0263371579349041
7a53668cf2da4557735aec0ecf5f29868584ebcf,What kind of instructional videos are in the dataset?,,tutorial videos for a photo-editing software,0.9259852766990662,0.12505000829696655
7a7e279170e7a2f3bc953c37ee393de8ea7bd82f,What two types the Chinese reading comprehension dataset consists of?, large - scale automatically generated training set and human - annotated validation and test set,cloze-style reading comprehension and user query reading comprehension questions,0.7681635618209839,0.12928614020347595
7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed,Is ROUGE their only baseline?,,"No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.",0.8866588473320007,-0.06059163436293602
7af01e2580c332e2b5e8094908df4e43a29c8792,How was lexical diversity measured?,,By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions,0.9984545707702637,0.054241519421339035
7b44bee49b7cb39cb7d5eec79af5773178c27d4d,How is the data in RAFAEL labelled?,,"Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner",0.9140965342521667,0.042956043034791946
7b89515d731d04dd5cbfe9c2ace2eb905c119cbc,Which is the baseline model?,,"The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. ",0.5312967300415039,0.03389933705329895
7cf726db952c12b1534cd6c29d8e7dfa78215f9e,What is a word confusion network?,,It is a network used to encode speech lattices to maintain a rich hypothesis space.,0.6983823180198669,0.06898652017116547
7d3c036ec514d9c09c612a214498fc99bf163752,What is the source of the dataset?,,"Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera",0.5791793465614319,0.08818528056144714
7d59374d9301a0c09ea5d023a22ceb6ce07fb490,How do they measure the diversity of inferences?, accuracy,by number of distinct n-grams,0.35803675651550293,0.1640987992286682
7f9bc06cfa81a4e3f7df4c69a1afef146ed5a1cf,What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?,,"10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%
50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%",0.9451175928115845,0.013212394900619984
8051927f914d730dfc61b2dc7a8580707b462e56,What baseline algorithms were presented?,,"a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm",0.9922044277191162,0.12804825603961945
81064bbd0a0d72a82d8677c32fb71b06501830a0,By how much is precission increased?,,"ROUGE-1 increases by 0.05, ROUGE-2 by 0.06 and ROUGE-L by 0.09",0.9980358481407166,0.017177004367113113
81cee2fc6edd9b7bc65bbf6b4aa35782339e6cff,What type of documents are supported by the annotation platform?,,"Variety of formats supported (PDF, Word...), user can define content elements of document",0.5579378604888916,-0.03565441444516182
81e8d42dad08a58fe27eea838f060ec8f314465e,What is the state-of-the art?,,neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder,0.9886019229888916,0.08871811628341675
8255f74cae1352e5acb2144fb857758dda69be02,How do they measure grammaticality?,How do they measure grammaticality?the grammaticality of the corpora following cda,by calculating log ratio of grammatical phrase over ungrammatical phrase,0.21522827446460724,0.4701544940471649
82a28c1ed7988513d5984f6dcacecb7e90f64792,How big are negative effects of proposed techniques on high-resource tasks?,,The negative effects were insignificant.,0.9979869723320007,0.07858891785144806
83f14af3ccca4ab9deb4c6d208f624d1e79dc7eb,Which of the two ensembles yields the best performance?,,Answer with content missing: (Table 2) CONCAT ensemble,0.2481546849012375,0.02483239211142063
8427988488b5ecdbe4b57b3813b3f981b07f53a5,On which task does do model do best?, author profiling,Variety prediction task,0.3389640152454376,0.07885672897100449
8434974090491a3c00eed4f22a878f0b70970713,How big is their model?,,Proposed model has 1.16 million parameters and 11.04 MB.,0.1566191464662552,0.07550755888223648
8568c82078495ab421ecbae38ddd692c867eac09,How many layers of self-attention does the model have?,,"1, 4, 8, 16, 32, 64",0.9076968431472778,0.11470283567905426
85e45b37408bb353c6068ba62c18e516d4f67fe9,What is the baseline?, multi - task architecture inspired by yang2016multi. in our baseline model there are no explicit connections between tasks,The baseline is a multi-task architecture inspired by another paper.,0.1767091602087021,0.6305789947509766
8602160e98e4b2c9c702440da395df5261f55b1f,What are the three datasets used in the paper?,,Data released for APDA shared task contains 3 datasets.,0.887032151222229,-0.010396886616945267
863d5c6305e5bb4b14882b85b6216fa11bcbf053,What are the 12 AV approaches which are examined?,,"MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD",0.8801457285881042,0.14795459806919098
86cd1228374721db67c0653f2052b1ada6009641,What domain does the dataset fall into?,,YouTube videos,0.9960659742355347,0.16601243615150452
880a76678e92970791f7c1aad301b5adfc41704f,What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?,,"ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.
NLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing ""retweets"", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation.",0.9854453802108765,0.05275707691907883
894c086a2cbfe64aa094c1edabbb1932a3d7c38a,What are the state-of-the-art systems?, semeval 2016 task 6,"For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN",0.25688108801841736,0.12271073460578918
8951fde01b1643fcb4b91e51f84e074ce3b69743,How they evaluate their approach?,,"They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise",0.9939951300621033,0.01682034134864807
8958465d1eaf81c8b781ba4d764a4f5329f026aa,What are the three measures of bias which are reduced in experiments?,,"RIPA, Neighborhood Metric, WEAT",0.9963377118110657,0.10174393653869629
8985ead714236458a7496075bc15054df0e3234e,What is the performance of the models on the tasks?,,"Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)",0.9132691621780396,-0.002454980043694377
89d1687270654979c53d0d0e6a845cdc89414c67,How do they obtain human judgements?,"How do they obtain human judgements?evaluating clustering assignments while the results from the above section allow us to compare our three computational methods against each other, we additionally collect human judgments to further ground our results",Using crowdsourcing ,0.26540812849998474,0.31760284304618835
8a0a51382d186e8d92bf7e78277a1d48958758da,How better is gCAS approach compared to other approaches?, outperforms all other methods,"For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52",0.24794535338878632,0.2846222519874573
8a1d4ed00d31c1f1cb05bc9d5e4f05fe87b0e5a4,Who manually annotated the semantic roles for the set of learner texts?,,Authors,0.978285551071167,0.23107966780662537
8a5254ca726a2914214a4c0b6b42811a007ecfc6,How much transcribed data is available for for Ainu language?,,Transcribed data is available for duration of 38h 54m 38s for 8 speakers.,0.9933545589447021,0.05640542879700661
8a871b136ccef78391922377f89491c923a77730,What are the baseline state of the art models?,,"Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention",0.9961399435997009,0.06144122779369354
8ad815b29cc32c1861b77de938c7269c9259a064,What languages are represented in the dataset?, 28 languages,"EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO",0.46522536873817444,0.3817541003227234
8c0a0747a970f6ea607ff9b18cfeb738502d9a95,What was the performance of both approaches on their dataset?,,ERR of 19.05 with i-vectors and 15.52 with x-vectors,0.20178745687007904,0.10036324709653854
8c8a32592184c88f61fac1eef12c7d233dbec9dc,Are this models usually semi/supervised or unsupervised?,unsupervised,"Both supervised and unsupervised, depending on the task that needs to be solved.",0.6844308376312256,0.5047515034675598
8d793bda51a53a4605c1c33e7fd20ba35581a518,what bottlenecks were identified?,,Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System.,0.9713607430458069,0.09155067056417465
8e2b125426d1220691cceaeaf1875f76a6049cbd,By how much do they improve the accuracy of inferences over state-of-the-art methods?,,"ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.
On Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.",0.9844319224357605,-0.00017104559810832143
8e9de181fa7d96df9686d0eb2a5c43841e6400fa,"Is CRWIZ already used for data collection, what are the results?",,"Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.",0.4840438663959503,0.06175185739994049
8ea4bd4c1d8a466da386d16e4844ea932c44a412,What dataset do they use?,,A parallel corpus where the source is an English expression of code and the target is Python code.,0.9614851474761963,0.08914416283369064
8f87215f4709ee1eb9ddcc7900c6c054c970160b,how is quality measured?,,Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.,0.2728137969970703,0.024102605879306793
90159e143487505ddc026f879ecd864b7f4f479e,How much of the ASR grapheme set is shared between languages?,,Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script.,0.9935652613639832,-0.03976152464747429
90bc60320584ebba11af980ed92a309f0c1b5507,How do they enrich the positional embedding with length information,,They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).,0.9941248297691345,0.015667473897337914
9299fe72f19c1974564ea60278e03a423eb335dc,What was the weakness in Hassan et al's evaluation design?,,"MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set
",0.9967421889305115,0.031120356172323227
9349acbfce95cb5d6b4d09ac626b55a9cb90e55e,What are the citation intent labels in the datasets?,,"Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.",0.840516984462738,0.04339970275759697
93b299acfb6fad104b9ebf4d0585d42de4047051,Which datasets are used?,,"ABSA SemEval 2014-2016 datasets
Yelp Academic Dataset
Wikipedia dumps",0.9068058729171753,0.05659481883049011
9447ec36e397853c04dcb8f67492ca9f944dbd4b,What is the dataset used as input to the Word2Vec algorithm?,,Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words,0.989139199256897,0.09777893126010895
944d5dbe0cfc64bf41ea36c11b1d378c408d40b8,Which of the two speech recognition models works better overall on CN-Celeb?,,x-vector,0.23345567286014557,0.20472660660743713
94bee0c58976b58b4fef9e0adf6856fe917232e5,How much bigger is Switchboard-2000 than Switchboard-300 database?,,Switchboard-2000 contains 1700 more hours of speech data.,0.9950190782546997,-0.01467885822057724
94e0cf44345800ef46a8c7d52902f074a1139e1a,What web and user-generated NER datasets are used for the analysis?,,"MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC",0.9911201000213623,0.013064317405223846
954c4756e293fd5c26dc50dc74f505cc94b3f8cc,What are dilated convolutions?, skip some input values,Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.,0.17481321096420288,0.34892570972442627
9555aa8de322396a16a07a5423e6a79dcd76816a,By how much does their model outperform both the state-of-the-art systems?,,w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%,0.9642520546913147,0.04272528737783432
957bda6b421ef7d2839c3cec083404ac77721f14,What stylistic features are used to detect drunk texts?,,"LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) 
 and Sentiment Ratio",0.944950520992279,0.04630810767412186
96be67b1729c3a91ddf0ec7d6a80f2aa75e30a30,What language do the agents talk in?,,English,0.6336812973022461,0.28494971990585327
96c09ece36a992762860cde4c110f1653c110d96,What was the result of the highest performing system?,,"For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.
For task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2",0.9935903549194336,-0.013244634494185448
973f6284664675654cc9881745880a0e88f3280e,What proficiency indicators are used to the score the utterances?,,"6 indicators:
- lexical richness
- pronunciation and fluency
- syntactical correctness
- fulfillment of delivery
- coherence and cohesion
- communicative, descriptive, narrative skills",0.993635356426239,0.06581202149391174
98515bd97e4fae6bfce2d164659cd75e87a9fc89,What is the source of the user interaction data? ,,Sociability from ego-network on Twitter,0.9944384098052979,0.11773386597633362
98785bf06e60fcf0a6fe8921edab6190d0c2cec1,How do they determine which words are informative?,,Informative are those that will not be suppressed by regularization performed.,0.9924005270004272,0.11102759093046188
98eb245c727c0bd050d7686d133fa7cd9d25a0fb,Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?,,BLEU scores,0.900604248046875,0.1464756280183792
993b896771c31f3478f28112a7335e7be9d03f21,What novel class of recurrent-like networks is proposed?,,"A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state.",0.9294080138206482,0.04579890891909599
999b20dc14cb3d389d9e3ba5466bc3869d2d6190,What is the latest paper covered by this survey?,,Kim et al. (2019),0.9518312811851501,0.1912979930639267
9a596bd3a1b504601d49c2bec92d1592d7635042,What is the performance of their model?,,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220.,0.9443663954734802,0.010929813608527184
9a65cfff4d99e4f9546c72dece2520cae6231810,What is the performance of proposed model on entire DROP dataset?,,"The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev",0.9877879023551941,0.06518682837486267
9aa52b898d029af615b95b18b79078e9bed3d766,How faster is training and decoding compared to former models?,,"Proposed vs best baseline:
Decoding: 8541 vs 8532 tokens/sec
Training: 8h vs 8h",0.9855701923370361,0.06653124839067459
9ab43f941c11a4b09a0e4aea61b4a5b4612e7933,What approach did previous models use for multi-span questions?,"What approach did previous models use for multi-span questions?first, we show that integrating this new approach into an existing model, nabert +, does not hinder performance on other questions types, while substantially improving the results on multi - span questions. later, we compare our results to the current state - of - the - art on multi - span questions. we show that our model has a clear advantage in handling multi - span questions, with a 29. 7 absolute improvement in em, and a 15. 1 absolute improvement in f1. furthermore, we show that our model slightly eclipses the current state - of - the - art results on the entire drop dataeset. finally, we present some ablation studies, analyzing the benefit gained from individual components of our model. we believe that combining our tag - based",Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span,0.24185606837272644,0.5265895128250122
9adcc8c4a10fa0d58f235b740d8d495ee622d596,How many additional task-specific layers are introduced?,,2 for the ADE dataset and 3 for the CoNLL04 dataset,0.9903719425201416,0.03536757454276085
9ae084e76095194135cd602b2cdb5fb53f2935c1,What metrics are used for evaluation?,,word error rate,0.6653135418891907,0.03346630930900574
9bcc1df7ad103c7a21d69761c452ad3cd2951bda,On which task does do model do worst?,,Gender prediction task,0.9962903261184692,0.12649287283420563
9c4a4dfa7b0b977173e76e2d2f08fa984af86f0e,How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?,,"Full Testing Set accuracy: 84.02
Cleaned Testing Set accuracy: 93.48",0.9887159466743469,0.0890677347779274
9c68d6d5451395199ca08757157fbfea27f00f69,Which OpenIE systems were used?, ollie bibref10 and reverb bibref10,OpenIE4 and MiniIE,0.6777315735816956,0.23880378901958466
9d578ddccc27dd849244d632dd0f6bf27348ad81,What are the results?, table tabref16,"Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. 
Using a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.",0.14984135329723358,0.10424776375293732
9d5df9022cc9eb04b9f5c5a9d8308a332ebdf50c,What is the new labeling strategy?, to partition texts into those with pure and mixed sentiment orientations,They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations,0.5924980640411377,0.5861415863037109
9d9b11f86a96c6d3dd862453bf240d6e018e75af,How does counterfactual data augmentation aim to tackle bias?, augmenting our dataset with a copy of every dialogue with gendered words swapped using the gendered word pair list provided by bibref21,The training dataset is augmented by swapping all gendered words by their other gender counterparts,0.22439682483673096,0.7512650489807129
9e04730907ad728d62049f49ac828acb4e0a1a2a,What were their performance results?,,"On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%",0.8830313682556152,-0.02746102772653103
9ebb2adf92a0f8db99efddcade02a20a219ca7d9,How is the proficiency score calculated?,,"They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.",0.19497524201869965,0.005753403063863516
9ef182b61461d0d8b6feb1d6174796ccde290a15,Do they annotate their own dataset or use an existing one?,,Use an existing one,0.9899137020111084,0.07639410346746445
9fe4a2a5b9e5cf29310ab428922cc8e7b2fc1d11,What are state of the art methods MMM is compared to?,,"FTLM++, BERT-large, XLNet",0.9817977547645569,0.08446671813726425
a02696d4ab728ddd591f84a352df9375faf7d1b4,How large is the Dialog State Tracking Dataset?,,"1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs",0.9976306557655334,0.13093824684619904
a09633584df1e4b9577876f35e38b37fdd83fa63,"How is human evaluation performed, what was the criteria?",,Through Amazon MTurk annotators to determine plausibility and content richness of the response,0.10098064690828323,0.06797632575035095
a1064307a19cd7add32163a70b6623278a557946,How many uniue words are in the dataset?,,908456 unique words are available in collected corpus.,0.9919109344482422,0.07313451170921326
a24a7a460fd5e60d71a7e787401c68caa4702df6,What monolingual word representations are used?,,"AraVec for Arabic, FastText for French, and Word2vec Google News for English.",0.9640904664993286,0.09567543864250183
a2be2bd84e5ae85de2ab9968147b3d49c84dfb7f,What genres are covered?," entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement","genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement",0.7243529558181763,0.9713292717933655
a379c380ac9f67f824506951444c873713405eed,What are the baselines?,,"CNN, LSTM, BERT",0.9957771301269531,0.12708328664302826
a381ba83a08148ce0324b48b8ff35128e66f580a,what models did they compare to?,,"High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM ",0.6465200781822205,0.1164451465010643
a3d83c2a1b98060d609e7ff63e00112d36ce2607,How many sentence transformations on average are available per unique sentence in dataset?,,27.41 transformation on average of single seed sentence is available in dataset.,0.5981154441833496,0.02774461731314659
a3f108f60143d13fe38d911b1cc3b17bdffde3bd,what was their system's f1 performance?,,"Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.",0.885920524597168,0.07229423522949219
a48c6d968707bd79469527493a72bfb4ef217007,Which training dataset allowed for the best generalization to benchmark sets?,,MultiNLI,0.9719710946083069,0.2701605558395386
a4d8fdcaa8adf99bdd1d7224f1a85c610659a9d3,"When they say ""comparable performance"", how much of a performance drop do these new embeddings result in?",,"Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.",0.9833601117134094,0.007227020338177681
a4fe5d182ddee24e5bbf222d6d6996b3925060c8,which datasets were used in evaluation?,,"CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0",0.5667269825935364,0.07119493931531906
a4ff1b91643e0c8a0d4cc1502d25ca85995cf428,Which two datasets does the resource come from?,,two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor,0.7682543396949768,0.008970978669822216
a516b37ad9d977cb9d4da3897f942c1c494405fe,Which models do they try out?,,"DocQA, SAN, QANet, ASReader, LM, Random Guess",0.6251225471496582,0.11129280924797058
a56fbe90d5d349336f94ef034ba0d46450525d19,What DCGs are used?, definite clause grammars,Author's own DCG rules are defined from scratch.,0.4669724106788635,0.16285212337970734
a5b67470a1c4779877f0d8b7724879bbb0a3b313,what metrics are used in evaluation?, micro - averaged inlineform0,micro-averaged F1,0.8241294622421265,0.6227045059204102
a71ebd8dc907d470f6bd3829fa949b15b29a0631,how did they ask if a tweet was racist?,,"if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.",0.9962013363838196,0.05228225141763687
a7adb63db5066d39fdf2882d8a7ffefbb6b622f0,what was the baseline?,,There is no baseline.,0.9957931637763977,0.08728743344545364
a81941f933907e4eb848f8aa896c78c1157bff20,"Can the model add new relations to the knowledge graph, or just new entities?",,The model does not add new relations to the knowledge graph.,0.958561897277832,0.015034081414341927
a891039441e008f1fd0a227dbed003f76c140737,What MC abbreviate for?,,machine comprehension,0.9699269533157349,0.18535210192203522
a978a1ee73547ff3a80c66e6db3e6c3d3b6512f4,How much improvement does their method get over the fine tuning baseline?,,"0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.",0.6158613562583923,0.008337960578501225
aa54e12ff71c25b7cff1e44783d07806e89f8e54,What is an example of a health-related tweet?,,"The health benefits of alcohol consumption are more limited than previously thought, researchers say",0.992364227771759,0.003463117638602853
aacb0b97aed6fc6a8b471b8c2e5c4ddb60988bf5,How many attention layers are there in their model?,,one,0.9851179718971252,0.44960346817970276
aaed6e30cf16727df0075b364873df2a4ec7605b,What is WNGT 2019 shared task?,,efficiency task aimed  at reducing the number of parameters while minimizing drop in performance,0.9380594491958618,0.0491938441991806
ac148fb921cce9c8e7b559bba36e54b63ef86350,What dataset they use for evaluation?,,The same 2K set from Gigaword used in BIBREF7,0.6752595901489258,0.04990188032388687
acc8d9918d19c212ec256181e51292f2957b37d7,What are the differences with previous applications of neural networks for this task?,,This approach considers related images,0.6303393244743347,0.15039215981960297
ace60950ccd6076bf13e12ee2717e50bc038a175,How are the two different models trained?, the two differ in the number of layers and hidden sizes in the underlying model,They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.,0.17332716286182404,0.25684311985969543
ad0a7fe75db5553652cd25555c6980f497e08113,How does the model compute the likelihood of executing to the correction semantic denotation?,,By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x.,0.9457082152366638,0.07479163259267807
ad1f230f10235413d1fe501e414358245b415476,Which models were compared?,,"BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT",0.9477829933166504,0.07703197747468948
ad5898fa0063c8a943452f79df2f55a5531035c7,Which embeddings do they detect biases in?,,Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset,0.8985651731491089,0.06854462623596191
ad67ca844c63bf8ac9fdd0fa5f58c5a438f16211,Which unlabeled data do they pretrain with?,,1000 hours of WSJ audio data,0.9796606302261353,-0.00032519595697522163
aeda22ae760de7f5c0212dad048e4984cd613162,What annotations are available in the dataset?,,"For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)",0.8436306118965149,0.013636225834488869
af75ad21dda25ec72311c2be4589efed9df2f482,How much does this system outperform prior work?,,"The system outperforms by 27.7% the LSTM model, 38.5% the RL-SPINN model and 41.6% the Gumbel Tree-LSTM",0.9957002997398376,0.029267843812704086
b13d0e463d5eb6028cdaa0c36ac7de3b76b5e933,What datasets are used to evaluate the model?,,WN18 and FB15k,0.998927891254425,0.07398019731044769
b2254f9dd0e416ee37b577cef75ffa36cbcb8293,How many domains of ontologies do they gather data from?,,"5 domains: software, stuff, african wildlife, healthcare, datatypes",0.8659050464630127,0.05370398238301277
b27f7993b1fe7804c5660d1a33655e424cea8d10,What is the source of the visual data? ,,Profile pictures from the Twitter users' profiles.,0.8187096118927002,0.14136986434459686
b3857a590fd667ecc282f66d771e5b2773ce9632,What is a string kernel?, bibref38,String kernel is a technique that uses character n-grams to measure the similarity of strings,0.6485296487808228,0.15148457884788513
b39f2249a1489a2cef74155496511cc5d1b2a73d,What is the accuracy reported by state-of-the-art methods?,,"Answer with content missing: (Table 1)
Previous state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)",0.9033333659172058,0.0778755247592926
b3a09d2e3156c51bd5fdc110a2a00a67bb8c0e42,what are the off-the-shelf systems discussed in the paper?,,"Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier.",0.9605777263641357,0.0778656154870987
b3fcab006a9e51a0178a1f64d1d084a895bd8d5c,what are the state of the art methods?,,"S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al.",0.9809737801551819,0.0761602371931076
b43fa27270eeba3e80ff2a03754628b5459875d6,What domains are present in the data?, 20 domains,"Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather",0.4367973804473877,0.17267128825187683
b540cd4fe9dc4394f64d5b76b0eaa4d9e30fb728,Could you tell me more about the metrics used for performance evaluation?,,"BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy",0.991938591003418,-0.03567078337073326
b5a2b03cfc5a64ad4542773d38372fffc6d3eac7,How are EAC evaluated?, qualitative and quantitative assessment,"Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement.",0.5355550050735474,0.5767410397529602
b5bfa6effdeae8ee864d7d11bc5f3e1766171c2d,Which regions of the United States do they consider?,,all regions except those that are colored black,0.9733121395111084,0.09578774124383926
b5e883b15e63029eb07d6ff42df703a64613a18a,How were topics of interest about DDEO identified?,,using topic modeling model Latent Dirichlet Allocation (LDA),0.9769085645675659,0.02238905429840088
b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1,What were the non-neural baselines used for the task?,,The Lemming model in BIBREF17,0.9979545474052429,0.12230715155601501
b6f5860fc4a9a763ddc5edaf6d8df0eb52125c9e,Which 5 languages appear most frequently in AA paper titles?,,"English, Chinese, French, Japanese and Arabic",0.9842246174812317,-0.0042778137139976025
b6fb72437e3779b0e523b9710e36b966c23a2a40,How many rules had to be defined?,,"WikiSQL - 2 rules (SELECT, WHERE)
SimpleQuestions - 1 rule
SequentialQA - 3 rules (SELECT, WHERE, COPY)",0.9899874925613403,-0.011849713511765003
b7708cbb50085eb41e306bd2248f1515a5ebada8,How do they get the formal languages?,,These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.,0.8817160725593567,-0.0019958128686994314
b8dea4a98b4da4ef1b9c98a211210e31d6630cf3,What is specific to gCAS cell?,,"It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner.",0.8593149781227112,0.0978156253695488
b8f711179a468fec9a0d8a961fb0f51894af4b31,What kind of neural network architecture do they use?, two - stream,CNN,0.7666705846786499,0.2902192175388336
b9025c39838ccc2a79c545bec4a676f7cc4600eb,Why do they think this task is hard?  What is the baseline performance?,,"1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.
2. Macro F1 = 14.6 (MLR, length 96 snippet)
Weighted F1 = 31.1 (LSTM, length 128 snippet)",0.4209561049938202,0.057865988463163376
b9f2a30f5ef664ff845d860cf4bfc2afb0a46e5a,How do they gather human judgements for similarity between relations?,,By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4,0.9649749994277954,-0.017074482515454292
ba1da61db264599963e340010b777a1723ffeb4c,What does recurrent deep stacking network do?, concatenates the outputs of previous frames into the input features of the current frame,Stacks and joins outputs of previous frames with inputs of the current frame,0.8482704758644104,0.7150930762290955
ba56afe426906c4cfc414bca4c66ceb4a0a68121,What are the datasets used for the task?,,"Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)",0.6332526803016663,-0.019833561033010483
bab8c69e183bae6e30fc362009db9b46e720225e,What are two strong baseline methods authors refer to?,,Marcheggiani and Titov (2017) and Cai et al. (2018),0.9893796443939209,0.18249036371707916
bb4de896c0fa4bf3c8c43137255a4895f52abeef,What is the baseline model?,,a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model,0.6164871454238892,0.022835003212094307
bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2,How do the various social phenomena examined manifest in different types of communities?,,"Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
",0.9721806645393372,0.16857951879501343
bbdb2942dc6de3d384e3a1b705af996a5341031b,What type of model are the ELMo representations used in?, deep learning,A bi-LSTM with max-pooling on top of it,0.3387858271598816,0.37008166313171387
bc9c31b3ce8126d1d148b1025c66f270581fde10,What datasets are used to evaluate this approach?,," Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ",0.9930570721626282,0.06310126185417175
bd1a3c651ca2b27f283d3f36df507ed4eb24c2b0,Is it a neural model? How is it trained?," deep neural networks, similar to those used for image captioning, are capable of producing these types of questions after extensive training","No, it is a probabilistic model trained by finding feature weights through gradient ascent",0.15427231788635254,0.2747032344341278
bd5379047c2cf090bea838c67b6ed44773bcd56f,Which experiments are perfomed?,,They used BERT-based models to detect subjective language in the WNC corpus,0.9899712204933167,0.031267281621694565
bd99aba3309da96e96eab3e0f4c4c8c70b51980a,Which existing models does this approach outperform?,,"RNN-context, SRB, CopyNet, RNN-distract, DRGD",0.9614422917366028,0.050900690257549286
bdc93ac1b8643617c966e91d09c01766f7503872,What is the size of the second dataset?,,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,0.9951727986335754,0.17945662140846252
bdd8368debcb1bdad14c454aaf96695ac5186b09,How is the intensity of the PTSD established?,,"Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.",0.9680302739143372,0.04160957410931587
bde6fa2057fa21b38a91eeb2bb6a3ae7fb3a2c62,what state of the accuracy did they obtain?, lower accuracy,51.5,0.6925802826881409,0.18968898057937622
bfce2afe7a4b71f9127d4f9ef479a0bfb16eaf76,What human evaluation metrics were used in the paper?,,rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context,0.9877259135246277,0.014198090881109238
c000a43aff3cb0ad1cee5379f9388531b5521e9a,how are the bidirectional lms obtained?, news articles,"They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs.",0.5309492945671082,0.08861894905567169
c0035fb1c2b3de15146a7ce186ccd2e366fb4da2,By how much of MGNC-CNN out perform the baselines?,,"In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. 
In case of Irony the difference is about 2.0. 
",0.9943411946296692,0.034581273794174194
c029deb7f99756d2669abad0a349d917428e9c12,How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?,,3%,0.32195383310317993,0.2785548269748688
c034f38a570d40360c3551a6469486044585c63c,How better is proposed method than baselines perpexity wise?,,Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.,0.9310049414634705,0.02592441439628601
c13fe4064df0cfebd0538f29cb13e917fc5c3be0,What is the network architecture?,,"The network architecture has a multi-task Bi-Directional Recurrent Neural Network, with an unsupervised sequence labeling task and a low-dimensional embedding layer between tasks. There is a hidden layer after each successive task with skip connections to the senior supervised layers.",0.37389349937438965,0.053495749831199646
c165ea43256d7ee1b1fb6f5c0c8af5f7b585e60d,How much is proposed model better than baselines in performed experiments?,,"most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)
GDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)
GDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)",0.9914230108261108,-0.0031645921990275383
c1c611409b5659a1fd4a870b6cc41f042e2e9889,What evaluations did the authors use on their system?,,"BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.",0.9554197788238525,0.046483736485242844
c1f4d632da78714308dc502fe4e7b16ea6f76f81,Which language-pair had the better performance?,,French-English,0.4611281156539917,0.1845773309469223
c348a8c06e20d5dee07443e962b763073f490079,What two components are included in their proposed framework?,,evidence extraction and answer synthesis,0.5816468596458435,0.07279247045516968
c359ab8ebef6f60c5a38f5244e8c18d85e92761d,How many paraphrases are generated per question?,,"10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans",0.820684015750885,0.06100820377469063
c45feda62f23245f53e855706e2d8ea733b7fd03,Which translation system do they use to translate to English?, pre - trained translation model,Attention-based translation model with convolution sequence to sequence model,0.6922069787979126,0.7182149291038513
c47e87efab11f661993a14cf2d7506be641375e4,How does new evaluation metric considers critical informative entities?,,Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities,0.8245450854301453,0.0069873277097940445
c4b5cc2988a2b91534394a3a0665b0c769b598bb,How do they define local variance?, the reciprocal of its variance,The reciprocal of the variance of the attention distribution,0.18731631338596344,0.5235741138458252
c4c9c7900a0480743acc7599efb359bc81cf3a4d,How much is pre-training loss increased in Low/Medium/Hard level of pruning?,,"The increase is linearly from lowest on average 2.0 , medium around 3.5, and the largest is 6.0",0.21333667635917664,-0.06389167904853821
c515269b37cc186f6f82ab9ada5d9ca176335ded,What evidence do they present that the model attends to shallow context clues?,,Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues,0.9937184453010559,0.10110259056091309
c554a453b6b99d8b59e4ef1511b1b506ff6e5aa4,how was the dataset built?,,"Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable"" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is “yes"" or “no""",0.8945901393890381,-0.015341556631028652
c69f4df4943a2ca4c10933683a02b179a5e76f64,What approach performs better in experiments global latent or sequence of fine-grained latent variables?, sequential latent variables,"PPL: SVT
Diversity: GVT
Embeddings Similarity: SVT
Human Evaluation: SVT",0.17133817076683044,0.11149947345256805
c77d6061d260f627f2a29a63718243bab5a6ed5a,How different is the dataset size of source and target?,,the training dataset is large while the target dataset is usually much smaller,0.9387074112892151,0.05869968235492706
c7d99e66c4ab555fe3d616b15a5048f3fe1f3f0e,What is an example of a computational social science NLP task?,,Visualization of State of the union addresses,0.46344858407974243,0.10091562569141388
c82e945b43b2e61c8ea567727e239662309e9508,What additional features are proposed for future work?,,distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort,0.9603080749511719,0.023363661020994186
ca26cfcc755f9d0641db0e4d88b4109b903dbb26,How better are results compared to baseline models?,,F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.,0.6991055011749268,0.01762983947992325
ca7e71131219252d1fab69865804b8f89a2c0a8f,How does this compare to traditional calibration methods like Platt Scaling?,,No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.,0.9708681702613831,-0.14732111990451813
cacb83e15e160d700db93c3f67c79a11281d20c5,Does this paper propose a new task that others can try to improve performance on?, past empirical results suggest two possible hypotheses of improving the model performance,"No, there has been previous work on recognizing social norm violation.",0.29827257990837097,0.10663896799087524
caf9819be516d2c5a7bfafc80882b07517752dfa,Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?,,They evaluate quantitatively.,0.9870338439941406,0.025191692635416985
cb78e280e3340b786e81636431834b75824568c3,How many emotions do they look at?,,9,0.6981241106987,0.3463934659957886
cb8a6f5c29715619a137e21b54b29e9dd48dad7d,"What is an ""answer style""?",,well-formed sentences vs concise answers,0.9951541423797607,0.07681803405284882
cbbcafffda7107358fa5bf02409a01e17ee56bfd,Was any variation in results observed based on language typology?,,It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information.,0.031333062797784805,0.02711259387433529
cc5d3903913fa2e841f900372ec74b0efd5e0c71,Which sentiment analysis tasks are addressed?," b  d, b  e, b  k, d  b, d  e, d  k, e  b, e  d, e  k, k  b, k  d, k  e",12 binary-class classification and multi-class classification of reviews based on rating,0.5035182237625122,0.21079334616661072
cc5d8e12f6aecf6a5f305e2f8b3a0c67f49801a9,What percentage fewer errors did professional translations make?,,36%,0.9983302354812622,0.22865775227546692
cd8de03eac49fd79b9d4c07b1b41a165197e1adb,How are multimodal representations combined?,,The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.,0.9899458885192871,0.06795907020568848
cdf1bf4b202576c39e063921f6b63dc9e4d6b1ff,What metric is used to measure performance?,,"Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks",0.9867379665374756,0.048019081354141235
ce807a42370bfca10fa322d6fa772e4a58a8dca1,What are the four forums the data comes from?,,"Darkode,  Hack Forums, Blackhat and Nulled.",0.9985113143920898,0.13425619900226593
cf93a209c8001ffb4ef505d306b6ced5936c6b63,From when are many VQA datasets collected?,,late 2014,0.7903040051460266,0.29475677013397217
cfbccb51f0f8f8f125b40168ed66384e2a09762b,How are discourse embeddings analyzed?,,They perform t-SNE clustering to analyze discourse embeddings,0.9920859932899475,0.004997185431420803
cfffc94518d64cb3c8789395707e4336676e0345,What approaches without reinforcement learning have been tried?,,"classification, regression, neural methods",0.994024932384491,0.014356113970279694
d028dcef22cdf0e86f62455d083581d025db1955,What are the strong baselines you have?,,optimize single task with no synthetic data,0.9321075677871704,0.002923784777522087
d0bc782961567dc1dd7e074b621a6d6be44bb5b4,How big is seed lexicon used for training?,,30 words,0.9941419363021851,0.274080365896225
d0c79f4a5d5c45fe673d9fcb3cd0b7dd65df7636,What are new best results on standard benchmark?,,"New best results of accuracy (P@1) on Vecmap:
Ours-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43",0.9809418320655823,0.05487583205103874
d0f831c97d345a5b8149a9d51bf321f844518434,What labels are in the dataset?,,binary label of stress or not stress,0.9144371151924133,0.0035974804777652025
d2fbf34cf4b5b1fd82394124728b03003884409c,Who was the top-scoring team?,,IDEA,0.9728937149047852,0.3686332404613495
d3092f78bdbe7e741932e3ddf997e8db42fa044c,What experimental evaluation is used?,,root mean square error between the actual and the predicted price of Bitcoin for every minute,0.9534769654273987,0.06656941026449203
d3bcfcea00dec99fa26283cdd74ba565bc907632,How big is dataset for this challenge?,,"133,287 images",0.16284450888633728,0.1589561402797699
d484a71e23d128f146182dccc30001df35cdf93f,How much is proposed model better in perplexity and BLEU score than typical UMT models?,,"Perplexity of the best model is 65.58 compared to best baseline 105.79.
Bleu of the best model is 6.57 compared to best baseline 5.50.",0.9071900844573975,0.034610945731401443
d5256d684b5f1b1ec648d996c358e66fe51f4904,what is the practical application for this paper?,,Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools.,0.20419180393218994,0.05967637896537781
d5498d16e8350c9785782b57b1e5a82212dbdaad,How accurate is model trained on text exclusively?,,Relative error is less than 5%,0.6373538374900818,-0.020757274702191353
d5fa26a2b7506733f3fa0973e2fe3fc1bbd1a12d,"How are possible sentence transformations represented in dataset, as new sentences?", in annotationinstructions,"Yes, as new sentences.",0.23631471395492554,0.2755776643753052
d60a3887a0d434abc0861637bbcd9ad0c596caf4,What semantic rules are proposed?,,rules that compute polarity of words after POS tagging or parsing steps,0.21875835955142975,0.05180884152650833
d653d994ef914d76c7d4011c0eb7873610ad795f,How were breast cancer related posts compiled from the Twitter streaming API?,"How were breast cancer related posts compiled from the Twitter streaming API?the classifier correctly identified 1, 140 tweets ( 85. 6 % ) from 845 profiles. a total of 48, 113 tweets from these accounts were compiled from both the ` cancer ' ( 69 % ) and ` breast ' ` cancer ' ( 31 % ) feeds. we provided tweet frequency statistics in figure figref7. this is an indicator that this population of breast cancer patients and survivors are actively tweeting about topics related to ` cancer ' including their experiences and complications. results a set of 845 breast cancer patient self - diagnostic twitter profiles was compiled by implementing our logistic model","By using  keywords `breast' AND `cancer' in tweet collecting process. 
",0.1330198347568512,0.6972990036010742
d6e2b276390bdc957dfa7e878de80cee1f41fbca,What models other than standalone BERT is new model compared to?,,Only Bert base and Bert large are compared to proposed approach.,0.9958950877189636,0.03564438968896866
d6e8b32048ff83c052e978ff3b8f1cb097377786,"How are customer satisfaction, customer frustration and overall problem resolution data collected?",,By annotators on Amazon Mechanical Turk.,0.8316085338592529,0.11917800456285477
d70ba6053e245ee4179c26a5dabcad37561c6af0,Which datasets did they experiment on?,,ConciergeQA and AmazonQA,0.7922232747077942,0.19412676990032196
d71cb7f3aa585e256ca14eebdc358edfc3a9539c,Which models achieve state-of-the-art performances?,,"CELEX (Dutch and English) - SVM-HMM
Festival, E-Hitz and OpenLexique - Liang hyphenation
IIT-Guwahat - Entropy CRF",0.9929690361022949,0.12889090180397034
d77c9ede2727c28e0b5a240b2521fd49a19442e0,What's the input representation of OpenIE tuples into the model?, the concatenation of word embedding,word embeddings,0.20038634538650513,0.7783601880073547
d79d897f94e666d5a6fcda3b0c7e807c8fad109e,What result from experiments suggest that natural language based agents are more robust?,,Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances,0.9980370402336121,0.11094281077384949
d7d611f622552142723e064f330d071f985e805c,How many utterances are in the corpus?,,Total number of utterances available is: 70607 (37344 ENG + 33263 GER),0.9787870049476624,0.04609784111380577
d824f837d8bc17f399e9b8ce8b30795944df0d51,How do they show their model discovers underlying syntactic structure?,,By visualizing syntactic distance estimated by the parsing network,0.8779103755950928,0.0628986731171608
d8de12f5eff64d0e9c9e88f6ebdabc4cdf042c22,How much gain does the model achieve with pretraining MVCNN?,,0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj,0.9938375353813171,0.1385730654001236
da8bda963f179f5517a864943dc0ee71249ee1ce,How many layers does their system have?,,4 layers,0.9685665369033813,0.15296699106693268
dac2591f19f5bbac3d4a7fa038ff7aa09f6f0d96,what are the three methods presented in the paper?,"what are the three methods presented in the paper?model architectures although more classic graph methods were initially attempted, along the lines of bibref4, where the challenge of semantic drift in multi - hop inference was analysed and the effectiveness of information extraction methods was demonstrated, the following 3 methods ( which now easily surpass the score of our competition submission","Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.",0.11803313344717026,0.2938520610332489
dafa760e1466e9eaa73ad8cb39b229abd5babbda,How large is the dataset they generate?,,4.756 million sentences,0.9890111088752747,0.14214946329593658
dbdf13cb4faa1785bdee90734f6c16380459520b,What cluster identification method is used in this paper?,,"A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18",0.8666349649429321,0.058214373886585236
dbfce07613e6d0d7412165e14438d5f92ad4b004,What affective-based features are used?,,"affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count",0.737306535243988,0.09786400943994522
dc69256bdfe76fa30ce4404b697f1bedfd6125fe,What are the languages used to test the model?,,"Hindi, English and German (German task won)",0.6543142199516296,0.1071421429514885
dcb18516369c3cf9838e83168357aed6643ae1b8,Which retrieval system was used for baselines?,,The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.,0.982379138469696,-0.054545458406209946
dd155f01f6f4a14f9d25afc97504aefdc6d29c13,What aspects have been compared between various language models?,,"Quality measures using perplexity and recall, and performance measured using latency and energy usage. ",0.47281572222709656,-0.0013950248248875141
de12e059088e4800d7d89e4214a3997994dbc0d9,What are the baseline systems that are compared against?,,"The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM",0.9949349164962769,0.008441043086349964
df2839dbd68ed9d5d186e6c148fa42fce60de64f,How big is the provided treebank?,,"1448 sentences more than the dataset from Bhat et al., 2017",0.9926137924194336,0.14929750561714172
df79d04cc10a01d433bb558d5f8a51bfad29f46b,Which languages do they test on?,,"Answer with content missing: (Applications section) We use Wikipedia articles
in five languages
(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams
et al. (2017).
Select:
Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English",0.9964354038238525,0.009587083011865616
dfbab3cd991f86d998223726617d61113caa6193,"For the purposes of this paper, how is something determined to be domain specific knowledge?",,reviews under distinct product categories are considered specific domain knowledge,0.9758496880531311,0.07632999867200851
e051d68a7932f700e6c3f48da57d3e2519936c6d,Which pre-trained English NER model do they use?,,Bidirectional LSTM based NER model of Flair,0.9465517997741699,0.13281218707561493
e09e89b3945b756609278dcffb5f89d8a52a02cd,How many speeches are in the dataset?,,5575 speeches,0.9095110297203064,0.19319584965705872
e0b7acf4292b71725b140f089c6850aebf2828d2,How is annotation projection done when languages have different word order?,,"Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments.",0.935708224773407,0.05847732350230217
e111925a82bad50f8e83da274988b9bea8b90005,How do they collect the control corpus?,,Randomly from Twitter,0.9512808322906494,0.15832430124282837
e1b36927114969f3b759cba056cfb3756de474e4,By how much does using phonetic feedback improve state-of-the-art systems?,,Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9,0.978971004486084,0.13063573837280273
e2427f182d7cda24eb7197f7998a02bc80550f15,How is the architecture fault-tolerant?,,By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault,0.9884918332099915,0.025454005226492882
e28019afcb55c01516998554503bc1b56f923995,"Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",,Personal thought of the annotator.,0.9342358112335205,0.1261993646621704
e292676c8c75dd3711efd0e008423c11077938b1,Which soft-selection approaches are evaluated?,,LSTM and BERT ,0.9741193056106567,0.11542478203773499
e2f269997f5a01949733c2ec8169f126dabd7571,Which data sources do they use?,,"- En-Fr (WMT14)
- En-De (WMT15)
- Skipthought (BookCorpus)
- AllNLI (SNLI + MultiNLI)
- Parsing (PTB + 1-billion word)",0.9577978253364563,0.07517987489700317
e3981a11d3d6a8ab31e1b0aa2de96f253653cfb2,For which languages most of the existing MRC datasets are created?,,English,0.9052191972732544,0.28494971990585327
e414d819f10c443cbefa8bdb9bd486ffc6d1fc6a,What is the average length of the recordings?,,40 minutes,0.9975519180297852,0.26435041427612305
e4a19b91b57c006a9086ae07f2d6d6471a8cf0ce,How are the main international development topics that states raise identified?,," They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.",0.9708235859870911,0.038565170019865036
e4cc2e73c90e568791737c97d77acef83588185f,How long is the dataset?,,8000,0.9962102770805359,0.190532848238945
e51d0c2c336f255e342b5f6c3cf2a13231789fed,Which Twitter corpus was used to train the word vectors?,,They collected tweets in Russian language using a heuristic query specific to Russian,0.5106556415557861,0.05850335583090782
e5a965e7a109ae17a42dd22eddbf167be47fca75,What are the problems related to ambiguity in PICO sentence prediction tasks?,,Some sentences are associated to ambiguous dimensions in the hidden state output,0.9705158472061157,0.10554206371307373
e63bde5c7b154fbe990c3185e2626d13a1bad171,What is the performance achieved on NarrativeQA?,,"Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87",0.8795718550682068,0.14765478670597076
e7329c403af26b7e6eef8b60ba6fefbe40ccf8ce,How much better does this baseline neural model do?,,"The model outperforms at every point in the
implicit-tuples PR curve reaching almost 0.8 in recall",0.9920100569725037,0.1056547462940216
e76139c63da0f861c097466983fbe0c94d1d9810,Is the model presented in the paper state of the art?,,"No, supervised models perform better for this task.",0.9839490056037903,0.005748045165091753
e829f008d62312357e0354a9ed3b0827c91c9401,Which psycholinguistic and basic linguistic features are used?,,"Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features",0.9627633094787598,0.13692402839660645
e84ba95c9a188fda4563f45e53fbc8728d8b5dab,Do they build one model per topic or on all topics?,,One model per topic.,0.9956111311912537,0.10573916137218475
e8fcfb1412c3b30da6cbc0766152b6e11e17196c,What improvement does the MOE model make over the SOTA on language modelling?,,Perpexity is improved from 34.7 to 28.0.,0.9740661978721619,0.13047808408737183
e91692136033bbc3f19743d0ee5784365746a820,"It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?",,using multiple pivot sentences,0.9496972560882568,0.07042787224054337
e9ccc74b1f1b172224cf9f01e66b1fa9e34d2593,What metadata is included?,,"besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date",0.9870164394378662,0.049352362751960754
e9d9bb87a5c4faa965ceddd98d8b80d4b99e339e,How much do they outperform previous state-of-the-art?,,"On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.
On subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.",0.7799248099327087,0.08940140902996063
ea6764a362bac95fb99969e9f8c773a61afd8f39,What is the highest accuracy score achieved?,,82.0%,0.962223470211029,0.194332093000412
ead5dc1f3994b2031a1852ecc4f97ac5760ea977,How many category tags are considered?,,14 categories,0.9930828809738159,0.24664057791233063
eb5ed1dd26fd9adb587d29225c7951a476c6ec28,What are the results of the experiment?,,"They were able to create a language model from the dataset, but did not test.",0.6799973845481873,-0.06227465346455574
eb95af36347ed0e0808e19963fe4d058e2ce3c9f,What is the accuracy of the proposed technique?,,51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge,0.9814273118972778,0.010395350866019726
ec2b8c43f14227cf74f9b49573cceb137dd336e7,How is the speech recognition system evaluated?,"How is the speech recognition system evaluated?speakers were assigned either to training or evaluation sets, with proportions of  and , respectively ; then training and evaluation lists were built, accordingly. table reports statistics from the spoken data set. the id all identifies the whole data set, while clean defines the subset in which sentences containing background voices, incomprehensible speech and word fragments were excluded. relation to prior work. scientific literature is rich in approaches for automated assessment of spoken language proficiency. performance is directly dependent on asr accuracy",Speech recognition system is evaluated using WER metric.,0.20875035226345062,0.6290842890739441
ed11b4ff7ca72dd80a792a6028e16ba20fccff66,How do they obtain region descriptions and object annotations?,,they are available in the Visual Genome dataset,0.722579836845398,-0.01787254773080349
ed522090941f61e97ec3a39f52d7599b573492dd,What is triangulation?,,"Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.",0.2672745883464813,0.033626824617385864
ed7985e733066cd067b399c36a3f5b09e532c844,What is different in BERT-gen from standard BERT?,,"They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens.",0.9839349389076233,0.11864622682332993
ed7a3e7fc1672f85a768613e7d1b419475950ab4,Does this approach perform better in the multi-domain or single-domain setting?, single domain,single-domain setting,0.2540842294692993,0.8494572639465332
edb068df4ffbd73b379590762125990fcd317862,which benchmark tasks did they experiment on?, stanford sentiment treebank bibref7 and the ag english news corpus bibref3, They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task.,0.6211449503898621,0.6431533694267273
edb2d24d6d10af13931b3a47a6543bd469752f0c,How did the select the 300 Reddit communities for comparison?,,They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.,0.8496325612068176,0.16906385123729706
eddabb24bc6de6451bcdaa7940f708e925010912,How are the EAU text spans annotated?,,Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.,0.9402992725372314,0.01771738938987255
ee9b95d773e060dced08705db8d79a0a6ef353da,How are content clusters used to improve the prediction of incident severity?,,they are used as additional features in a supervised classification task,0.9962775111198425,0.060948971658945084
ef7b62a705f887326b7ebacbd62567ee1f2129b3,What were the baselines?,,"Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations",0.9173455834388733,0.003442059736698866
ef872807cb0c9974d18bbb886a7836e793727c3d,What contextual features are used?,,The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.,0.5462151765823364,0.05714106187224388
efb3a87845460655c53bd7365bcb8393c99358ec,What were their results on the three datasets?,,"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR",0.9812394976615906,0.04872262477874756
efc65e5032588da4a134d121fe50d49fe8fe5e8c,What supplemental tasks are used for multitask learning?, auxiliary tasks,"Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question",0.48220136761665344,0.3580471873283386
f10325d022e3f95223f79ab00f8b42e3bb7ca040,How are discourse features incorporated into the model?,,They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.,0.7572962045669556,0.02754630148410797
f1831b2e96ff8ef65b8fde8b4c2ee3e04b7ac4bf,How close do clusters match to ground truth tone categories?, closely,"NMI between cluster assignments and ground truth tones for all sylables is:
Mandarin: 0.641
Cantonese: 0.464",0.6102162599563599,0.032683439552783966
f1bd66bb354e3dabf5dc4a71e6f08b17d472ecc9,How do slot binary classifiers improve performance?,,by adding extra supervision to generate the slots that will be present in the response,0.28014156222343445,0.04909803345799446
f2c5da398e601e53f9f545947f61de5f40ede1ee,How do their interpret the coefficients?,,The coefficients are projected back to the dummy variable space.,0.8787297010421753,0.04342021048069
f398587b9a0008628278a5ea858e01d3f5559f65,By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?,,"SPNet vs best baseline:
ROUGE-1: 90.97 vs 90.68
CIC: 70.45 vs 70.25",0.9934147000312805,0.09196978807449341
f4238f558d6ddf3849497a130b3a6ad866ff38b3,How is moral bias measured?,,"Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q) − cos(b, q)
Bias is calculated as substraction of cosine similarities of question and some answer for two opposite answers.",0.9384656548500061,0.0316300243139267
f42d470384ca63a8e106c7caf1cb59c7b92dbc27,What evaluation metrics are used?,,"exact match, f1 score, edit distance and goal match",0.9356527924537659,0.062422290444374084
f463db61de40ae86cf5ddd445783bb34f5f8ab67,what are the baselines?,,Perceptron model using the local features.,0.2607191205024719,0.0747072845697403
f4e17b14318b9f67d60a8a2dad1f6b506a10ab36,How is the generative model evaluated?,,Comparing BLEU score of model with and without attention,0.996243953704834,0.0406535342335701
f513e27db363c28d19a29e01f758437d7477eb24,what are the baselines?, attention sum reader,"AS Reader, GA Reader, CAS Reader",0.5158200263977051,0.42272859811782837
f52b2ca49d98a37a6949288ec5f281a3217e5ae8,How do they condition the output to a given target-source class?,,"They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.",0.9886292815208435,0.049605969339609146
f5707610dc8ae2a3dc23aec63d4afa4b40b7ec1e,What are resolution model variables?,"What are resolution model variables?resolution mode variables according to previous work bibref17, bibref18, bibref1, antecedents are resolved by different categories of information for different mentions","Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.",0.18669429421424866,0.45073139667510986
f5e571207d9f4701b4d01199ef7d0bfcfa2c0316,What are the linguistic differences between each class?,,"Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes",0.948899507522583,0.012811053544282913
f62c78be58983ef1d77049738785ec7ab9f2a3ee,what datasets did the authors use?,,"Kaggle
Subversive Kaggle
Wikipedia
Subversive Wikipedia
Reddit
Subversive Reddit ",0.9842566847801208,0.1981971710920334
f6380c60e2eb32cb3a9d3bca17cf4dc5ae584eca,Why is big data not appropriate for this task?, the size of a dataset does not correspond to its degree of relevance for a particular analysis,Training embeddings from small-corpora can increase the performance of some tasks,0.3556165397167206,0.20205473899841309
f68bd65b5251f86e1ed89f0c858a8bb2a02b233a,how do they collect the comparable corpus?,,Randomly from a Twitter dump,0.37142518162727356,0.06104201450943947
f7817b949605fb04b1e4fec9dd9ca8804fb92ae9,Why does not the approach from English work on other languages?,,"Because, unlike other languages, English does not mark grammatical genders",0.25857606530189514,0.05044739693403244
f7c34b128f8919e658ba4d5f1f3fc604fb7ff793,What are all the input modalities considered in prior work in question generation?,,"Textual inputs, knowledge bases, and images.",0.9977895021438599,0.07780130207538605
fa2a384a23f5d0fe114ef6a39dced139bddac20e,How big is the dataset?,,903019 references,0.9583562016487122,0.12136232852935791
fa2ffc6b4b046e17bc41e199855c4941673e2caf,What parallel corpus did they use?, monolingual,Parallel monolingual corpus in English and Mandarin,0.2687797248363495,0.5925003290176392
fa3312ae4bbed11a5bebd77caf15d651962e0b26,What was the performance on the self-collected corpus?,,F1 scores of 86.16 on slot filling and 94.56 on intent detection,0.9806433916091919,0.10462991893291473
fb5ce11bfd74e9d7c322444b006a27f2ff32a0cf,What is task success rate achieved? ,,96-97.6% using the objects color or shape and 79% using shape alone,0.14421243965625763,0.01103416457772255
fb76e994e2e3fa129f1e94f1b043b274af8fb84c,How does the context-aware variational autoencoder learn event background information?,," CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.",0.9593514204025269,0.03375924378633499
fbe5e513745d723aad711ceb91ce0c3c2ceb669e,What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?,,None,0.9871564507484436,0.35559049248695374
fbee81a9d90ff23603ee4f5986f9e8c0eb035b52,What are their initial results on this task?,,"Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.",0.11145871877670288,0.0585755817592144
fc3f0eb297b2308b99eb4661a510c9cdbb6ffba2,What is the size of the dataset?,,3029,0.9846582412719727,0.24073486030101776
fc8bc6a3c837a9d1c869b7ee90cf4e3c39bcd102,Is the baseline a non-heirarchical model like BERT?,,There were hierarchical and non-hierarchical baselines; BERT was one of those baselines,0.9916107058525085,0.03178113326430321
fd7f13b63f6ba674f5d5447b6114a201fe3137cb,What language is the experiment done in?, natural,english language,0.5693298578262329,0.2472381591796875
fd8b6723ad5f52770bec9009e45f860f4a8c4321,What QA models were used?,,"A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.",0.9872688055038452,0.05155031383037567
fe90eec1e3cdaa41d2da55864c86f6b6f042a56c,What are the sources of the data?, 1000 raw data samples,"User reviews written in Chinese collected online for hotel, mobile phone, and travel domains",0.7146623134613037,-0.004423707723617554
fee5aef7ae521ccd1562764a91edefecec34624d,How does explicit constraint on the KL divergence term that authors propose looks like?,,"Answer with content missing: (Formula 2) Formula 2 is an answer: 
\big \langle\! \log p_\theta({x}|{z}) \big \rangle_{q_\phi({z}|{x})}  -  \beta |D_{KL}\big(q_\phi({z}|{x}) || p({z})\big)-C|",0.9452211260795593,0.029898546636104584
ff28d34d1aaa57e7ad553dba09fc924dc21dd728,What are their correlation results?, high or very high bibref23,High correlation results range from 0.472 to 0.936,0.1196240559220314,0.3892223834991455
ff2bcf2d8ffee586751ce91cf15176301267b779,What are the characteristics of the city dialect?,,Lexicon of the cities tend to use most forms of a particular concept,0.9935650825500488,0.023318124935030937
