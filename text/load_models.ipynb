{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118b485a",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee791dc3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e61c74",
   "metadata": {},
   "source": [
    "# Loading and testing the Question Answering models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f8d62",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"Starting download and caching of Hugging Face models...\")\n",
    "\n",
    "QA_MODELS = [\n",
    "      \"deepset/tinyroberta-squad2\",\n",
    "      \"deepset/roberta-base-squad2\",\n",
    "      \"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70919072",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for model_name in QA_MODELS:\n",
    "    try:\n",
    "        # print(f\"Loading model: {model_name}\")\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # This is the essential part for downloading and caching the model\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "        \n",
    "        # The following lines simulate a quick inference to ensure everything is loaded correctly\n",
    "        question = \"How many programming languages does BLOOM support?\"\n",
    "        context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\"\n",
    "        inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        answer_start_index = outputs.start_logits.argmax()\n",
    "        answer_end_index = outputs.end_logits.argmax()\n",
    "        predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "        decoded_answer = tokenizer.decode(predict_answer_tokens)\n",
    "        \n",
    "        print(f\"Successfully loaded and tested: {model_name}. Predicted answer: '{decoded_answer}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model {model_name}. Error: {e}\")\n",
    "\n",
    "print(\"Model loading process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c393c2",
   "metadata": {},
   "source": [
    "# Loading and Testing the Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb4416",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAMES = [\"all-MiniLM-L6-v2\", \n",
    "              \"all-mpnet-base-v2\",\n",
    "              \"all-MiniLM-L12-v2\"]\n",
    "\n",
    "for model_name in EMBEDDING_MODEL_NAMES:\n",
    "    model_name = 'sentence-transformers/' + model_name\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    print(embedding_model)\n",
    "    docs = ['this is a test', 'I want to see if the model works properly', 'from the docker container.']\n",
    "    a = embedding_model.embed_documents(docs)\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
