question_id,question,model_answer,ground_truth,confidence,cosine_sim
0038b073b7cca847033177024f9719c971692042,How is the input triple translated to a slot-filling task?,,"The relation R(x,y) is mapped onto a question q whose answer is y",0.09852835536003113,0.016371306031942368
00bcdffff7e055f99aaf1b05cf41c98e2748e948,What is the baseline method for the task?,tensorflow backend,"For the emotion recognition from text they use described neural network as baseline.
For audio and face there is no baseline.",0.2396630048751831,0.14180725812911987
00ef9cc1d1d60f875969094bb246be529373cb1d,What methodology is used to compensate for limited labelled data?,binary labels,Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.,0.47862815856933594,0.15864785015583038
01123a39574bdc4684aafa59c52d956b532d2e53,By how much does their method outperform state-of-the-art OOD detection?,,"AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average",0.10225030779838562,0.09622544050216675
01dc6893fc2f49b732449dfe1907505e747440b0,What debate topics are included in the dataset?,"law, ethics","Ethics, Gender, Human rights, Sports, Freedom of Speech, Society, Religion, Philosophy, Health, Culture, World, Politics, Environment, Education, Digital Freedom, Economy, Science and Law",0.45033130049705505,0.3447180688381195
01edeca7b902ae3fd66264366bf548acea1db364,What are the results achieved from the introduced method?,comparison of different models,"Their model resulted in values of 0.476, 0.672 and 0.893 for recall at position 1,2 and 5 respectively in 10 candidates.",0.1904325634241104,0.2633415162563324
02348ab62957cb82067c589769c14d798b1ceec7,What simpler models do they look at?,,"BiGRUs with attention, ROUGE, Language model, and next sentence prediction ",0.07589086890220642,0.13931652903556824
02417455c05f09d89c2658f39705ac1df1daa0cd,How much does it minimally cost to fine-tune some model according to benchmarking framework?,3. 06 per hour,"$1,728",0.8329645395278931,0.21601372957229614
02e4bf719b1a504e385c35c6186742e720bcb281,How are relations used to propagate polarity?,from seed predicates that directly report one ' s emotions,"based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ",0.3835096061229706,0.25553280115127563
03ce42ff53aa3f1775bc57e50012f6eb1998c480,What 6 language pairs is experimented on?,muse,"EN<->ES
EN<->DE
EN<->IT
EN<->EO
EN<->MS
EN<->FI",0.06054704263806343,0.12094466388225555
04012650a45d56c0013cf45fd9792f43916eaf83,How much is performance hurt when using too small amount of layers in encoder?,decreasing the number of layers on the decoder side does not hurt the performance,"comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ",0.1501849740743637,0.49560388922691345
0457242fb2ec33446799de229ff37eaad9932f2a,Which elements of the platform are modular?,integrative,"handling large volume incoming data, sentiment analysis on tweets and predictive online learning",0.8872187733650208,0.15159547328948975
04b43deab0fd753e3419ed8741c10f652b893f02,What are the two decoding functions?,linear projection,"a linear projection and a bijective function with continuous transformation though  ‘affine coupling layer’ of (Dinh et al.,2016). ",0.12848903238773346,0.5978147387504578
04f72eddb1fc73dd11135a80ca1cf31e9db75578,How much more coverage is in the new dataset?,better coverage,278 more annotations,0.2672751843929291,0.18394991755485535
05671d068679be259493df638d27c106e7dd36d0,What is the performance proposed model achieved on MathQA?,what is the performance proposed model achieved on mathqa?,"Operation accuracy: 71.89
Execution accuracy: 55.95",0.03928804397583008,0.3911072313785553
056fc821d1ec1e8ca5dc958d14ea389857b1a299,How many feature maps are generated for a given triple?,4,3 feature maps for a given tuple,0.0509042963385582,0.22101396322250366
06095a4dee77e9a570837b35fc38e77228664f91,"Is all text in this dataset a question, or are there unrelated sentences in between questions?",question - answer pairs,the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ,0.21828904747962952,0.17986658215522766
068dbcc117c93fa84c002d3424bafb071575f431,How was quality measured?,inter - annotator agreement,"Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.",0.2631027400493622,0.7593716979026794
07580f78b04554eea9bb6d3a1fc7ca0d37d5c612,Can the approach be generalized to other technical domains as well? ,"can the approach be generalized to other technical domains as well? float selected : figure 2 : nmt training after replacing technical term pairs with technical term tokens “ tti ” ( i = 1, 2,... ) ; float selected : figure 3 : nmt decoding with technical term tokens “ tti ” ( i = 1, 2,... ) and smt technical term translation ; float selected : figure 4 : nmt rescoring of 1, 000 - best smt translations with technical term tokens “ tti ” ( i = 1, 2,... ) ; in this paper, we propose a method that enables nmt to translate patent sentences with a large vocabulary of technical terms. we use an nmt model similar to that used by sutskever et al. sutskever14, which uses a deep long short - term memories ( lstm ) bibref7 to encode the input sentence and a separate deep lstm to output the translation. we train the nmt model on a bilingual corpus in which the technical terms are replaced with technical term tokens ; this allows it to translate most of the source sentences except technical terms. similar to sutskever et al. sutskever14, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using statistical machine translation ( smt ). we also use it to rerank the 1, 000 - best smt translations on the basis of the average of the smt and nmt scores of the translated sentences that have been rescored with the technical term tokens. our experiments on japanese - chinese patent sentences show that our proposed nmt system achieves a substantial improvement of up to 3. 1 bleu points and 2. 3 ribes points over a traditional smt system and an improvement of approximately 0. 6 bleu points and 0. 8 ribes points over an equivalent nmt system without our proposed technique. ; one important difference between our nmt model and the one used by sutskever et al. sutskever14 is that we added an attention mechanism. recently, bahdanau et al. given a parallel sentence pair containing a japanese technical term, the chinese translation candidates collected from the phrase translation table are matched against the chinese sentence of the parallel sentence pair. of those found in, with the largest translation probability is selected, and the bilingual technical term pair is identified. ; for","There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.",0.10812628269195557,0.5468923449516296
07c59824f5e7c5399d15491da3543905cfa5f751,How big is dataset used for training/testing?,"4, 748","4,261  days for France and 4,748 for the UK",0.1546807885169983,0.2860376238822937
0828cfcf0e9e02834cc5f279a98e277d9138ffd9,How was the dataset collected?,we extracted 200 sentences from sorani kurdish books,extracted text from Sorani Kurdish books of primary school and randomly created sentences,0.19064587354660034,0.8589099645614624
08333e4dd1da7d6b5e9b645d40ec9d502823f5d7,How much performance gap between their approach and the strong handcrafted method?,table 4,"0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C",0.16350297629833221,0.2234402447938919
085147cd32153d46dd9901ab0f9195bfdbff6a85,What are the baseline models?,simply concatenating embeddings for each word to form long vector inputs,"MC-CNN
MVCNN
CNN",0.21215073764324188,0.16637466847896576
093039f974805952636c19c12af3549aa422ec43,Is this library implemented into Torch or is framework agnostic?,modularity,It uses deep learning framework (pytorch),0.6035341024398804,0.104822538793087
097ab15f58cb1fce5b5ffb5082b8d7bbee720659,Which language has the lowest error rate reduction?,english,thai,0.25329774618148804,0.5038207173347473
0a3a8d1b0cbac559f7de845d845ebbfefb91135e,What accuracy is achieved by the speech recognition system?,,"Accuracy not available: WER results are reported 42.6 German, 35.9 English",0.06101150065660477,0.06139291450381279
0a7ac8eccbc286e0ab55bc5949f3f8d2ea2d1a60,How many improvements on the French-German translation benchmark?,how many improvements on the french - german translation benchmark?,one,0.1049128994345665,-0.01207924447953701
0b31eb5bb111770a3aaf8a3931d8613e578e07a8,"What are the selection criteria for ""causal statements""?",,"Presence of only the exact unigrams 'caused', 'causing', or 'causes'",0.06807316839694977,0.08478130400180817
0b411f942c6e2e34e3d81cc855332f815b6bc123,What's the method used here?,,Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8.,0.08681861311197281,0.01894042268395424
0b54032508c96ff3320c3db613aeb25d42d00490,What is the training and test data used?,,"Tweets related to a Bank of America DDos attack were used as training data. The test datasets contain tweets related to attacks to Bank of America, PNC and Wells Fargo.",0.055243171751499176,0.01571107655763626
0bd683c51a87a110b68b377e9a06f0a3e12c8da0,What are the tasks that this method has shown improvements?,bilingual dictionary induction and monolingual and cross - lingual word similarity,"bilingual dictionary induction, monolingual and cross-lingual word similarity, and cross-lingual hypernym discovery",0.1523074507713318,0.9095362424850464
0bd864f83626a0c60f5e96b73fb269607afc7c09,How are sentence embeddings incorporated into the speech recognition system?,,BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer.,0.09488961100578308,0.06288011372089386
0c09a0e8f9c5bdb678563be49f912ab6e3f97619,How many roles are proposed?,semantic role,12,0.11663330346345901,0.12698376178741455
0c7823b27326b3f5dff51f32f45fc69c91a4e06d,In which setting they achieve the state of the art?,evaluation results on vqa dataset,in open-ended task esp. for counting-type questions ,0.10135851055383682,0.2055152952671051
0d7de323fd191a793858386d7eb8692cc924b432,What writing styles are present in the corpus?,stylistic,"current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.",0.39384108781814575,0.15520304441452026
0da6cfbc8cb134dc3d247e91262f5050a2200664,What topic clusters are identified by LDA?,inlineform0,"Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club",0.392024964094162,-0.04593726992607117
0fcac64544842dd06d14151df8c72fc6de5d695c,What previous methods is the proposed method compared against?,our approaches on swda dataset,"BLSTM+Attention+BLSTM
Hierarchical BLSTM-CRF
CRF-ASN
Hierarchical CNN (window 4)
mLSTM-RNN
DRLM-Conditional
LSTM-Softmax
RCNN
CNN
CRF
LSTM
BERT",0.2152767926454544,0.2782291769981384
0fd678d24c86122b9ab27b73ef20216bbd9847d1,What evaluation metrics are used?,what evaluation metrics are used?,Accuracy on each dataset and the average accuracy on all datasets.,0.10800257325172424,0.4171909987926483
10ddc5caf36fe9d7438eb5a3936e24580c4ffe6a,Which competitive relational classification models do they test?,position - aware neural sequence model,For relation prediction they test TransE and for relation extraction they test position aware neural sequence model,0.2824833393096924,0.5444269180297852
1165fb0b400ec1c521c1aef7a4e590f76fee1279,How do they model travel behavior?,dummy encoding,The data from collected travel surveys is used to model travel behavior.,0.3295787572860718,0.0530468225479126
11d2f0d913d6e5f5695f8febe2b03c6c125b667c,How is performance of this system measured?,human evaluation,using the BLEU score as a quantitative metric and human evaluation for quality,0.2188551276922226,0.5279308557510376
11dde2be9a69a025f2fc29ce647201fb5a4df580,By how much does the new parser outperform the current state-of-the-art?,slightly,Proposed method achieves 94.5 UAS and 92.4 LAS  compared to 94.3 and 92.2 of best state-of-the -art greedy based parser. Best state-of-the art parser overall achieves 95.8 UAS and 94.6 LAS.,0.20277340710163116,0.0664549395442009
126e8112e26ebf8c19ca7ff3dd06691732118e90,What are simulated datasets collected?,users open some existing documents in a file system,There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents,0.07998459041118622,0.2158181369304657
12ac76b77f22ed3bcb6430bcd0b909441d79751b,What are the competing models?,float selected : table 2 : results on emnlp2017 wmt news dataset. the 95 % confidence intervals from multiple trials,"TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.",0.04785481467843056,0.23051095008850098
12cfbaace49f9363fcc10989cf92a50dfe0a55ea,what results do they achieve?,"overall performance increases from 90. 87 % to 91. 93 % inlineform0 for the conll 2003 ner task, a more then 1 % absolute f1 increase, and a substantial improvement over the previous state of the art",91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task,0.16072297096252441,0.7015514373779297
12eaaf3b6ebc51846448c6e1ad210dbef7d25a96,How many convolutional layers does their model have?,five,wav2vec has 12 convolutional layers,0.7259802222251892,0.08937554806470871
13d92cbc2c77134626e26166c64ca5c00aec0bf5,What baseline approaches do they compare against?,what baseline approaches do they compare against?,"HotspotQA: Yang, Ding, Muppet
Fever: Hanselowski, Yoneda, Nie",0.10504614561796188,-0.0016817152500152588
13e87f6d68f7217fd14f4f9a008a65dd2a0ba91c,What is the performance of the model?,achieves the best acc and f1 on all datasets,"Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates
Experiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)",0.12079664319753647,0.3900224566459656
144714fe0d5a2bb7e21a7bf50df39d790ff12916,What are state of the art methods authors compare their work with? ,baselines including hybrid cnn bibref15 and lstm with attention bibref16,"ISOT dataset: LLVM
Liar dataset: Hybrid CNN and LSTM with attention",0.4529748857021332,0.5694676637649536
14634943d96ea036725898ab2e652c2948bd33eb,What is the accuracy of the model for the six languages tested?,,"Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)",0.09106438606977463,0.024534745141863823
14e259a312e653f8fc0d52ca5325b43c3bdfb968,"Is any data-to-text generation model trained on this new corpus, what are the results?","we confirmed on the e2e dataset that it performed on par with, or even slightly better than, the strong baseline models from the e2e nlg challenge","Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.",0.09755508601665497,0.26393014192581177
14eb2b89ba39e56c52954058b6b799a49d1b74bf,How are their changes evaluated?,used the incrementalinterpreter,The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset,0.05054333060979843,0.22747614979743958
157b9f6f8fb5d370fa23df31de24ae7efb75d6f3,How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?,we compare our gender and variety accuracies against the ldr - baseline bibref10,"They achieved best result in the PAN 2017 shared task with accuracy for Variety prediction task 0.0013 more than the 2nd best baseline, accuracy for Gender prediction task 0.0029 more than 2nd best baseline and accuracy for Joint prediction task 0.0101 more than the 2nd best baseline",0.12247107177972794,0.4975266456604004
160e6d2fc6e04bb0b4ee8d59c06715355dec4a17,What accuracy score do they obtain?,highest scores,the best performing model obtained an accuracy of 0.86,0.07796163856983185,0.3053118586540222
16af38f7c4774637cf8e04d4b239d6d72f0b0a3a,How large is the dataset?,multilingual corpora,over 104k documents,0.1495925933122635,0.09554857015609741
16f71391335a5d574f01235a9c37631893cd3bb0, What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?,much faster,"Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)",0.04182823747396469,0.26706576347351074
171ebfdc9b3a98e4cdee8f8715003285caeb2f39,How better is accuracy of new model compared to previously reported models?,it achieves the best performance on average,"Average accuracy of proposed model vs best prevous result:
Single-task Training: 57.57 vs 55.06
Multi-task Training: 50.17 vs 50.59",0.048100389540195465,0.3539026975631714
1771a55236823ed44d3ee537de2e85465bf03eaf,What is the difference in recall score between the systems?,float selected : table 2. comparison with traditional ners,"Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.",0.024519847705960274,0.3239181339740753
18288c7b0f8bd7839ae92f9c293e7fb85c7e146a,How strong was the correlation between exercise and diabetes?,the strongest correlation among the topics was determined to be between exercise and obesity ( inlineform0 ),weak correlation with p-value of 0.08,0.08292754739522934,0.10598110407590866
182b6d77b51fa83102719a81862891f49c23a025,What limitations are mentioned?,"deciding publisher partisanship, the number of people from whom we computed the score was small","deciding publisher partisanship, risk annotator bias because of short description text provided to annotators",0.0236665066331625,0.5572531819343567
18fbf9c08075e3b696237d22473c463237d153f5,Did the annotators agreed and how much?,90. 5 %,"For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.",0.1632619947195053,0.22391732037067413
1959e0ebc21fafdf1dd20c6ea054161ba7446f61,How is the clinical text structuring task defined?,fetching medical research data from electronic health records,CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.,0.10184213519096375,0.5183678269386292
19c9cfbc4f29104200393e848b7b9be41913a7ac,How many questions are in the dataset?,"2, 714","2,714 ",0.6023526787757874,1.0
1a69696034f70fb76cd7bb30494b2f5ab97e134d,By how much does their model outperform existing methods?,f1 score,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result.,0.11187876760959625,0.6424553394317627
1a8b7d3d126935c09306cacca7ddb4b953ef68ab,What were their results?,nalcs ( english ) and lms ( traditional chinese ) datasets,Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set,0.36839935183525085,0.4234667122364044
1adbdb5f08d67d8b05328ccc86d297ac01bf076c,What languages do they use?,,"Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.",0.10727478563785553,-0.04099542647600174
1b9119813ea637974d21862a8ace83bc1acbab8e,What dataset do they use?,"what dataset do they use? the fundamental idea of this system is how to make a system that has the diversity of viewing an input. that because of the variety of the meaning in vietnamese language especially with the acronym, teen code type. to make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words bibref5, pre - trained embedding as fasttext ( trained on wiki vietnamese language ) bibref6 and sonvx ( trained on vietnamese newspaper ) bibref7. each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. we also make a sentence embedding by using roberta architecture bibref8. cbow and roberta models trained on text from some resources including vlsp 2016 sentiment analysis, vlsp 2018 sentiment analysis, vlsp 2019 hsd and text crawled from facebook. the model tends to output all clean classes.. after having sentence representation, we use some classification models to classify input sentences. those models will be described in detail in the section secref13. with the multiply output results, we will use an ensemble method to combine them and output the final result. ensemble method we use here is stacking method will be introduced in the section secref16. ; the dataset in this hsd task is really imbalance",They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).,0.07445336878299713,0.5578634142875671
1baf87437b70cc0375b8b7dc2cfc2830279bc8b5,"What is the source of the ""control"" corpus?",,"Randomly selected from a Twitter dump, temporally matched to causal documents",0.0682779923081398,0.08076364547014236
1c4cd22d6eaefffd47b93c2124f6779a06d2d9e1,How big is their dataset?,,"3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing",0.10936076194047928,0.10786581039428711
1c68d18b4b65c4d75dc199d2043079490f6310f8,What are the two PharmaCoNER subtasks?,"proteinas, normalizables, no _ normalizables and unclear",Entity identification with offset mapping and concept indexing,0.1274353265762329,0.056843675673007965
1cbca15405632a2e9d0a7061855642d661e3b3a7,How much improvement do they get?,experimental results,Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.,0.2193775177001953,0.18517056107521057
1d74fd1d38a5532d20ffae4abbadaeda225b6932,What is their f1 score and recall?,comparison with traditional ners,"F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.",0.019985882565379143,0.3871208429336548
1ed6acb88954f31b78d2821bb230b722374792ed,What is private dashboard?,leaderboard,Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).,0.34949707984924316,0.44587063789367676
1f63ccc379f01ecdccaa02ed0912970610c84b72,How much is the gap between using the proposed objective and using only cross-entropy objective?,"the deep residual coattention yielded the highest contribution to model performance, followed by the mixed objective",The mixed objective improves EM by 2.5% and F1 by 2.2%,0.04484538733959198,0.5086537599563599
1fb73176394ef59adfaa8fc7827395525f9a5af7,Where did they get training data?,conciergeqa,AmazonQA and ConciergeQA datasets,0.3394071161746979,0.44844767451286316
20ec88c45c1d633adfd7bff7bbf3336d01fb6f37,what are the evaluation metrics?,conll 2003 english results,"Precision, Recall, F1",0.13476751744747162,0.25908738374710083
2122bd05c03dde098aa17e36773e1ac7b6011969,What task do they evaluate on?,answering open - domain fill - in - the - blank natural language questions,Fill-in-the-blank natural language questions,0.6590794920921326,0.8557713627815247
21663d2744a28e0d3087fbff913c036686abbb9a,How does their model differ from BERT?,"12 layers ( i. e., transformer blocks ), a hidden size of 768, and 12 self - attention heads",Their model does not differ from BERT.,0.07506411522626877,0.07539099454879761
2210178facc0e7b3b6341eec665f3c098abef5ac,What type of recurrent layers does the model use?,one - hot word vectors,GRU,0.05726468190550804,0.12335950881242752
22b8836cb00472c9780226483b29771ae3ebdc87,What is the new initialization method proposed in this paper?,skip - gram with negative - sampling,They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.,0.3930821418762207,0.3007083833217621
22c802872b556996dd7d09eb1e15989d003f30c0,How do they correlate NED with emotional bond levels?,we compute the correlation of the proposed ned measure,They compute Pearson’s correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating,0.09141423553228378,0.42680102586746216
234ccc1afcae4890e618ff2a7b06fc1e513ea640,How big is performance improvement proposed methods are used?,,"Data augmentation (es)  improved Adv es by 20% comparing to baseline 
Data augmentation (cs) improved Adv cs by 16.5% comparing to baseline
Data augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline 
All models show improvements over adversarial sets  
",0.012293078005313873,0.11762212216854095
2376c170c343e2305dac08ba5f5bda47c370357f,How was the dataset collected?,database construction,"They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ",0.27476218342781067,0.4311148226261139
238ec3c1e1093ce2f5122ee60209b969f7669fae,How is the fluctuation in the sense of the word and its neighbors measured?,surrounding uniformity,"Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.
4) Computing the mean m and the sample variance σ for the uniformities of ai .
5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.",0.2884361147880554,0.2488732784986496
23d32666dfc29ed124f3aa4109e2527efa225fbc,Do the use word embeddings alone or they replace some previous features of the model with word embeddings?,we link two nodes ( words ) if the corresponding word embedding representation is similar,They use it as addition to previous model - they add new edge between words if word embeddings are similar.,0.024907711893320084,0.4237954616546631
255fb6e20b95092c548ba47d8a295468e06698bd,What datasets are used to evaluate the introduced method?,,"They used a dataset from Taobao which contained a collection of conversation records between customers and customer service staffs. It contains over five kinds of conversations,
including chit-chat, product and discount consultation, querying delivery progress and after-sales feedback. ",0.12358107417821884,0.08695058524608612
25b2ae2d86b74ea69b09c140a41593c00c47a82b,How were the navigation instructions collected?,simulated environments,using Amazon Mechanical Turk using simulated environments with topological maps,0.1716931015253067,0.4062376022338867
25e4dbc7e211a1ebe02ee8dff675b846fb18fdc5,What external sources are used?,"what external sources are used? float selected : table 3 : statistics of external data. ; neural network models for nlp benefit from pretraining of word / character embeddings, learning distributed sementic information from large raw texts for reducing sparsity. the three basic elements in our neural segmentor, namely characters, character bigrams and words, can all be pretrained over large unsegmented data. we pretrain the five - character window network in figure figref13 as an unit, learning the mlp parameter together with character and bigram embeddings. we consider four types of commonly explored external data to this end, all of which have been studied for statistical word segmentation, but not for neural network segmentors. ; raw text. although raw texts do not contain explicit word boundary information, statistics such as mutual information between consecutive characters can be useful features for guiding segmentation bibref11. for neural segmentation, these distributional statistics can be implicitly learned by pretraining character embeddings. we therefore consider a more explicit clue for pretraining our character window network, namely punctuations bibref10. ; automatically segmented text. large texts automatically segmented by a baseline segmentor can be used for self - training bibref13 or deriving statistical features bibref12. we adopt a simple strategy, taking automatically segmented text as silver data to pretrain the five - character window network. given inlineform0, inlineform1 is derived using the mlp in figure figref13, and then used to classify the segmentation of inlineform2 into b ( begining ) / m ( middle ) / e ( end ) / s ( single character word ) labels. displayform0 ; heterogenous training data. multiple segmentation corpora exist for chinese, with different segmentation granularities. there has been investigation on leveraging two corpora under different annotation standards to improve statistical segmentation bibref16. we try to utilize heterogenous treebanks by taking an external treebank as labeled data, training a b / m / e / s classifier for the character windows network. displayform0 ; pos data. previous research has shown that pos information is closely related to segmentation bibref14, bibref15. we verify the utility of pos information for our segmentor by pretraining a classifier that predicts the po","Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily",0.085903599858284,0.3839293420314789
25fd61bb20f71051fe2bd866d221f87367e81027,What baselines have been used in this work?,four,"NDM, LIDM, KVRN, and TSCP/RL",0.17370550334453583,0.06865350902080536
26c290584c97e22b25035f5458625944db181552,What is the size of their dataset?,table 2,"10,001 utterances",0.11378716677427292,0.13589432835578918
26faad6f42b6d628f341c8d4ce5a08a591eea8c2,How many documents are in the Indiscapes dataset?,scripts,508,0.6477764248847961,0.09291844069957733
271019168ed3a2b0ef5e3780b48a1ebefc562b57,What was performance of classifiers before/after using distant supervision?,close to using the full 17k manually labeled token,"Bi-LSTM: For low resource <17k clean data: Using distant supervision resulted in huge boost of F1 score (1k eg. ~9 to ~36 wit distant supervision)
BERT: <5k clean data boost of F1 (1k eg. ~32 to ~47 with distant supervision)",0.3583980202674866,0.3119679093360901
2815bac42db32d8f988b380fed997af31601f129,What is improvement in accuracy for short Jokes in relation other types of jokes?,8 percent,It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%,0.06055482476949692,0.2974158525466919
281cd4e78b27a62713ec43249df5000812522a89,What is the average length of the claims?,inlineform0,Average claim length is 8.9 tokens.,0.3437933623790741,-0.004478177521377802
2869d19e54fb554fcf1d6888e526135803bb7d75,What performance did they obtain on the SemEval dataset?,,F1 score of 82.10%,0.0658494159579277,0.08331740647554398
28b2a20779a78a34fb228333dc4b93fd572fda15,Does proposed end-to-end approach learn in reinforcement or supervised learning manner?,,supervised learning,0.10748157650232315,0.1469505876302719
29d917cc38a56a179395d0f3a2416fca41a01659,How are the potentially relevant text fragments identified?,ibm ' s alchemyapi," Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5–10 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.",0.13541099429130554,0.3335544466972351
2a1e6a69e06da2328fc73016ee057378821e0754,How did they detect entity mentions?,create one node per mention,Exact matches to the entity string and predictions from a coreference resolution system,0.4604550898075104,0.29407352209091187
2a46db1b91de4b583d4a5302b2784c091f9478cc,How many examples do they have in the target domain?,tst2013,"Around 388k examples, 194k from tst2013 (in-domain) and 194k from newstest2014 (out-of-domain)",0.7932778596878052,0.46336042881011963
2a6469f8f6bf16577b590732d30266fd2486a72e,What is novel in author's approach?,using self - play learning for the neural response ranker,"They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data.",0.3782934546470642,0.44626811146736145
2cf8825639164a842c3172af039ff079a8448592,How is the data annotated?,metadata values of each user,The data are self-reported by Twitter users and then verified by two human experts.,0.13082091510295868,0.31264787912368774
2d274c93901c193cf7ad227ab28b1436c5f410af,What are the baselines that Masque is compared against?,,"BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D",0.09992176294326782,0.1363036036491394
2d307b43746be9cedf897adac06d524419b0720b,How long are the datasets?,,"Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses",0.011959155090153217,-0.017003275454044342
2d3bf170c1647c5a95abae50ee3ef3b404230ce4,Which baseline methods are used?,standard parametrized attention bibref2,standard parametrized attention and a non-attention baseline,0.2428635060787201,0.6784067153930664
2d47cdf2c1e0c64c73518aead1b94e0ee594b7a5,How big is slot filing dataset?,onsei intent,"Dataset has 1737 train, 497 dev and 559 test sentences.",0.09509725123643875,0.10560756921768188
2d4d0735c50749aa8087d1502ab7499faa2f0dd8,By how much do they outperform previous state-of-the-art models?,"by how much do they outperform previous state - of - the - art models? all models are trained and evaluated using the same ( w. r. t. data shuffle and split ) 10 - fold cross - validation ( cv ) on trafficking - 10k, except for htdn, whose result is read from the original paper bibref9. during each train - test split, inlineform0 of the training set is further reserved as the validation set for tuning hyperparameters such as l2 - penalty in it, at and lad, and learning rate in ornn. so the overall train - validation - test ratio is 70 % - 20 % - 10 %. we report the mean metrics from the cv in table tabref14. as previous research has pointed out that there is no unbiased estimator of the variance of cv bibref29, we report the naive standard error treating metrics across cv as independent. ; we can see that ornn has the best mae, inlineform0 and acc. as well as a close 2nd best wt. acc. among all models. its wt. acc. is a substantial improvement over htdn despite the fact that the latter use both text and image data. acc. ). the results are averaged across 10 - fold cv on trafficking10k with naive standard errors in the parentheses. the best and second best results are highlighted. ; float selected : table 2 : comparison of the proposed ordinal regression neural network ( ornn ) against immediate - threshold ordinal logistic regression ( it ), all - threshold ordinal logistic regression ( at ), least absolute deviation ( lad ), multi - class logistic regression ( mc ), and the human trafficking deep network ( htdn ) in terms of mean absolute error ( mae ), macro - averaged mean absolute error ( maem ), binary classification accuracy ( acc. ) and weighted binary classification accuracy ( wt. acc. ). the results are averaged across 10 - fold cv on trafficking10k with naive standard errors in the parentheses. the best and second best results are highlighted.. it is important to note that htdn is trained using binary labels, whereas the other models are trained using ordinal labels and then have their ordinal predictions converted to binary predictions. this is most likely the reason that even the baseline models except for lad can yield better wt. acc. than htdn,","Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)",0.09414677321910858,0.5193867683410645
2d536961c6e1aec9f8491e41e383dc0aac700e0a,What are all 15 types of modifications ilustrated in the dataset?,annotationinstructions,"- paraphrase 1
- paraphrase 2
- different meaning
- opposite meaning
- nonsense
- minimal change
- generalization
- gossip
- formal sentence
- non-standard sentence
- simple sentence
- possibility
- ban
- future
- past",0.46942129731178284,0.165284663438797
2df2f6e4efd19023434c84f5b4f29a2f00bfc9fb,What other non-neural baselines do the authors compare to? ,bow,"bag of words, tf-idf, bag-of-means",0.5135995149612427,0.056999728083610535
2e1660405bde64fb6c211e8753e52299e269998f,How long is the dataset?,,"645, 600000",0.10376280546188354,0.09842821955680847
2e1ededb7c8460169cf3c38e6cde6de402c1e720,What is the prediction accuracy of the model?,predicted mean,"mean prediction accuracy 0.99582651
S&P 500 Accuracy 0.99582651",0.13128790259361267,0.5560339689254761
2ec97cf890b537e393c2ce4c2b3bd05dfe46f683,How do they measure correlation between the prediction and explanation quality?,explanation performance should correlate with prediction performance.,They look at the performance accuracy of explanation and the prediction performance,0.06976030021905899,0.8324589729309082
2fa0b9d0cb26e1be8eae7e782ada6820bc2c037f,What were their accuracy results on the task?,table 3 : lemmatization accuracy using wikinews testset,97.32%,0.14285391569137573,0.12227589637041092
3070d6d6a52aa070f0c0a7b4de8abddd3da4f056,How do they measure performance of language model tasks?,table tabref39,"BPC, Perplexity",0.1913585513830185,0.10884780436754227
30803eefd7cdeb721f47c9ca72a5b1d750b8e03b,How well does their system perform on the development set of SRE?,,"EER 16.04, Cmindet 0.6012, Cdet 0.6107",0.10306698828935623,0.07186440378427505
30b5e5293001f65d2fb9e4d1fdf4dc230e8cf320,What text classification task is considered?,word embeddings and a uni - directional gru connected to a dense layer end - to - end for text classification on a set of scraped tweets,To classify a text as belonging to one of the ten possible classes.,0.09800710529088974,0.29899710416793823
30dad5d9b4a03e56fa31f932c879aa56e11ed15b,What is the 12 class bilingual text?,description of class label along with distribution of each class ( in % ) in the acquired dataset,"Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant",0.08274528384208679,0.0962471142411232
3103502cf07726d3eeda34f31c0bdf1fc0ae964e,How do Zipf and Herdan-Heap's laws differ?,"grows with the size of a text, the number of which, empirically, has been found to grow as a power of the text size","Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)",0.05760267749428749,0.40051302313804626
311a7fa62721e82265f4e0689b4adc05f6b74215,How do they define upward and downward reasoning?,upward entailing,"Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.",0.1635936051607132,0.6185213327407837
3213529b6405339dfd0c1d2a0f15719cdff0fa93,What is the baseline model used?,bert,"The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data",0.03756336495280266,0.2521170377731323
327e06e2ce09cf4c6cc521101d0aecfc745b1738,What evaluation metrics did they look at?,"[ 0, 1 ] y de su desviacion estandar",accuracy with standard deviation,0.5461452603340149,0.13358697295188904
327e6c6609fbd4c6ae76284ca639951f03eb4a4c,By how much is performance on CN-Celeb inferior to performance on VoxCeleb?,%,"For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb",0.779301106929779,0.010008192621171474
32e8eda2183bcafbd79b22f757f8f55895a0b7b2,How many categories of offensive language were there?,two,3,0.3804067373275757,0.7066473960876465
334f90bb715d8950ead1be0742d46a3b889744e7,What semantic features help in detecting whether a piece of text is genuine or generated? of ,statistical properties,"No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.",0.2620707154273987,0.36756354570388794
33d864153822bd378a98a732ace720e2c06a6bc6,What is new state-of-the-art performance on CoNLL-2009 dataset?,"table tabref46 shows that our open model achieves more than 3 points of f1 - score than the state - of - the - art result, and relawe with deppath & relpath achieves the best in both closed and open settings. notice that our best closed model can almost perform as well as the state - of - the - art model while the latter utilizes pre - trained word embeddings",In closed setting 84.22 F1 and in open 87.35 F1.,0.0371730662882328,0.3420666456222534
33f72c8da22dd7d1378d004cbd8d2dcd814a5291,What is the metric that is measures in this paper?,error rate,error rate in a minimal pair ABX discrimination task,0.3854406476020813,0.45309948921203613
34af2c512ec38483754e94e1ea814aa76552d60a,What benchmarks are created?,human annotators,Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples,0.08778897672891617,0.09265562891960144
34fab25d9ceb9c5942daf4ebdab6c5dd4ff9d3db,What dataset did they use?,,"weibo-100k, Ontonotes, LCQMC and XNLI",0.046036701649427414,0.18293973803520203
36a9230fadf997d3b0c5fc8af8d89bd48bf04f12,Which model architecture do they for sentence encoding?,standalone training objectives for sentence representations,"Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs
- RNN",0.10638312250375748,0.5115882158279419
36b25021464a9574bf449e52ae50810c4ac7b642,Where does the information on individual-level demographics come from?,,From Twitter profile descriptions of the users.,0.1097026914358139,0.1319243311882019
36feaac9d9dee5ae09aaebc2019b014e57f61fbf,How do they measure model size?,table tabref20,By the number of parameters.,0.3121667802333832,0.10958123952150345
3703433d434f1913307ceb6a8cfb9a07842667dd,What learning paradigms do they cover in this survey?,,"Considering ""What"" and ""How"" separately versus jointly optimizing for both.",0.10617717355489731,0.009681330993771553
3748787379b3a7d222c3a6254def3f5bfb93a60e,What linguistic quality aspects are addressed?,five,"Grammaticality, non-redundancy, referential clarity, focus, structure & coherence",0.5929960608482361,0.13661222159862518
37753fbffc06ce7de6ada80c89f1bf5f190bbd88,What document context was added?,a preceding and a following sentence,Preceding and following sentence of each metaphor and paraphrase are added as document context,0.8717782497406006,0.4310067296028137
37bc8763eb604c14871af71cba904b7b77b6e089,How is module that analyzes behavioral state trained?,,pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus,0.0923815369606018,0.08298034965991974
37c7c62c9216d6cf3d0858cf1deab6db4b815384,how was annotation done?,distributing it over dozens of people,Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations,0.3786645531654358,0.13730652630329132
37e8f5851133a748c4e3e0beeef0d83883117a98,How better is performance of proposed model compared to baselines?,final win rate,"Proposed model achive 66+-22 win rate, baseline CNN 13+-1  and baseline FiLM 32+-3 .",0.222702756524086,0.38702264428138733
384d571e4017628ebb72f3debb2846efaf0cb0cb,On what dataset is Aristo system trained?,"bid25 ). ; the regents exam questions are taken verbatim from the new york regents examination board, using the 4th grade science, 8th grade science, and 12th grade living environment examinations. the questions are partitioned into train / dev / test by exam, i. e., each exam is either in train, dev, or test but not split up between them. the arc","Aristo Corpus
Regents 4th
Regents 8th
Regents `12th
ARC-Easy
ARC-challenge ",0.046341609209775925,0.5191612839698792
38a5cc790f66a7362f91d338f2f1d78f48c1e252,What baseline is used?,bibref7,SVM,0.4552275240421295,0.23580233752727509
38c74ab8292a94fc5a82999400ee9c06be19f791,How large is the corpus?,multilingual corpora,"It contains 106,350 documents",0.061217714101076126,0.21149322390556335
39a450ac15688199575798e72a2cc016ef4316b5,How much performance improvements they achieve on SQuAD?,+ 4. 3 %,Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ,0.44804346561431885,0.26749545335769653
39c78924df095c92e058ffa5a779de597e8c43f4,How are the topics embedded in the #MeToo tweets extracted?,tf - idf,Using Latent Dirichlet Allocation on TF-IDF transformed from the corpus,0.2983498275279999,0.6966345906257629
39f8db10d949c6b477fa4b51e7c184016505884f,How does their model learn using mostly raw data?,a very small seed lexicon and a large raw corpus,by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity,0.12758837640285492,0.5414260625839233
3a3a65c65cebc2b8c267c334e154517d208adc7d,What extraction model did they use?,tuple,"Multi-Encoder, Constrained-Decoder model",0.3085784614086151,0.17010271549224854
3aa7173612995223a904cc0f8eef4ff203cbb860,What baseline models do they compare against?,experimental results,"SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)",0.30018070340156555,0.08033574372529984
3aee5c856e0ee608a7664289ffdd11455d153234,What was the performance of their model?,test datasets,"For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81",0.25507283210754395,0.3502810597419739
3b391cd58cf6a61fe8c8eff2095e33794e80f0e3,What is the dataset used in the paper?,webhose archived data,"historical S&P 500 component stocks
 306242 news articles",0.05605484917759895,0.0838361382484436
3b745f086fb5849e7ce7ce2c02ccbde7cfdedda5,By how much do they outperform other models in the sentiment in intent classification tasks?,6 to 8,In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average,0.5456513166427612,0.18317373096942902
3b995a7358cefb271b986e8fc6efe807f25d60dc,What types of word representations are they evaluating?,,GloVE; SGNS,0.14668774604797363,0.20303863286972046
3bfdbf2d4d68e01bef39dc3371960e25489e510e,how do they measure discussion quality?,annotation study of five transcripts of classroom discussion,"Measuring three aspects: argumentation, specificity and knowledge domain.",0.22257380187511444,0.33111757040023804
3c362bfa11c60bad6c7ea83f8753d427cda77de0,Why did they think this was a good idea?,to generate candidate prescriptions to facilitate the prescribing procedure instead of substituting the human practitioners completely,They think it will help human TCM practitioners make prescriptions.,0.0650629848241806,0.5682803392410278
3d49b678ff6b125ffe7fb614af3e187da65c6f65,"What does ""explicitly leverages their probabilistic correlation to guide the training process of both models"" mean?",regularization term,"The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization.",0.47682252526283264,0.4540025293827057
3d6015d722de6e6297ba7bfe7cb0f8a67f660636,What are the 12 categories devised?,float,"Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study",0.13381651043891907,0.03756488859653473
3e839783d8a4f2fe50ece4a9b476546f0842b193,What was their result on Stance Sentiment Emotion Corpus?,what was their result on stance sentiment emotion corpus? float selected : table ii f - score of various models on sentiment and emotion test dataset. ; we compare the performance of our proposed system with the state - of - the - art systems of semeval 2016 task 6 and the systems of bibref15. experimental results show that the proposed system improves the existing state - of - the - art systems,F1 score of 66.66%,0.07330715656280518,0.3031225800514221
3f326c003be29c8eac76b24d6bba9608c75aa7ea,What evaluation metric is used?,average and one standard deviation,F1 and Weighted-F1,0.2061905562877655,0.08075734227895737
3f3c09c1fd542c1d9acf197957c66b79ea1baf6e,How many annotators participated?,two thousand,1,0.6092033982276917,0.4047445058822632
3f5f74c39a560b5d916496e05641783c58af2c5d,How are the synthetic examples generated?,randomly perturbing 1. 8 million segments from wikipedia,"Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out",0.4556232690811157,0.5670974850654602
3fb4334e5a4702acd44bd24eb1831bb7e9b98d31,How big are the datasets used?,how big are the datasets used?,"Evaluation datasets used:
CMRC 2018 - 18939 questions, 10 answers
DRCD - 33953 questions, 5 answers
NIST MT02/03/04/05/06/08 Chinese-English - Not specified

Source language train data:
SQuAD - Not specified",0.028407270088791847,0.4310566782951355
405964517f372629cda4326d8efadde0206b7751,How is performance measured?,the area under the curve,they use ROC curves and cross-validation,0.22240397334098816,0.11635294556617737
4059c6f395640a6acf20a0ed451d0ad8681bc59b,How is the delta-softmax calculated?,"where is the true relation of the edu pair, represents the token at index of tokens, and represents the input sequence with the masked position",Answer with content missing: (Formula) Formula is the answer.,0.16119755804538727,0.13149172067642212
40c2bab4a6bf3c0628079fcf19e8b52f27f51d98,How do they generate the synthetic dataset?,generative process,using generative process,0.282473623752594,0.9558075070381165
415f35adb0ef746883fb9c33aa53b79cc4e723c3,"In the targeted data collection approach, what type of data is targetted?",male - gendered character personas,Gendered characters in the dataset,0.029115894809365273,0.6596490144729614
41d3ab045ef8e52e4bbe5418096551a22c5e9c43,what datasets were used?,"iwslt14 german - english, turkish - english, and wmt14 english - german","IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German",0.9133556485176086,0.9668579697608948
41e300acec35252e23f239772cecadc0ea986071,What neural machine translation models can learn in terms of transfer learning?,bibref22,Multilingual Neural Machine Translation Models,0.08403468877077103,0.1267629861831665
4226a1830266ed5bde1b349205effafe7a0e2337,What meta-information is being transferred?,common relation information,"high-order representation of a relation, loss gradient of relation meta",0.3079540431499481,0.4117949604988098
42394c54a950bae8cebecda9de68ee78de69dc0d,What is the source of external knowledge?,english wikipedia,counts of predicate-argument tuples from English Wikipedia,0.37475112080574036,0.2881525158882141
427252648173c3ba78c211b86fa89fc9f4406653,What domains are detected in this paper?,experimental setup,"Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW),
“Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). Exceptionally, GEN does
not have a natural root category.",0.8335446119308472,0.11232809722423553
43f86cd8aafe930ebb35ca919ada33b74b36c7dd,In what way is the input restructured?,conditioning it on the entity,"In four entity-centric ways - entity-first, entity-last, document-level and sentence-level",0.14179092645645142,0.36731094121932983
44104668796a6ca10e2ea3ecf706541da1cec2cf,What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?,diacritic swapping showed a remarkably poor performance,Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.,0.23832078278064728,0.15629135072231293
445e792ce7e699e960e2cb4fe217aeacdd88d392,How do this framework facilitate demographic inference from social media?,"utilizing these two weighted lexicon of terms, we are predicting the demographic information ( age or gender ) of inlineform0 ( denoted by inlineform1 ) using following equation : inlineform2",Demographic information is predicted using weighted lexicon of terms.,0.04689887538552284,0.7131208777427673
44c4bd6decc86f1091b5fc0728873d9324cdde4e,How big is the Japanese data?,,"7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus",0.07315298914909363,0.0627359002828598
4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb,what amounts of size were used on german-english?,159000 parallel sentences,"Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development",0.035312216728925705,0.5623601675033569
45893f31ef07f0cca5783bd39c4e60630d6b93b3,How do they select monotonicity facts?,semanticist linguist and swim move,They derive it from Wordnet,0.08311326056718826,0.37534773349761963
458dbf217218fcab9153e33045aac08a2c8a38c6,"How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets?",table 3,"Total number of annotated data:
Semeval'15: 10712
Semeval'16: 28632
Tass'15: 69000
Sentipol'14: 6428",0.3911573588848114,0.15986964106559753
45a2ce68b4a9fd4f04738085865fbefa36dd0727,what dataset was used?,table tabref16,The dataset from a joint ADAPT-Microsoft project,0.6580530405044556,0.15481823682785034
4640793d82aa7db30ad7b88c0bf0a1030e636558,what previous systems were compared to?,published state of the art results without additional labeled data or task specific gazetteers,"Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Søgaard and Goldberg (2016) ",0.4367556869983673,0.14176705479621887
4688534a07a3cbd8afa738eea02cc6981a4fd285,How do they combine MonaLog with BERT?,compositional data augmentation,They use Monalog for data-augmentation to fine-tune BERT on this task,0.1760765165090561,0.4676021933555603
4704cbb35762d0172f5ac6c26b67550921567a65,By how much does transfer learning improve performance on this task?,table 2,"In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%",0.04085276275873184,0.20103859901428223
471d624498ab48549ce492ada9e6129da05debac,What context modelling methods are evaluated?,13,"Concat
Turn
Gate
Action Copy
Tree Copy
SQL Attn
Concat + Action Copy
Concat + Tree Copy
Concat + SQL Attn
Turn + Action Copy
Turn + Tree Copy
Turn + SQL Attn
Turn + SQL Attn + Action Copy",0.30982813239097595,0.07875534892082214
496b4ae3c0e26ec95ff6ded5e6790f24c35f0f5b,How do they incorporate human advice?,explicitly represent advice in calculating gradients,by converting human advice to first-order logic format and use as an input to calculate gradient,0.36001288890838623,0.7375673055648804
4a4616e1a9807f32cca9b92ab05e65b05c2a1bf5,What were the sizes of the test sets?,what were the sizes of the test sets?,Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences,0.08236715942621231,0.4403281509876251
4a61260d6edfb0f93100d92e01cf655812243724,Which 3 NLP areas are cited the most?,gender of the first authors of corresponding papers,"machine translation, statistical machine, sentiment analysis",0.08088727295398712,0.14800599217414856
4b41f399b193d259fd6e24f3c6e95dc5cae926dd,How big dataset is used for training this system?,"how big dataset is used for training this system?. it is complemented by the cornell - movie dialogues dataset bibref27, which contains a collection of fictional conversations extracted from raw movie scripts. persona - chat ' s sentences have a maximum of 15 words, making it easier to learn for machines and a total of 162, 064 utterances over 10, 907 dialogues. while cornell - movie dataset contains 304, 713 utterances over 220, 579 conversational exchanges between 10, 292 pairs of movie characters. we use ms coco, bing and flickr datasets from bibref26 to train the model that generates questions. these datasets contain natural questions about images with the purpose of knowing more about the picture. as can be seen in the figure figref8, questions cannot be answered by only looking at the image. each source contains 5, 000 images with 5 questions per image, adding a total of 15, 000 images with 75, 000 questions. coco dataset includes images of complex everyday scenes containing common objects in their natural context, but it is limited in terms of the concepts it covers. bing dataset contains more event related questions and has a wider range of questions longitudes ( between 3 and 20 words ), while flickr questions are shorter ( less than 6 words ) and the images appear to be more casual. ; we use two datasets to train our chatbot model. the first one is the persona - chat bibref15","For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.",0.07458039373159409,0.6883971691131592
4b8257cdd9a60087fa901da1f4250e7d910896df,How do the authors define or exemplify 'incorrect words'?,sentences with missing or incorrect words,typos in spellings or ungrammatical words,0.40428072214126587,0.5816093683242798
4b8a0e99bf3f2f6c80c57c0e474c47a5ee842b2c,To what other competitive baselines is this approach compared?,vhred ( attn ) and mmi,"LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL",0.6202362775802612,0.3949581980705261
4c50f75b1302f749c1351de0782f2d658d4bea70,How is quality of annotation measured?,maximum 120 headlines,Annotators went through various phases to make sure their annotations did not deviate from the mean.,0.18362878262996674,0.024906113743782043
4c7ac51a66c15593082e248451e8f6896e476ffb,What is the performance proposed model achieved on AlgoList benchmark?,state - of - the - art,"Full Testing Set Accuracy: 84.02
Cleaned Testing Set Accuracy: 93.48",0.4587724804878235,0.025025561451911926
4c822bbb06141433d04bbc472f08c48bc8378865,How do they extract causality from text?,,"They identify documents that contain the unigrams 'caused', 'causing', or 'causes'",0.02554986998438835,0.034985676407814026
4ca0d52f655bb9b4bc25310f3a76c5d744830043,How large is the first dataset?,5 to 24 lines each,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,0.11676274240016937,0.22390194237232208
4d05a264b2353cff310edb480a917d686353b007,What kind of information do the HMMs learn that the LSTMs don't?,complementary features,The HMM can identify punctuation or pick up on vowels.,0.06693721562623978,0.12329613417387009
4d5e2a83b517e9c082421f11a68a604269642f29,how many domains did they experiment with?,two,2,0.8138626217842102,0.8183472156524658
4dad15fee1fe01c3eadce8f0914781ca0a6e3f23,How do they prevent the model complexity increasing with the increased number of slots?,slot - independent model,They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).,0.056260846555233,0.3138737082481384
4e9684fd68a242cb354fa6961b0e3b5c35aae4b6,What is the results of multimodal compared to unimodal models?,,"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4 ",0.026777086779475212,0.05292564630508423
4ef2fd79d598accc54c084f0cca8ad7c1b3f892a,What is the size of their collected dataset?,30 hours,3347 unique utterances ,0.708182156085968,0.13015544414520264
5067e5eb2cddbb34b71e8b74ab9210cd46bb09c5,Which matching features do they employ?,multi - perspective,Matching features from matching sentences from various perspectives.,0.8508809208869934,0.376869261264801
50716cc7f589b9b9f3aca806214228b063e9695b,What language technologies have been introduced in the past?,,"- Font & Keyboard
- Speech-to-Text
- Text-to-Speech
- Text Prediction
- Spell Checker
- Grammar Checker
- Text Search
- Machine Translation
- Voice to Text Search
- Voice to Speech Search",0.0987006425857544,0.09037499874830246
51aaec4c511d96ef5f5c8bae3d5d856d8bc288d3,What baselines did they compare their model with?,"bibref23, bibref6",the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search,0.8839907050132751,0.14775283634662628
51de39c8bad62d3cbfbec1deb74bd8a3ac5e69a8,Which modifications do they make to well-established Seq2seq architectures?,"in addition to encoder / decoder rnns from the original seq2seq, they also included a bottleneck prenet module","Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible",0.06435950100421906,0.2346404790878296
521a7042b6308e721a7c8046be5084bc5e8ca246,What is a confusion network or lattice?,,"graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences",0.3001393973827362,0.11429314315319061
52e8f79814736fea96fd9b642881b476243e1698,What systems are tested?,voxceleb - based systems on bulats and linguaskill,"BULATS i-vector/PLDA
BULATS x-vector/PLDA
VoxCeleb x-vector/PLDA
PLDA adaptation (X1)
 Extractor fine-tuning (X2) ",0.10971784591674805,0.42129993438720703
52f7e42fe8f27d800d1189251dfec7446f0e1d3b,How much better is performance of proposed method than state-of-the-art methods in experiments?,significantly,"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.",0.23101511597633362,0.10494906455278397
53a0763eff99a8148585ac642705637874be69d4,How does the active learning model work?,"the learning engine is essentially a classifier, which is mainly used for training of classification problems","Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.",0.012571771629154682,0.46326738595962524
53bf6238baa29a10f4ff91656c470609c16320e1,What is the source of the textual data? ,n - gram,Users' tweets,0.7747694849967957,0.16294054687023163
540e9db5595009629b2af005e3c06610e1901b12,How was a quality control performed so that the text is noisy but the annotations are accurate?,resolve ground - truth links using a wikipedia dump from april 2016,The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.,0.07342413812875748,0.7030652761459351
5455b3cdcf426f4d5fc40bc11644a432fa7a5c8f,What do they mean by answer styles?,corresponded to the two tasks,well-formed sentences vs concise answers,0.3410554528236389,0.16191929578781128
545ff2f76913866304bfacdb4cc10d31dbbd2f37,What data were they used to train the multilingual encoder?,72 million sentence pairs,WMT 2014 En-Fr parallel corpus,0.2447817474603653,0.4844779372215271
54c7fc08598b8b91a8c0399f6ab018c45e259f79,How better is performance compared to competitive baselines?,better performance than adv - c - procrustes on most language pairs,"Proposed method vs best baseline result on Vecmap (Accuracy P@1):
EN-IT: 50 vs 50
IT-EN: 42.67 vs 42.67
EN-DE: 51.6 vs 51.47
DE-EN: 47.22 vs 46.96
EN-FI: 35.88 vs 36.24
FI-EN: 39.62 vs 39.57
EN-ES: 39.47 vs 39.30
ES-EN: 36.43 vs 36.06",0.2346305400133133,0.31882232427597046
54c9147ffd57f1f7238917b013444a9743f0deb8,Which are the sequence model architectures this method can be transferred across?,"lstm - based, the cnn - based, and the transformer - based",The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models,0.5962295532226562,0.5568974614143372
54fa5196d0e6d5e84955548f4ef51bfd9b707a32,"Are this techniques used in training multilingual models, on what languages?",english - french ( en - fr ) and english - german,English to French and English to German,0.2719642221927643,0.7594910264015198
54fe8f05595f2d1d4a4fd77f4562eac519711fa6,How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?,this may be due to the text style or level of complexities of both datasets,Systems do not perform well both in Facebook and Twitter texts,0.1479058563709259,0.3519490659236908
551f77b58c48ee826d78b4bf622bb42b039eca8c,What are the weaknesses of their proposed interpretability quantification method?,precise interpretability scores that are measured here are biased by the dataset used,can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories,0.12937505543231964,0.46414199471473694
55588ae77496e7753bff18763a21ca07d9f93240,What are the characteristics of the rural dialect?,"small, scattered population",It uses particular forms of a concept rather than all of them uniformly,0.654248058795929,0.07094580680131912
564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee,Which languages do they validate on?,,"Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur",0.03303704038262367,0.15357428789138794
5712a0b1e33484ebc6d71c70ae222109c08dede2,What benchmark datasets they use?,"classical semantic parser backed by a database, and another which induces logical predicates using linear classifiers over both spatial and distributional features",VQA and GeoQA,0.07425431162118912,0.1261693239212036
572458399a45fd392c3a4e07ce26dcff2ad5a07d,How much more accurate is the model than the baseline?,,"For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. ",0.10820196568965912,0.0649459958076477
57388bf2693d71eb966d42fa58ab66d7f595e55f,How is morphology knowledge implemented in the method?,on the stem unit of each word after morpheme segmentation,A BPE model is applied to the stem after morpheme segmentation.,0.05493301898241043,0.6856384873390198
579941de2838502027716bae88e33e79e69997a6,What is difference in peformance between proposed model and state-of-the art on other question types?,performance,"For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.
For number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. 
For date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.",0.12730202078819275,0.3341403007507324
58a340c338e41002c8555202ef9adbf51ddbb7a1,Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?,,SST-2 dataset,0.17616583406925201,-0.025608446449041367
58edc6ed7d6966715022179ab63137c782105eaf,Which one of the four proposed models performed best?,lft,the hybrid model MinAvgOut + RL,0.8663390278816223,0.13475768268108368
58f50397a075f128b45c6b824edb7a955ee8cba1,How many shared layers are in the system?,1,1,0.18056991696357727,1.0
5908d7fb6c48f975c5dfc5b19bb0765581df2b25,How big is the dataset?,7934 messages,Resulting dataset was 7934 messages for train and 700 messages for test.,0.21324238181114197,0.5925534963607788
593e307d9a9d7361eba49484099c7a8147d3dade,What are causal attribution networks?,,"networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans",0.11670748889446259,0.0965314656496048
5a0841cc0628e872fe473874694f4ab9411a1d10,By how much did they outperform the other methods?,acc,"on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI",0.0511605478823185,0.21116095781326294
5a23f436a7e0c33e4842425cf86d5fd8ba78ac92,How big is dataset used?,float,"553,451 documents",0.25144246220588684,0.06819705665111542
5a29b1f9181f5809e2b0f97b4d0e00aea8996892,What makes it a more reliable metric?,takes into account the agreement between them,It takes into account the agreement between different systems,0.33248966932296753,0.7416404485702515
5a33ec23b4341584a8079db459d89a4e23420494,What is public dashboard?,leaderboard,"Public dashboard where competitors can see their results during competition, on part of the test set (public test set).",0.791729211807251,0.31261610984802246
5a9f94ae296dda06c8aec0fb389ce2f68940ea88,By how much does their method outperform the multi-head attention model?,how much does their method outperform the multi - head attention model,Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points.,0.022057034075260162,0.3484524190425873
5b2480c6533696271ae6d91f2abe1e3a25c4ae73,Is the assumption that natural language is stationary and ergodic valid?,an assumption that we follow,It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement.,0.06284258514642715,0.20874741673469543
5b6aec1b88c9832075cd343f59158078a91f3597,How does proposed word embeddings compare to Sindhi fastText word representations?,,"Proposed SG model vs SINDHI FASTTEXT:
Average cosine similarity score: 0.650 vs 0.388
Average semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391",0.04875290393829346,-0.0035120234824717045
5bcc12680cf2eda2dd13ab763c42314a26f2d993,What evaluation metrics were used in the experiment?,accuracy and mrr ( mean reciprocal ranking ),"For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy",0.5092825293540955,0.4413164556026459
5be94c7c54593144ba2ac79729d7545f27c79d37,What is the challenge for other language except English,danish,not researched as much as English,0.8854271173477173,0.20721349120140076
5c4c8e91d28935e1655a582568cc9d94149da2b2,Does DCA or GMM-based attention perform better in experiments?,dca or gmm - based attention perform better in experiments? float selected : table 3. mos naturalness results along with 95 % confidence intervals,About the same performance,0.05319802090525627,0.2391088306903839
5c90e1ed208911dbcae7e760a553e912f8c237a5,How big are the datasets?,,"In-house dataset consists of  3716 documents 
ACE05 dataset consists of  1635 documents",0.05483200028538704,0.04452846199274063
5c95808cd3ee9585f05ef573b0d4a52e86d04c60,Which journal and conference are cited the most in recent years?,1965 – 2016 ( left side ) and 2010 – 2016,CL Journal and EMNLP conference,0.03825106471776962,0.2185099720954895
5d6cc65b73f428ea2a499bcf91995ef5441f63d4,How they evaluate quality of generated output?,human evaluation,Through human evaluation where they are asked to evaluate the generated output on a likert scale.,0.09124995023012161,0.6057270169258118
5d9b088bb066750b60debfb0b9439049b5a5c0ce,what processing was done on the speeches before being parsed?,removing all numbers and interjections,Remove numbers and interjections,0.40318965911865234,0.9720438718795776
5dc1aca619323ea0d4717d1f825606b2b7c21f01,Which major geographical regions are studied?,"northeast, south, west, and midwest","Northeast U.S, South U.S., West U.S. and Midwest U.S.",0.9765774607658386,0.9021651744842529
5dfa59c116e0ceb428efd99bab19731aa3df4bbd,How many natural language explanations are human-written?,2. 0 is the combination of snli - ve - 2. 0 with explanations from either e - snli or our crowdsourced annotations where applicable. the statistics of e - snli - ve - 2. 0 are shown in table tabref40. ; float selected : table 3,Totally 6980 validation and test image-sentence pairs have been corrected.,0.044862743467092514,0.19875791668891907
5e5460ea955d8bce89526647dd7c4f19b173ab34,How many of the utterances are transcribed?,7,Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned),0.2372083216905594,0.13799446821212769
5e65bb0481f3f5826291c7cc3e30436ab4314c61,What discourse features are used?,( i ) grammatical relations ( gr ) or ( ii ) rst discourse relations ( rst ),Entity grid with grammatical relations and RST discourse relations.,0.2928084135055542,0.7188860177993774
5e9732ff8595b31f81740082333b241d0a5f7c9a,How much better were results of the proposed models than base LSTM-RNN model?,statistically significantly higher f1 values ( p < 0. 001 ),on diversity 6.87 and on relevance 4.6 points higher,0.11137302964925766,0.1506640911102295
5f7850254b723adf891930c6faced1058b99bd57,"What kind of features are used by the HMM models, and how interpretable are those?","10 lstm state dimensions and 10 hmm states in figures 3 and 3, showing which features are identified by the hmm and lstm components","A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. 
The interpretability of the model is shown in Figure 2. ",0.05159410089254379,0.6814088821411133
5f9bd99a598a4bbeb9d2ac46082bd3302e961a0f,What are the models evaluated on?,"what are the models evaluated on?. therefore, we evaluate an agent ' s performance during both its training and testing phases. during training, we report training curves averaged over 3 random seeds. during test, we follow common practice in supervised learning tasks where we report the agent ' s test performance corresponding to its best validation performance. we build the isquad and inewsqa datasets based on squad v1. 1 bibref0 and newsqa bibref1",They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA),0.06319021433591843,0.6561374664306641
5fb348b2d7b012123de93e79fd46a7182fd062bd,What datasets are used to evaluate the approach?,nell - one and wiki - one,"NELL-One, Wiki-One",0.8233290910720825,0.9868476390838623
5fb6a21d10adf4e81482bb5c1ec1787dc9de260d,How do they quantify moral relevance?,complement our morally relevant seed words with a corresponding set of seed words,By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence,0.1482558399438858,0.8346302509307861
602396d1f5a3c172e60a10c7022bcfa08fa6cbc9,By how much do they outperform BiLSTMs in Sentiment Analysis?,inlineform2,Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets.,0.15766143798828125,0.05802157148718834
61fb982b2c67541725d6db76b9c710dd169b533d,Is infinite-length sequence generation a result of training with maximum likelihood?,a decoded sequence from the trained model follows the distribution induced by a decoding algorithm,There are is a strong conjecture that it might be the reason but it is not proven.,0.04673248901963234,-0.0065369680523872375
6270d5247f788c4627be57de6cf30112560c863f,Did they experiment with tasks other than word problems in math?,did they experiment with tasks other than word problems in math?,They experimented with sentiment analysis and natural language inference task,0.10976757109165192,0.5465658903121948
63723c6b398100bba5dc21754451f503cb91c9b8,What is the state of the art?,conll 2018 shared task results,"POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)",0.3821655213832855,0.4834482669830322
63850ac98a47ae49f0f49c1c1a6e45c6c447272c,What is the problem with existing metrics that they are trying to address?,,"Answer with content missing: (whole introduction) However, recent
studies observe the limits of ROUGE and find in
some cases, it fails to reach consensus with human.
judgment (Paulus et al., 2017; Schluter, 2017).",0.06054992228746414,-0.0050966814160346985
6389d5a152151fb05aae00b53b521c117d7b5e54,What is typical GAN architecture for each text-to-image synhesis group?,each subsection will introduce several typical frameworks,"Semantic Enhancement GANs: DC-GANs, MC-GAN
Resolution Enhancement GANs: StackGANs, AttnGAN, HDGAN
Diversity Enhancement GANs: AC-GAN, TAC-GAN etc.
Motion Enhancement GAGs: T2S, T2V, StoryGAN",0.14489774405956268,0.055033110082149506
63c0128935446e26eacc7418edbd9f50cba74455,What is the size of the released dataset?,,"440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples.",0.10409842431545258,0.09132225811481476
6412e97373e8e9ae3aa20aa17abef8326dc05450,What baseline model is used?,dataset - h,Human evaluators,0.5974723696708679,0.13581474125385284
6472f9d0a385be81e0970be91795b1b97aa5a9cf,Do they train a different training method except from scheduled sampling?,since the official test set has not been released publicly,"Answer with content missing: (list missing) 
Scheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.

Yes.",0.03271310403943062,0.2981029450893402
657edbf39c500b2446edb9cca18de2912c628b7d,What was their perplexity score?,10 %,Perplexity score 142.84 on dev and 138.91 on test,0.5712712407112122,0.1336083859205246
675f28958c76623b09baa8ee3c040ff0cf277a5a,What is the size of the dataset?,"300, 000 sentences","300,000 sentences with 1.5 million single-quiz questions",0.40735316276550293,0.7052688598632812
67ee7a53aa57ce0d0bc1a20d41b64cb20303f4b7,How much training data is used?,"source - domain baseline rnn - t : approximately 120m segmented utterances ( 190, 000 hours of audio","163,110,000 utterances",0.015366092324256897,0.4948606491088867
68794289ed6078b49760dc5fdf88618290e94993,What are proof paths?,"dash - separated rectangles denote proof states ( left : substitutions, right : proof score - generating neural network",A sequence of logical statements represented in a computational graph,0.3076499402523041,0.4195789098739624
68e3f3908687505cb63b538e521756390c321a1c,What is the performance difference of using a generated summary vs. a user-written one?,outperforms all the baseline models,2.7 accuracy points,0.05227262154221535,0.3361257314682007
68ff2a14e6f0e115ef12c213cf852a35a4d73863,Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?,only 50 percent,The dataset contains about 590 tweets about DDos attacks.,0.23228247463703156,0.13571186363697052
6a9eb407be6a459dc976ffeae17bdd8f71c8791c,What is the reward model for the reinforcement learning appraoch?,"1 for successfully completing the task, and 0 otherwise","reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail",0.18603970110416412,0.7097521424293518
6b91fe29175be8cd8f22abf27fb3460e43b9889a,what genres do they songs fall under?,14,"Gospel, Sertanejo, MPB, Forró, Pagode, Rock, Samba, Pop, Axé, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda",0.2966451644897461,0.024290915578603745
6baf5d7739758bdd79326ce8f50731c785029802,Which four languages do they experiment with?,ser,"German, English, Italian, Chinese",0.055813275277614594,0.13885189592838287
6ce057d3b88addf97a30cb188795806239491154,What models are included in baseline benchmarking results?,pretrained bert model,"BERT, XLNET RoBERTa, ALBERT, DistilBERT",0.20980411767959595,0.4939064085483551
6dcbe941a3b0d5193f950acbdc574f1cfb007845,What are the domains covered in the dataset?,17,"Alarm
Bank
Bus
Calendar
Event
Flight
Home
Hotel
Media
Movie
Music
RentalCar
Restaurant
RideShare
Service
Travel
Weather",0.4753710627555847,0.07222118973731995
6e8c587b6562fafb43a7823637b84cd01487059a,How much is the BLEU score?,##k and sequence length,Ranges from 44.22 to 100.00 depending on K and the sequence length.,0.04277745261788368,0.4397467076778412
6e97c06f998f09256be752fa75c24ba853b0db24,How do the authors measure performance?,"“ w / ” represents “ with ”, lines marked with “ * ” are experiments results from kobayashi ( kobayashi, 2018 )",Accuracy across six datasets,0.03489569574594498,0.0005358215421438217
6ea63327ffbab2fc734dd5c2414e59d3acc56ea5,How large is the gap in performance between the HMMs and the LSTMs?,,"With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.",0.010744587518274784,-0.04499754309654236
6f2118a0c64d5d2f49eee004d35b956cb330a10e,What datasets are used for training/testing models? ,msr ' s dialogue management code and knowledge base,"Microsoft Research dataset containing movie, taxi and restaurant domains.",0.29133662581443787,0.18975432217121124
6f2f304ef292d8bcd521936f93afeec917cbe28a,How much improvement is gained from the proposed approaches?,,It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.,0.03690304607152939,0.026285801082849503
707db46938d16647bf4b6407b2da84b5c7ab4a81,How much F1 was improved after adding skip connections?,how much f1 was improved after adding skip connections? table tabref20 reports the f1 and em scores obtained for the experiments on the base model. the first column reports the base bert,"Simple Skip improves F1 from 74.34 to 74.81
Transformer Skip improes F1 from 74.34 to 74.95 ",0.006339893210679293,0.4663099944591522
7182f6ed12fa990835317c57ad1ff486282594ee,How does the SCAN dataset evaluate compositional generalization?,"in a sequence - to - sequence ( seq2seq ) setting by systematically holding out of the training set all inputs containing a basic primitive verb ( "" jump "" ), and testing on sequences containing that verb","it systematically holds out inputs in the training set containing basic primitive verb, ""jump"", and tests on sequences containing that verb.",0.15150214731693268,0.7743220329284668
71d59c36225b5ee80af11d3568bdad7425f17b0c,How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?,,Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ,0.033506520092487335,0.12442844361066818
728a55c0f628f2133306b6bd88af00eb54017b12,What geometric properties do embeddings display?,"words were grouped by context or influence on the electricity consumption. for instance, we observed that winter words were together and far away from summer ones",Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.,0.06433897465467453,0.5894219875335693
7348e781b2c3755b33df33f4f0cab4b94fcbeb9b,"Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",statistical and language analysis tools,Only automatic methods,0.10533778369426727,0.2587156891822815
7358a1ce2eae380af423d4feeaa67d2bd23ae9dd,How do their train their embeddings?,using the embeddings train set,"The embeddings are learned several times using the training set, then the average is taken.",0.3898281753063202,0.6062471866607666
73633afbefa191b36cca594977204c6511f9dad4,"Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?","we plan to apply semantic slot scaffold to news summarization. specifically, we can annotate the critical entities such as person names or location names to ensure that they are captured correctly in the generated summary. we also plan to collect a human - human dialog dataset with more diverse human - written summaries.","Not at the moment, but summaries can be additionaly extended with this annotations.",0.10772653669118881,0.5269067287445068
737397f66751624bcf4ef891a10b29cfc46b0520,Which datasets are used in the paper?,which datasets are used in the paper?,"Google N-grams
COHA
Moral Foundations Dictionary (MFD)
",0.0744166225194931,0.22638382017612457
73bb8b7d7e98ccb88bb19ecd2215d91dd212f50d,What human evaluation method is proposed?,simple,comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant,0.38453128933906555,0.0588858388364315
73bbe0b6457423f08d9297a0951381098bd89a2b,what were the baselines?,"pre - identified predicates as inputs, while our model puts syntax aside and jointly learns and predicts predicates and arguments. ; float selected : table 1 : a chronicle of related work for span and dependency srl. sa represents syntax - aware system ( no + indicates syntaxagnostic system ) and st indicates sequence tagging model. f1 is the result of single model on official test set","2008 Punyakanok et al. 
2009 Zhao et al. + ME 
2008 Toutanova et al. 
2010 Bjorkelund et al.  
2015 FitzGerald et al. 
2015 Zhou and Xu 
2016 Roth and Lapata 
2017 He et al. 
2017 Marcheggiani et al.
2017 Marcheggiani and Titov 
2018 Tan et al. 
2018 He et al. 
2018 Strubell et al. 
2018 Cai et al. 
2018 He et al. 
2018 Li et al. 
",0.06686417013406754,0.12209035456180573
74091e10f596428135b0ab06008608e09c051565,How is knowledge stored in the memory?,a question on the document represented as another sequence of words and an answer to the question,entity memory and relational memory.,0.04939081147313118,0.23377490043640137
74261f410882551491657d76db1f0f2798ac680f,What are the six target languages?,"bibref56, bibref57, bibref17","Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI).",0.1721091866493225,0.14935609698295593
74cd51a5528c6c8e0b634f3ad7a9ce366dfa5706,What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?,,it is less expensive and quantifies interpretability using continuous values rather than binary evaluations,0.08127086609601974,0.02051548659801483
74db8301d42c7e7936eb09b2171cd857744c52eb,How is the performance on the task evaluated?,,Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors,0.018259471282362938,0.03358779847621918
753990d0b621d390ed58f20c4d9e4f065f0dc672,What is the seed lexicon?,positive and negative predicates,a vocabulary of positive and negative predicates that helps determine the polarity score of an event,0.8550531268119812,0.67779541015625
75b69eef4a38ec16df63d60be9708a3c44a79c56,How much better peformance is achieved in human evaluation when model is trained considering proposed metric?,"how much better peformance is achieved in human evaluation when model is trained considering proposed metric?. we can see that the proposed comparative evaluator with skill rating significantly outperforms all compared baselines, including comparative evaluator with averaged sample - level scores. this demonstrates the effectiveness of the skill rating system for performing model - level comparison with pairwise sample - level evaluation. in addition, the poor correlation between conventional evaluation metrics including bleu and perplexity demonstrates the necessity of better automated evaluation metrics in open domain nlg evaluation. ; float selected : table 1 : sample - level correlation between metrics and human judgments, with p - values shown in brackets. ; float selected : table 2 : model - level correlation between metrics and human judgments, with p - values shown in brackets. the experimental results are summarized in table 1. we can see that the proposed comparative evaluator correlates far better with human judgment than bleu and perplexity. when compared with recently proposed parameterized metrics including adversarial evaluator and adem, our model consistently outperforms them by a large margin","Pearson correlation to human judgement - proposed vs next best metric
Sample level comparison:
- Story generation: 0.387 vs 0.148
- Dialogue: 0.472 vs 0.341
Model level comparison:
- Story generation:  0.631 vs 0.302
- Dialogue: 0.783 vs 0.553",0.06584623456001282,0.45093441009521484
76377e5bb7d0a374b0aefc54697ac9cd89d2eba8,How do they obtain word lattices from words?,directed graph inlineform0,By considering words as vertices and generating directed edges between neighboring words within a sentence,0.12868599593639374,0.3116055428981781
78577fd1c09c0766f6e7d625196adcc72ddc8438,What dataset is used for train/test of this method?,training dataset ; ( i ) tts system dataset,Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset.,0.37664729356765747,0.6878007650375366
785eb3c7c5a5c27db14006ac357299ed1216313a,What they formulate the question generation as?,optimization problem,LASSO optimization problem,0.6306357979774475,0.6516257524490356
78c010db6413202b4063dc3fb6e3cc59ec16e7e3,What is different in the improved annotation protocol?,"removes redundant roles by picking the more naturally phrased questions. for example, in table tabref4 ex. 1, one worker could have chosen “ 47 people ”, while another chose “ the councillor ” ; in this case the consolidator would include both of those answers. in section secref4, we show that this process yields better coverage",a trained worker consolidates existing annotations ,0.06140695884823799,0.3464171886444092
7920f228de6ef4c685f478bac4c7776443f19f39,What language is the Twitter content in?,classical music,English,0.05895530804991722,0.2683202028274536
7994b4001925798dfb381f9aa5c0545cdbd77220,How do they perform data augmentation?,berkeley neural parser bibref19,They randomly sample sentences from Wikipedia that contains an object RC and add them to training data,0.04521874338388443,0.3410545587539673
7997b9971f864a504014110a708f215c84815941,"What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?",,"Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text",0.10742590576410294,0.07895972579717636
79a28839fee776d2fed01e4ac39f6fedd6c6a143,What is the main contribution of the paper? ,utilization of phonetic information to improve neural lid,"Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance",0.16682250797748566,0.8113973140716553
79bb1a1b71a1149e33e8b51ffdb83124c18f3e9c,What accuracy does CNN model achieve?,class - wise average,Combined per-pixel accuracy for character line segments is 74.79,0.03130076453089714,0.150895357131958
79ed71a3505cf6f5e8bf121fd7ec1518cab55cae,How do they damage different neural modules?,randomly initializing their weights,"Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.",0.5487749576568604,0.3985375761985779
7a53668cf2da4557735aec0ecf5f29868584ebcf,What kind of instructional videos are in the dataset?,screencast,tutorial videos for a photo-editing software,0.38260987401008606,0.2925576865673065
7a7e279170e7a2f3bc953c37ee393de8ea7bd82f,What two types the Chinese reading comprehension dataset consists of?,training and test set,cloze-style reading comprehension and user query reading comprehension questions,0.04397683963179588,0.10153757035732269
7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed,Is ROUGE their only baseline?,our first baseline is rouge - l bibref1,"No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.",0.2732422351837158,0.44014567136764526
7af01e2580c332e2b5e8094908df4e43a29c8792,How was lexical diversity measured?,response density,By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions,0.17014600336551666,0.33942481875419617
7b2bf0c1a24a2aa01d49f3c7e1bdc7401162c116,How are the keywords associated with events such as protests selected?,"bayesian approach to learn which terms were associated with events, regardless of whether they are standard language, acronyms, or even a made - up word, so long as they match the events of interest. the third and fourth challenges are approached by using word - pairs, where we extract all the pairs of co - occurring words within each tweet. this allows us to recognize the context of the word ( ' messi ', ' strike ' ) is different than ( ' labour ', ' strike ' ). ; according to the distributional semantic hypothesis, event - related words are likely to be used on the day of an event more frequently than any normal day before or after the event. this will form a spike in the keyword count magnitude along the timeline as illustrated in figure figref6. to find the words most associated with events, we search for the words that achieve the highest number of spikes matching the days of events","By using a Bayesian approach  and by using word-pairs, where they extract all the pairs of co-occurring words within each tweet.  They search for the words that achieve the highest number of spikes matching the days of events.",0.061413854360580444,0.75355464220047
7b44bee49b7cb39cb7d5eec79af5773178c27d4d,How is the data in RAFAEL labelled?,selection of the most probable interpretation,"Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner",0.7475070953369141,0.13122153282165527
7b89515d731d04dd5cbfe9c2ace2eb905c119cbc,Which is the baseline model?,i - vector model,"The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. ",0.6980308294296265,0.28182452917099
7cf726db952c12b1534cd6c29d8e7dfa78215f9e,What is a word confusion network?,cnets,It is a network used to encode speech lattices to maintain a rich hypothesis space.,0.7829583883285522,0.2529802918434143
7d3c036ec514d9c09c612a214498fc99bf163752,What is the source of the dataset?,news articles,"Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera",0.17812637984752655,0.3534611165523529
7d59374d9301a0c09ea5d023a22ceb6ce07fb490,How do they measure the diversity of inferences?,the number of distinct n - gram,by number of distinct n-grams,0.4122779965400696,0.8298078775405884
7ee29d657ccb8eb9d5ec64d4afc3ca8b5f3bcc9f,What were the results of the first experiment?,we reach an f - score ( 0. 72 ) only slightly lower than the one reported in bibref3 ( 0. 74 ),Best performance achieved is 0.72 F1 score,0.12428641319274902,0.536471426486969
7f9bc06cfa81a4e3f7df4c69a1afef146ed5a1cf,What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?,pearson - spearman correlation,"10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%
50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%",0.04309919476509094,0.6312068104743958
8051927f914d730dfc61b2dc7a8580707b462e56,What baseline algorithms were presented?,,"a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm",0.07520559430122375,0.12804822623729706
81064bbd0a0d72a82d8677c32fb71b06501830a0,By how much is precission increased?,figure 3 : rouge quality of produced summaries in term of precision,"ROUGE-1 increases by 0.05, ROUGE-2 by 0.06 and ROUGE-L by 0.09",0.07674261927604675,0.3986528515815735
81cee2fc6edd9b7bc65bbf6b4aa35782339e6cff,What type of documents are supported by the annotation platform?,pdfs and microsoft word,"Variety of formats supported (PDF, Word...), user can define content elements of document",0.7384108901023865,0.6680821180343628
81e8d42dad08a58fe27eea838f060ec8f314465e,What is the state-of-the art?,"what is the state - of - the art?. our encoder first reads the text, and the results from the first read help represent the text in the second pass over the source text. our second contribution is a simple copy mechanism that allows us to significantly reduce the decoder vocabulary size resulting in much faster inference times. furthermore our copy mechanism allows us to handle out - of - vocabulary words in a principled manner. finally our experiments show state - of - the - art performance on the duc competition. very recently, the success of deep neural networks in many natural language processing tasks ( bibref20 ) has inspired new work in abstractive summarization. bibref2 propose a neural attention model with a convolutional encoder to solve this task. bibref3 build a large dataset for chinese text summarization and propose to feed all hidden states from the encoder into the decoder. more recently, bibref4 extended bibref2 ' s work with an rnn decoder, and bibref8 proposed an rnn encoder - decoder architecture for summarization",neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder,0.03472364693880081,0.5996658802032471
8255f74cae1352e5acb2144fb857758dda69be02,How do they measure grammaticality?,log ratio,by calculating log ratio of grammatical phrase over ungrammatical phrase,0.8410887718200684,0.60010826587677
82a28c1ed7988513d5984f6dcacecb7e90f64792,How big are negative effects of proposed techniques on high-resource tasks?,"compared to single - task baselines, performance improved on the low - resource en - de task and was comparable on high - resource en - fr task.",The negative effects were insignificant.,0.13160160183906555,0.08379168063402176
83f14af3ccca4ab9deb4c6d208f624d1e79dc7eb,Which of the two ensembles yields the best performance?,e - bert,Answer with content missing: (Table 2) CONCAT ensemble,0.2663402557373047,0.06784844398498535
8427988488b5ecdbe4b57b3813b3f981b07f53a5,On which task does do model do best?,joint task,Variety prediction task,0.1458713263273239,0.25067585706710815
8434974090491a3c00eed4f22a878f0b70970713,How big is their model?,model size of each model on the restaurant dataset,Proposed model has 1.16 million parameters and 11.04 MB.,0.09046453982591629,0.406055212020874
8568c82078495ab421ecbae38ddd692c867eac09,How many layers of self-attention does the model have?,6,"1, 4, 8, 16, 32, 64",0.8939271569252014,0.47633451223373413
858c51842fc3c1f3e6d2d7d853c94f6de27afade,Which of the classifiers showed the best performance?,classification avcs using word - pairs,Logistic regression,0.04270210489630699,0.12842833995819092
85912b87b16b45cde79039447a70bd1f6f1f8361,How large is the corpus they use?,float selected : table 1 : experimental conditions.,449050,0.058519404381513596,0.0750482901930809
85e45b37408bb353c6068ba62c18e516d4f67fe9,What is the baseline?,multi - task architecture,The baseline is a multi-task architecture inspired by another paper.,0.4484485983848572,0.7057497501373291
8602160e98e4b2c9c702440da395df5261f55b1f,What are the three datasets used in the paper?,,Data released for APDA shared task contains 3 datasets.,0.07115930318832397,-0.010397011414170265
863d5c6305e5bb4b14882b85b6216fa11bcbf053,What are the 12 AV approaches which are examined?,,"MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD",0.026600824669003487,0.14795459806919098
86cd1228374721db67c0653f2052b1ada6009641,What domain does the dataset fall into?,"training, validation, and testing",YouTube videos,0.4242578446865082,0.2717384696006775
880a76678e92970791f7c1aad301b5adfc41704f,What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?,sentence classification,"ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.
NLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing ""retweets"", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation.",0.8180510997772217,0.4210216701030731
894c086a2cbfe64aa094c1edabbb1932a3d7c38a,What are the state-of-the-art systems?,,"For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN",0.07059243321418762,0.060827042907476425
8951fde01b1643fcb4b91e51f84e074ce3b69743,How they evaluate their approach?,"in several low - resource settings across different languages with real, distantly supervised data with non - synthetic noise","They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise",0.15765981376171112,0.9492777585983276
8958465d1eaf81c8b781ba4d764a4f5329f026aa,What are the three measures of bias which are reduced in experiments?,"[ - | | w | |, | | w | | ] k - 2 x y a b v b, and,,, and are groups of words for which the association is measured","RIPA, Neighborhood Metric, WEAT",0.027252165600657463,0.20085234940052032
8985ead714236458a7496075bc15054df0e3234e,What is the performance of the models on the tasks?,all models perform well below estimated human agreement,"Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)",0.41266146302223206,0.37403780221939087
89d1687270654979c53d0d0e6a845cdc89414c67,How do they obtain human judgements?,crowdsourced,Using crowdsourcing ,0.9246926307678223,0.8618167638778687
8a0a51382d186e8d92bf7e78277a1d48958758da,How better is gCAS approach compared to other approaches?,entity f1 and success f1,"For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52",0.032177623361349106,0.4768679141998291
8a1d4ed00d31c1f1cb05bc9d5e4f05fe87b0e5a4,Who manually annotated the semantic roles for the set of learner texts?,we manually annotate the predicate – argument structures for the 600 l2 - l1 pairs,Authors,0.24114415049552917,0.1286773383617401
8a5254ca726a2914214a4c0b6b42811a007ecfc6,How much transcribed data is available for for Ainu language?,corpus,Transcribed data is available for duration of 38h 54m 38s for 8 speakers.,0.018616218119859695,0.3716473877429962
8a871b136ccef78391922377f89491c923a77730,What are the baseline state of the art models?,table 3 : evaluation results of different approaches compared to ours,"Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention",0.10687524080276489,0.23272860050201416
8ad815b29cc32c1861b77de938c7269c9259a064,What languages are represented in the dataset?,54,"EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO",0.2684359848499298,0.09198126196861267
8c0a0747a970f6ea607ff9b18cfeb738502d9a95,What was the performance of both approaches on their dataset?,%,ERR of 19.05 with i-vectors and 15.52 with x-vectors,0.2164781093597412,0.044013552367687225
8c8a32592184c88f61fac1eef12c7d233dbec9dc,Are this models usually semi/supervised or unsupervised?,,"Both supervised and unsupervised, depending on the task that needs to be solved.",0.1092485710978508,0.04912120848894119
8cc56fc44136498471754186cfa04056017b4e54,By how much does their system outperform the lexicon-based models?,,"Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . 
Under the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029",0.05571312829852104,0.04644162207841873
8d793bda51a53a4605c1c33e7fd20ba35581a518,what bottlenecks were identified?,,Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System.,0.08037784695625305,0.0915507897734642
8e2b125426d1220691cceaeaf1875f76a6049cbd,By how much do they improve the accuracy of inferences over state-of-the-art methods?,,"ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.
On Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.",0.06646446138620377,-0.00017115846276283264
8e9de181fa7d96df9686d0eb2a5c43841e6400fa,"Is CRWIZ already used for data collection, what are the results?",145 unique dialogues were collected ( each dialogue consists of a conversation between two participants ). all the dialogues were manually checked by one of the authors and those where the workers were clearly not partaking in the task or collaborating were removed from the dataset,"Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.",0.062363866716623306,0.560685396194458
8ea4bd4c1d8a466da386d16e4844ea932c44a412,What dataset do they use?,18805,A parallel corpus where the source is an English expression of code and the target is Python code.,0.46581944823265076,0.06451933085918427
8f87215f4709ee1eb9ddcc7900c6c054c970160b,how is quality measured?,accuracy and the macro - f1 ( averaged f1 over positive and negative classes ),Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.,0.0925336629152298,0.7974168062210083
90159e143487505ddc026f879ecd864b7f4f479e,How much of the ASR grapheme set is shared between languages?,432 characters,Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script.,0.29517748951911926,0.10876800864934921
90bc60320584ebba11af980ed92a309f0c1b5507,How do they enrich the positional embedding with length information,"length encoding method ; inspired by bibref11, we use length encoding to provide the network with information about the remaining sentence length during decoding",They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).,0.028529400005936623,0.3076452314853668
9299fe72f19c1974564ea60278e03a423eb335dc,What was the weakness in Hassan et al's evaluation design?,"what was the weakness in hassan et al ' s evaluation design?. moreover, different human translations of the same source text sometimes show considerable differences in quality, and a comparison with an mt system only makes sense if the human reference translations are of high quality. bibref3, for example, had the wmt source texts re - translated as they were not convinced of the quality of the human translations in the test set. at wmt 2018, the organisers themselves noted that the manual evaluation included several reports of ill - formed reference translations bibref5. we hypothesise that the quality of the human translations has a significant effect on findings of human – machine parity, which would indicate that it is necessary to ensure that human translations used to assess parity claims need to be carefully vetted for their quality.. we hypothesise that expert translators will provide more nuanced ratings than non - experts, and that their ratings will show a higher difference between mt outputs and human translations. ; mt has been evaluated almost exclusively at the sentence level, owing to the fact that most mt systems do not yet take context across sentence boundaries into account. however, when machine translations are compared to those of professional translators, the omission of linguistic context — e. g., by random ordering of the sentences to be evaluated — does not do justice to humans who, in contrast to most mt systems, can and do take inter - sentential context into account bibref15, bibref16. we hypothesise that an evaluation of sentences in isolation, as applied by bibref3, precludes raters from detecting translation errors that become apparent only when inter - sentential context is available, and that they will judge mt quality less favourably when evaluating full documents. ; the human reference translations with which machine translations are compared within the scope of a human – machine parity assessment play an important role. bibref3 used all source texts of the wmt 2017 chinese – english test set for their experiments, of which only half were originally written in chinese ; the other half were translated from english into chinese. since translated texts are usually simpler than their original counterparts bibref17, they should be easier to translate for mt systems the human evaluation of mt output in research scenarios is typically conducted by crowd workers in order to minimise costs. bibref13 shows that aggregated assessments of bilingual crowd workers are very similar to those of mt developers, and bibref14, based on experiments with data from wmt","MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set
",0.10620537400245667,0.6554049253463745
9349acbfce95cb5d6b4d09ac626b55a9cb90e55e,What are the citation intent labels in the datasets?,citation intent labels,"Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.",0.1366724669933319,0.23998194932937622
93b299acfb6fad104b9ebf4d0585d42de4047051,Which datasets are used?,"english, spanish, french, dutch, russian and turkish. from left to right each row displays the number of tokens, number of targets and the number of multiword targets for each training and test set. for english, it should be noted that the size of the 2015 set is less than half with respect to the 2014 dataset in terms of tokens, and only one third in number of targets. the french, spanish and dutch datasets are quite similar in terms of tokens although the number of targets in the dutch dataset is comparatively smaller, possibly due to the tendency to construct compound terms in that language. the russian dataset is the largest whereas the turkish set is by far the smallest one. ; float selected : table 1 : absa semeval 2014 - 2016 datasets for the restaurant domain. for the rest of the languages we used their corresponding wikipedia dumps","ABSA SemEval 2014-2016 datasets
Yelp Academic Dataset
Wikipedia dumps",0.11686030775308609,0.5854166746139526
9447ec36e397853c04dcb8f67492ca9f944dbd4b,What is the dataset used as input to the Word2Vec algorithm?,2. 6 gb of raw text,Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words,0.1624600887298584,0.1921495497226715
944d5dbe0cfc64bf41ea36c11b1d378c408d40b8,Which of the two speech recognition models works better overall on CN-Celeb?,i - vector and x - vector,x-vector,0.5919244289398193,0.8159942626953125
94bee0c58976b58b4fef9e0adf6856fe917232e5,How much bigger is Switchboard-2000 than Switchboard-300 database?,only 50 epochs,Switchboard-2000 contains 1700 more hours of speech data.,0.08609193563461304,0.1406581550836563
94e0cf44345800ef46a8c7d52902f074a1139e1a,What web and user-generated NER datasets are used for the analysis?,social media,"MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC",0.13036900758743286,0.07596656680107117
954c4756e293fd5c26dc50dc74f505cc94b3f8cc,What are dilated convolutions?,skip some input values,Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.,0.43544521927833557,0.3489258885383606
9555aa8de322396a16a07a5423e6a79dcd76816a,By how much does their model outperform both the state-of-the-art systems?,69k or 200k,w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%,0.14452582597732544,0.08971784263849258
955ca31999309685c1daa5cb03867971ca99ec52,What datasets are used to evaluate the model?,wn18 and fb15k,"WN18, FB15k",0.8734975457191467,0.9366908669471741
957bda6b421ef7d2839c3cec083404ac77721f14,What stylistic features are used to detect drunk texts?,table 1,"LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) 
 and Sentiment Ratio",0.6495864987373352,0.14764225482940674
96be67b1729c3a91ddf0ec7d6a80f2aa75e30a30,What language do the agents talk in?,,English,0.10610659420490265,0.28494974970817566
96c09ece36a992762860cde4c110f1653c110d96,What was the result of the highest performing system?,,"For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.
For task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2",0.08355248719453812,-0.01324462704360485
973f6284664675654cc9881745880a0e88f3280e,What proficiency indicators are used to the score the utterances?,6,"6 indicators:
- lexical richness
- pronunciation and fluency
- syntactical correctness
- fulfillment of delivery
- coherence and cohesion
- communicative, descriptive, narrative skills",0.03790372610092163,0.19283431768417358
9776156fc93daa36f4613df591e2b49827d25ad2,"By how much, the proposed method improves BiDAF and DCN on SQuAD dataset?",table 1 : effect of character embedding,"In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively.",0.04371459409594536,0.01860112138092518
98515bd97e4fae6bfce2d164659cd75e87a9fc89,What is the source of the user interaction data? ,ego - network,Sociability from ego-network on Twitter,0.36069706082344055,0.6695438027381897
98785bf06e60fcf0a6fe8921edab6190d0c2cec1,How do they determine which words are informative?,,Informative are those that will not be suppressed by regularization performed.,0.06007302552461624,0.11102758347988129
98eb245c727c0bd050d7686d133fa7cd9d25a0fb,Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?,,BLEU scores,0.047610312700271606,0.14647558331489563
993b896771c31f3478f28112a7335e7be9d03f21,What novel class of recurrent-like networks is proposed?,,"A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state.",0.03854719176888466,0.04579886421561241
999b20dc14cb3d389d9e3ba5466bc3869d2d6190,What is the latest paper covered by this survey?,table 2 : existing nqg models with their best - reported performance on squad,Kim et al. (2019),0.036922164261341095,0.07517948001623154
99c50d51a428db09edaca0d07f4dab0503af1b94,What kind of Youtube video transcripts did they use?,news,"youtube video transcripts on news covering different topics like technology, human rights, terrorism and politics",0.20360000431537628,0.29916948080062866
9a596bd3a1b504601d49c2bec92d1592d7635042,What is the performance of their model?,competitive advantage in dealing with drug - drug interaction extraction,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220.,0.17958910763263702,0.0701739564538002
9a65cfff4d99e4f9546c72dece2520cae6231810,What is the performance of proposed model on entire DROP dataset?,the best overall,"The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev",0.22428691387176514,0.12841396033763885
9aa52b898d029af615b95b18b79078e9bed3d766,How faster is training and decoding compared to former models?,"our model is the fastest, followed by biaf","Proposed vs best baseline:
Decoding: 8541 vs 8532 tokens/sec
Training: 8h vs 8h",0.20013152062892914,0.30728670954704285
9ab43f941c11a4b09a0e4aea61b4a5b4612e7933,What approach did previous models use for multi-span questions?,to train a dedicated categorical variable to predict the number of spans to extract,Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span,0.24602651596069336,0.614985466003418
9adcc8c4a10fa0d58f235b740d8d495ee622d596,How many additional task-specific layers are introduced?,1,2 for the ADE dataset and 3 for the CoNLL04 dataset,0.14050491154193878,0.21863773465156555
9ae084e76095194135cd602b2cdb5fb53f2935c1,What metrics are used for evaluation?,cross - domain scenarios,word error rate,0.09805890917778015,-0.008056141436100006
9bcc1df7ad103c7a21d69761c452ad3cd2951bda,On which task does do model do worst?,,Gender prediction task,0.08788876235485077,0.12649285793304443
9c4a4dfa7b0b977173e76e2d2f08fa984af86f0e,How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?,outperforms all existing models on both the full test set and the cleaned test set,"Full Testing Set accuracy: 84.02
Cleaned Testing Set accuracy: 93.48",0.11572325974702835,0.5967322587966919
9c68d6d5451395199ca08757157fbfea27f00f69,Which OpenIE systems were used?,openie 4 bibref5,OpenIE4 and MiniIE,0.6795554757118225,0.6107865571975708
9d578ddccc27dd849244d632dd0f6bf27348ad81,What are the results?,performance of various models on the acp test set. ; float selected : table 4 : results for small labeled training data,"Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. 
Using a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.",0.059773072600364685,0.5707038640975952
9d5df9022cc9eb04b9f5c5a9d8308a332ebdf50c,What is the new labeling strategy?,two - stage,They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations,0.1913956254720688,0.28088366985321045
9d9b11f86a96c6d3dd862453bf240d6e018e75af,How does counterfactual data augmentation aim to tackle bias?,,The training dataset is augmented by swapping all gendered words by their other gender counterparts,0.13427706062793732,0.08975470066070557
9e04730907ad728d62049f49ac828acb4e0a1a2a,What were their performance results?,acc,"On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%",0.04224037006497383,0.23366177082061768
9ebb2adf92a0f8db99efddcade02a20a219ca7d9,How is the proficiency score calculated?,according to the proficiency levels and the type of test,"They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.",0.09502633661031723,0.6365551948547363
9ef182b61461d0d8b6feb1d6174796ccde290a15,Do they annotate their own dataset or use an existing one?,select a dataset whose utterances have been correctly synchronised at recording time,Use an existing one,0.015490462072193623,-0.03099816106259823
9fe4a2a5b9e5cf29310ab428922cc8e7b2fc1d11,What are state of the art methods MMM is compared to?,baselines,"FTLM++, BERT-large, XLNet",0.6898671388626099,0.10846436768770218
a02696d4ab728ddd591f84a352df9375faf7d1b4,How large is the Dialog State Tracking Dataset?,task 6,"1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs",0.1834542602300644,0.2838703989982605
a09633584df1e4b9577876f35e38b37fdd83fa63,"How is human evaluation performed, what was the criteria?",plausibility and content richness,Through Amazon MTurk annotators to determine plausibility and content richness of the response,0.5429409742355347,0.473515123128891
a1064307a19cd7add32163a70b6623278a557946,How many uniue words are in the dataset?,,908456 unique words are available in collected corpus.,0.04534607753157616,0.07313452661037445
a1557ec0f3deb1e4cd1e68f4880dcecda55656dd,Which geographical regions correlate to the trend?,"northeast, west and south regions","Northeast U.S., West U.S. and South U.S.",0.4703449308872223,0.8379498720169067
a24a7a460fd5e60d71a7e787401c68caa4702df6,What monolingual word representations are used?,,"AraVec for Arabic, FastText for French, and Word2vec Google News for English.",0.04917863756418228,0.0956755056977272
a25c1883f0a99d2b6471fed48c5121baccbbae82,What performance does the Entity-GCN get on WIKIHOP?,outperforms recent prior work,"During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models",0.5064820051193237,0.24016626179218292
a2be2bd84e5ae85de2ab9968147b3d49c84dfb7f,What genres are covered?,genres,"genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement",0.2614894509315491,0.5240694880485535
a379c380ac9f67f824506951444c873713405eed,What are the baselines?,valid and test datasets,"CNN, LSTM, BERT",0.11055461317300797,0.1855269819498062
a381ba83a08148ce0324b48b8ff35128e66f580a,what models did they compare to?,"tree - lstm bibref21, bibref22 and the high - order cnn bibref16","High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM ",0.6528263092041016,0.6366938352584839
a3d83c2a1b98060d609e7ff63e00112d36ce2607,How many sentence transformations on average are available per unique sentence in dataset?,4262,27.41 transformation on average of single seed sentence is available in dataset.,0.3726027309894562,0.08339273929595947
a3f108f60143d13fe38d911b1cc3b17bdffde3bd,what was their system's f1 performance?,12 f1 points,"Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.",0.31247758865356445,0.4906642436981201
a48c6d968707bd79469527493a72bfb4ef217007,Which training dataset allowed for the best generalization to benchmark sets?,baseline results,MultiNLI,0.13275274634361267,0.08799776434898376
a4d8fdcaa8adf99bdd1d7224f1a85c610659a9d3,"When they say ""comparable performance"", how much of a performance drop do these new embeddings result in?",significantly improves interpretability,"Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.",0.16717882454395294,0.16292831301689148
a4fe5d182ddee24e5bbf222d6d6996b3925060c8,which datasets were used in evaluation?,table 1,"CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0",0.3651036322116852,0.19694536924362183
a4ff1b91643e0c8a0d4cc1502d25ca85995cf428,Which two datasets does the resource come from?,two different surveys,two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor,0.3928181827068329,0.3681817352771759
a516b37ad9d977cb9d4da3897f942c1c494405fe,Which models do they try out?,human,"DocQA, SAN, QANet, ASReader, LM, Random Guess",0.12098074704408646,0.08681812882423401
a56fbe90d5d349336f94ef034ba0d46450525d19,What DCGs are used?,1st and 3rd steps in the three - steps conversion,Author's own DCG rules are defined from scratch.,0.2918475568294525,0.1715897023677826
a5b67470a1c4779877f0d8b7724879bbb0a3b313,what metrics are used in evaluation?,micro - averaged inlineform0,micro-averaged F1,0.8991430401802063,0.6227043867111206
a71ebd8dc907d470f6bd3829fa949b15b29a0631,how did they ask if a tweet was racist?,social media,"if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.",0.3862082064151764,0.13723450899124146
a7510ec34eaec2c7ac2869962b69cc41031221e5,What was their F1 score on the Bengali NER corpus?,over 5 points,52.0%,0.3249426484107971,0.2967539131641388
a7adb63db5066d39fdf2882d8a7ffefbb6b622f0,what was the baseline?,default values,There is no baseline.,0.027276530861854553,0.24003618955612183
a81941f933907e4eb848f8aa896c78c1157bff20,"Can the model add new relations to the knowledge graph, or just new entities?",kg to resolve a ranked list of target entities,The model does not add new relations to the knowledge graph.,0.014332711696624756,0.1675228476524353
a891039441e008f1fd0a227dbed003f76c140737,What MC abbreviate for?,"bibref0, bibref1, bibref2, bibref3, bibref4, bibref5",machine comprehension,0.08288024365901947,0.09338979423046112
a978a1ee73547ff3a80c66e6db3e6c3d3b6512f4,How much improvement does their method get over the fine tuning baseline?,domain adaptation,"0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.",0.0486636608839035,-0.005108291283249855
a979749e59e6e300a453d8a8b1627f97101799de,Why does the model improve in monolingual spaces as well? ,"the reasons behind the higher quality displayed by our bilingual embeddings in monolingual settings. while vecmap and muse do not transform the initial monolingual spaces, our model transforms both spaces simultaneously",because word pair similarity increases if the two words translate to similar parts of the cross-lingual embedding space,0.05789487063884735,0.6114333868026733
a996b6aee9be88a3db3f4127f9f77a18ed10caba,What's the precision of the system?,overall typing performance,"0.8320 on semantic typing, 0.7194 on entity matching",0.4255948066711426,0.3625933527946472
aa54e12ff71c25b7cff1e44783d07806e89f8e54,What is an example of a health-related tweet?,convolutional autoencoder,"The health benefits of alcohol consumption are more limited than previously thought, researchers say",0.13915212452411652,-0.06078027933835983
aacb0b97aed6fc6a8b471b8c2e5c4ddb60988bf5,How many attention layers are there in their model?,254,one,0.09516607969999313,0.35304808616638184
aaed6e30cf16727df0075b364873df2a4ec7605b,What is WNGT 2019 shared task?,bibref0,efficiency task aimed  at reducing the number of parameters while minimizing drop in performance,0.713862419128418,0.05750729888677597
ab9453fa2b927c97b60b06aeda944ac5c1bfef1e,Which datasets are used in experiments?,machine translation datasets of wmt ' 17,Sequence Copy Task and WMT'17,0.06123489886522293,0.5547104477882385
ac148fb921cce9c8e7b559bba36e54b63ef86350,What dataset they use for evaluation?,bibref7,The same 2K set from Gigaword used in BIBREF7,0.7294692993164062,0.5856055021286011
acc8d9918d19c212ec256181e51292f2957b37d7,What are the differences with previous applications of neural networks for this task?,use of only textual features available in the dataset,This approach considers related images,0.24301373958587646,0.14419764280319214
ace60950ccd6076bf13e12ee2717e50bc038a175,How are the two different models trained?,we vary the number of word - pieces from each article that are used in training,They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.,0.26927781105041504,0.5273462533950806
ad0a7fe75db5553652cd25555c6980f497e08113,How does the model compute the likelihood of executing to the correction semantic denotation?,the logical form receiving the highest score will be used for execution. the ranker is a discriminative log - linear model,By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x.,0.0455324687063694,0.5212928056716919
ad1f230f10235413d1fe501e414358245b415476,Which models were compared?,"esim bibref19, which includes cross - sentence attention, and kim bibref2","BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT",0.2839483618736267,0.44439494609832764
ad5898fa0063c8a943452f79df2f55a5531035c7,Which embeddings do they detect biases in?,googlenews bibref8,Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset,0.16090847551822662,0.3930969834327698
ad67ca844c63bf8ac9fdd0fa5f58c5a438f16211,Which unlabeled data do they pretrain with?,speech,1000 hours of WSJ audio data,0.506004810333252,0.1285521537065506
aeda22ae760de7f5c0212dad048e4984cd613162,What annotations are available in the dataset?,two distinct paraphrases,"For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)",0.2061063051223755,0.4846663773059845
af75ad21dda25ec72311c2be4589efed9df2f482,How much does this system outperform prior work?,accuracy,"The system outperforms by 27.7% the LSTM model, 38.5% the RL-SPINN model and 41.6% the Gumbel Tree-LSTM",0.058587122708559036,0.3363420069217682
b0376a7f67f1568a7926eff8ff557a93f434a253,How big is the performance difference between this method and the baseline?,micro f1,"Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.",0.4300893247127533,0.079503633081913
b13d0e463d5eb6028cdaa0c36ac7de3b76b5e933,What datasets are used to evaluate the model?,wn18 and fb15k,WN18 and FB15k,0.8623623847961426,1.0
b2254f9dd0e416ee37b577cef75ffa36cbcb8293,How many domains of ontologies do they gather data from?,,"5 domains: software, stuff, african wildlife, healthcare, datatypes",0.04018115624785423,0.053704120218753815
b27f7993b1fe7804c5660d1a33655e424cea8d10,What is the source of the visual data? ,posted images,Profile pictures from the Twitter users' profiles.,0.6813567280769348,0.42188674211502075
b3857a590fd667ecc282f66d771e5b2773ce9632,What is a string kernel?,a way of using information at the character level by measuring the similarity of strings through character n - grams,String kernel is a technique that uses character n-grams to measure the similarity of strings,0.3454245626926422,0.7875440120697021
b39f2249a1489a2cef74155496511cc5d1b2a73d,What is the accuracy reported by state-of-the-art methods?,accuracy,"Answer with content missing: (Table 1)
Previous state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)",0.15161694586277008,0.2660377621650696
b3a09d2e3156c51bd5fdc110a2a00a67bb8c0e42,what are the off-the-shelf systems discussed in the paper?,,"Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier.",0.0705188736319542,0.07786571234464645
b3fcab006a9e51a0178a1f64d1d084a895bd8d5c,what are the state of the art methods?,local or global temporal structure,"S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al.",0.44159674644470215,0.13995938003063202
b43fa27270eeba3e80ff2a03754628b5459875d6,What domains are present in the data?,"salons, dentists, doctors","Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather",0.049324218183755875,0.20109570026397705
b540cd4fe9dc4394f64d5b76b0eaa4d9e30fb728,Could you tell me more about the metrics used for performance evaluation?,,"BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy",0.032318148761987686,-0.035670846700668335
b5a2b03cfc5a64ad4542773d38372fffc6d3eac7,How are EAC evaluated?,,"Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement.",0.09423769265413284,0.006640469655394554
b5bfa6effdeae8ee864d7d11bc5f3e1766171c2d,Which regions of the United States do they consider?,lower 48 states,all regions except those that are colored black,0.43878161907196045,0.2672261893749237
b5e883b15e63029eb07d6ff42df703a64613a18a,How were topics of interest about DDEO identified?,topic modeling,using topic modeling model Latent Dirichlet Allocation (LDA),0.33035147190093994,0.7870478630065918
b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1,What were the non-neural baselines used for the task?,pre - extracted edit trees,The Lemming model in BIBREF17,0.09741219133138657,0.10473781079053879
b6b5f92a1d9fa623b25c70c1ac67d59d84d9eec8,By how much do they outpeform previous results on the word discrimination task?,5,Their best average precision tops previous best result by 0.202,0.06411147117614746,0.12419238686561584
b6f5860fc4a9a763ddc5edaf6d8df0eb52125c9e,Which 5 languages appear most frequently in AA paper titles?,122,"English, Chinese, French, Japanese and Arabic",0.10849224030971527,0.07164175808429718
b6fb72437e3779b0e523b9710e36b966c23a2a40,How many rules had to be defined?,how many rules had to be defined?,"WikiSQL - 2 rules (SELECT, WHERE)
SimpleQuestions - 1 rule
SequentialQA - 3 rules (SELECT, WHERE, COPY)",0.10794349759817123,0.3435983657836914
b7708cbb50085eb41e306bd2248f1515a5ebada8,How do they get the formal languages?,by analyzing the cell states and the activations of the gates in their lstm model,These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.,0.1043112725019455,0.28942185640335083
b8dea4a98b4da4ef1b9c98a211210e31d6630cf3,What is specific to gCAS cell?,three sequentially connected gated units handling the three components of the tuple,"It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner.",0.2524822950363159,0.5141351222991943
b8f711179a468fec9a0d8a961fb0f51894af4b31,What kind of neural network architecture do they use?,siamese,CNN,0.49442386627197266,0.29925382137298584
b9025c39838ccc2a79c545bec4a676f7cc4600eb,Why do they think this task is hard?  What is the baseline performance?,there may be situations where more than one action is reasonable,"1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.
2. Macro F1 = 14.6 (MLR, length 96 snippet)
Weighted F1 = 31.1 (LSTM, length 128 snippet)",0.16709311306476593,0.45773547887802124
b9f2a30f5ef664ff845d860cf4bfc2afb0a46e5a,How do they gather human judgements for similarity between relations?,wikidata bibref8,By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4,0.35752490162849426,0.4860169291496277
ba1da61db264599963e340010b777a1723ffeb4c,What does recurrent deep stacking network do?,stacks and concatenates the outputs of previous frames into the input features of the current frame,Stacks and joins outputs of previous frames with inputs of the current frame,0.6552029252052307,0.8971679210662842
ba56afe426906c4cfc414bca4c66ceb4a0a68121,What are the datasets used for the task?,table i,"Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)",0.11735222488641739,0.15320537984371185
ba6422e22297c7eb0baa381225a2f146b9621791,What is the performance difference between proposed method and state-of-the-arts on these datasets?,slight degradation in translation quality,Difference is around 1 BLEU score lower on average than state of the art methods.,0.10008536279201508,0.21072544157505035
bab8c69e183bae6e30fc362009db9b46e720225e,What are two strong baseline methods authors refer to?,,Marcheggiani and Titov (2017) and Cai et al. (2018),0.026089724153280258,0.18249046802520752
bb4de896c0fa4bf3c8c43137255a4895f52abeef,What is the baseline model?,atts2s bibref8,a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model,0.16277940571308136,0.3828897476196289
bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2,How do the various social phenomena examined manifest in different types of communities?,,"Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
",0.0378689244389534,0.16857939958572388
bbdb2942dc6de3d384e3a1b705af996a5341031b,What type of model are the ELMo representations used in?,pre - trained,A bi-LSTM with max-pooling on top of it,0.4605713188648224,0.15433575212955475
bc9c31b3ce8126d1d148b1025c66f270581fde10,What datasets are used to evaluate this approach?,data statistics of the benchmarks," Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ",0.39247724413871765,0.09122370928525925
bd1a3c651ca2b27f283d3f36df507ed4eb24c2b0,Is it a neural model? How is it trained?,"100, 000 iterations of gradient ascent","No, it is a probabilistic model trained by finding feature weights through gradient ascent",0.1738007664680481,0.2930153012275696
bd5379047c2cf090bea838c67b6ed44773bcd56f,Which experiments are perfomed?,which experiments are perfomed?,They used BERT-based models to detect subjective language in the WNC corpus,0.08147303760051727,0.1443636119365692
bd99aba3309da96e96eab3e0f4c4c8c70b51980a,Which existing models does this approach outperform?,rouge,"RNN-context, SRB, CopyNet, RNN-distract, DRGD",0.430402934551239,0.0066038258373737335
bdc93ac1b8643617c966e91d09c01766f7503872,What is the size of the second dataset?,5 to 24 lines each,1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,0.17432202398777008,0.22390194237232208
bdd8368debcb1bdad14c454aaf96695ac5186b09,How is the intensity of the PTSD established?,mean squared error of 1. 2,"Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.",0.20948712527751923,0.4222603440284729
bde6fa2057fa21b38a91eeb2bb6a3ae7fb3a2c62,what state of the accuracy did they obtain?,2015,51.5,0.7901573777198792,0.20289519429206848
bfce2afe7a4b71f9127d4f9ef479a0bfb16eaf76,What human evaluation metrics were used in the paper?,,rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context,0.24511078000068665,0.014198033139109612
c000a43aff3cb0ad1cee5379f9388531b5521e9a,how are the bidirectional lms obtained?,concatenate,"They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs.",0.3473704755306244,0.17102953791618347
c0035fb1c2b3de15146a7ce186ccd2e366fb4da2,By how much of MGNC-CNN out perform the baselines?,,"In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. 
In case of Irony the difference is about 2.0. 
",0.09034451842308044,0.034581176936626434
c029deb7f99756d2669abad0a349d917428e9c12,How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?,table 4,3%,0.13382737338542938,0.17719928920269012
c034f38a570d40360c3551a6469486044585c63c,How better is proposed method than baselines perpexity wise?,significant improvements,Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.,0.22151021659374237,0.21929879486560822
c13fe4064df0cfebd0538f29cb13e917fc5c3be0,What is the network architecture?,builds upon sogaard2016deep,"The network architecture has a multi-task Bi-Directional Recurrent Neural Network, with an unsupervised sequence labeling task and a low-dimensional embedding layer between tasks. There is a hidden layer after each successive task with skip connections to the senior supervised layers.",0.39027726650238037,0.09390382468700409
c165ea43256d7ee1b1fb6f5c0c8af5f7b585e60d,How much is proposed model better than baselines in performed experiments?,"how much is proposed model better than baselines in performed experiments?. it can be noted that we do not compare with the e2ecm baseline in apra. e2ecm only uses a simple classifier to recognize the label of the acts and ignores the parameters information. in our experiment, apra of e2ecm is slightly better than our method. considering the lack of parameters of the acts, it ' s unfair for our gdp method. furthermore, the cdm baseline considers the parameters of the act. but gdp is far better than cdm in supervised learning and reinforcement learning. ; bleu results : gdp significantly outperforms the baselines on bleu. as mentioned above, e2ecm is actually slightly better than gdp in apra. but in fact, we can find that the language quality of the response generated by gdp is still better than e2ecm, which proves that lack of enough parameters information makes it difficult to find the appropriate sentence template in nlg. it can be found that the bleu of all models is very poor on maluuba dataset float selected : table 2 : the performance of baselines and proposed model on dstc2 and maluuba dataset. t imefull is the time spent on training the whole model, t imedp is the time spent on training the dialogue policy maker. ; bpra results : as shown in table tabref35, most of the models have similar performance on bpra on these two datasets, which can guarantee a consistent impact on the dialogue policy maker. all the models perform very well in bpra on dstc2 dataset. on maluuba dataset, the bpra decreases because of the complex domains. we can notice that bpra of cdm is slightly worse than other models on maluuba dataset, the reason is that the cdm ' s dialogue policy maker contains lots of classifications and has the bigger loss than other models because of complex domains, which affects the training of the dialogue belief tracker. ; apra results : compared with baselines, gdp achieves the best performance in apra on two datasets. the reason is that maluuba is a human - human task - oriented dialogue dataset, the utterances are very flexible, the natural language generator for all methods is difficult to generate an accurate utterance based on the context. and dstc2 is a human - machine dialog dataset. the response is very regular so the","most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)
GDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)
GDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)",0.10002388060092926,0.5930922031402588
c1c611409b5659a1fd4a870b6cc41f042e2e9889,What evaluations did the authors use on their system?,,"BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.",0.04908810183405876,0.04648374021053314
c1f4d632da78714308dc502fe4e7b16ea6f76f81,Which language-pair had the better performance?,english - french,French-English,0.5170482993125916,0.9818572402000427
c33d0bc5484c38de0119c8738ffa985d1bd64424,Do the images have multilingual annotations or monolingual ones?,do the images have multilingual annotations or monolingual ones?,monolingual,0.09269246459007263,0.5494133234024048
c348a8c06e20d5dee07443e962b763073f490079,What two components are included in their proposed framework?,question and passage,evidence extraction and answer synthesis,0.6058371663093567,0.27103832364082336
c359ab8ebef6f60c5a38f5244e8c18d85e92761d,How many paraphrases are generated per question?,how many paraphrases are generated per question?,"10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans",0.09659159928560257,0.7625042200088501
c45feda62f23245f53e855706e2d8ea733b7fd03,Which translation system do they use to translate to English?,bibref6,Attention-based translation model with convolution sequence to sequence model,0.8712974190711975,0.12091983109712601
c47e87efab11f661993a14cf2d7506be641375e4,How does new evaluation metric considers critical informative entities?,,Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities,0.09423446655273438,0.006987310945987701
c4a6b727769328333bb48d59d3fc4036a084875d,What baseline did they compare Entity-GCN to?,recent prior work,"Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN",0.6283400058746338,0.12950259447097778
c4b5cc2988a2b91534394a3a0665b0c769b598bb,How do they define local variance?,the reciprocal of its variance expecting the attention model to be able to focus on more salient parts,The reciprocal of the variance of the attention distribution,0.31817302107810974,0.8373409509658813
c4c9c7900a0480743acc7599efb359bc81cf3a4d,How much is pre-training loss increased in Low/Medium/Hard level of pruning?,"how much is pre - training loss increased in low / medium / hard level of pruning? we ' ve seen that over - pruning bert deletes information useful for downstream tasks. is this information equally useful to all tasks? we might consider the pre - training loss as a proxy for how much pre - training information we ' ve deleted in total. similarly, the performance of information - deletion models is a proxy for how much of that information was useful for each task. figure figref18 shows that the pre - training loss linearly predicts the effects of information deletion on downstream accuracy. ; float selected : figure 2 : ( left ) pre - training loss predicts information deletion glue accuracy linearly as sparsity increases","The increase is linearly from lowest on average 2.0 , medium around 3.5, and the largest is 6.0",0.05732552334666252,0.13865971565246582
c515269b37cc186f6f82ab9ada5d9ca176335ded,What evidence do they present that the model attends to shallow context clues?,verb semantics,Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues,0.26178789138793945,0.5568389892578125
c554a453b6b99d8b59e4ef1511b1b506ff6e5aa4,how was the dataset built?,manually constructed set of indicator words,"Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable"" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is “yes"" or “no""",0.3328697085380554,0.11135564744472504
c69f4df4943a2ca4c10933683a02b179a5e76f64,What approach performs better in experiments global latent or sequence of fine-grained latent variables?,sequential latent variables of the svt,"PPL: SVT
Diversity: GVT
Embeddings Similarity: SVT
Human Evaluation: SVT",0.21991007030010223,0.3913032114505768
c77d6061d260f627f2a29a63718243bab5a6ed5a,How different is the dataset size of source and target?,much less training data,the training dataset is large while the target dataset is usually much smaller,0.24321477115154266,0.7087247371673584
c7d99e66c4ab555fe3d616b15a5048f3fe1f3f0e,What is an example of a computational social science NLP task?,,Visualization of State of the union addresses,0.014650391414761543,0.10091562569141388
c82e945b43b2e61c8ea567727e239662309e9508,What additional features are proposed for future work?,deeper analysis of clinical narratives in ehrs,distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort,0.31341788172721863,0.3848876357078552
ca26cfcc755f9d0641db0e4d88b4109b903dbb26,How better are results compared to baseline models?,it has similar performance,F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.,0.12884478271007538,0.3285459578037262
ca7e71131219252d1fab69865804b8f89a2c0a8f,How does this compare to traditional calibration methods like Platt Scaling?,,No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.,0.07504761964082718,-0.1473211795091629
cacb83e15e160d700db93c3f67c79a11281d20c5,Does this paper propose a new task that others can try to improve performance on?,,"No, there has been previous work on recognizing social norm violation.",0.10469142347574234,0.07772450149059296
caf9819be516d2c5a7bfafc80882b07517752dfa,Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?,"do they quantitavely or qualitatively evalute the output of their low - rank approximation to verify the grouping of lexical items? in this section, we evaluate the proposed method intrinsically in terms of whether the co - occurrence matrix after the low - rank approximation is able to capture similar concepts on student response data sets, and also extrinsically",They evaluate quantitatively.,0.10589966177940369,0.30682995915412903
cb78e280e3340b786e81636431834b75824568c3,How many emotions do they look at?,"joy, sadness, anger, fear, anticipation, surprise, love, disgust, neutral",9,0.12489480525255203,0.15999647974967957
cb8a6f5c29715619a137e21b54b29e9dd48dad7d,"What is an ""answer style""?",abstractive summary of the question and ten passages,well-formed sentences vs concise answers,0.1369815319776535,0.3899514973163605
cbbcafffda7107358fa5bf02409a01e17ee56bfd,Was any variation in results observed based on language typology?,it only encodes at most more information than some trivial baseline knowledge,It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information.,0.026457712054252625,0.4099554419517517
cc5d3903913fa2e841f900372ec74b0efd5e0c71,Which sentiment analysis tasks are addressed?,"b d, b e, b k, d b, d e, d k, e b, e d, e k, k b, k d, k e",12 binary-class classification and multi-class classification of reviews based on rating,0.7523805499076843,0.2107933759689331
cc5d8e12f6aecf6a5f305e2f8b3a0c67f49801a9,What percentage fewer errors did professional translations make?,what percentage fewer errors did professional translations make? float selected : table 5 : classification of errors in machine translation mt1 and two professional human translation outputs ha and hb. errors represent the number of sentences ( out of n = 150,36%,0.08442380279302597,0.23923994600772858
cd8de03eac49fd79b9d4c07b1b41a165197e1adb,How are multimodal representations combined?,linear cross - modal projection layer of dimensions,The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.,0.1682279109954834,0.13876181840896606
cdf1bf4b202576c39e063921f6b63dc9e4d6b1ff,What metric is used to measure performance?,"what metric is used to measure performance?. for those datasets nested cross - validation is used to tune the l2 penalty. for the trec dataset, as for the mrsp dataset, the l2 penalty is tuned on the predefined train split using 10 - fold cross - validation, and the accuracy is computed on the test set. ; unsupervised similarity evaluation. we perform unsupervised evaluation of the learnt sentence embeddings using the sentence cosine similarity, on the sts 2014 bibref31 and sick 2014 bibref32 datasets. these similarity scores are compared to the gold - standard human judgements using pearson ' s inlineform0 bibref33 and spearman ' s inlineform1 bibref34 correlation scores. the sick dataset consists of about 10, 000 sentence pairs along with relatedness scores of the pairs. the sts 2014 dataset contains 3, 770 pairs, divided into six different categories on the basis of the origin of sentences / phrases, namely twitter, headlines, news, forum, wordnet and images. we use a standard set of supervised as well as unsupervised benchmark tasks from the literature to evaluate our trained models, following bibref16. the breadth of tasks allows to fairly measure generalization to a wide area of different domains, testing the general - purpose quality ( universality ) of all competing sentence embeddings. for downstream supervised evaluations, sentence embeddings are combined with logistic regression to predict target labels. in the unsupervised evaluation for sentence similarity, correlation of the cosine similarity between two embeddings is compared to human annotators. ; downstream supervised evaluation. sentence embeddings are evaluated for various supervised classification tasks as follows. we evaluate paraphrase identification ( msrp ) bibref25, classification of movie review sentiment ( mr ) bibref26, product reviews ( cr ) bibref27, subjectivity classification ( subj ) bibref28, opinion polarity ( mpqa ) bibref29 and question type classification ( trec ) bibref30. to classify, we use the code provided by bibref22 in the same manner as in bibref16. for the msrp dataset, containing pairs of sentences inlineform0 with associated paraphrase label, we generate feature vectors by concatenating their sent2vec representations inlineform","Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks",0.10844177007675171,0.41725537180900574
ce807a42370bfca10fa322d6fa772e4a58a8dca1,What are the four forums the data comes from?,,"Darkode,  Hack Forums, Blackhat and Nulled.",0.048718586564064026,0.13425622880458832
cebf3e07057339047326cb2f8863ee633a62f49f,In which languages did the approach outperform the reported results?,in which languages did the approach outperform the reported results?,"Arabic, German, Portuguese, Russian, Swedish",0.07883576303720474,0.34902113676071167
cf63a4f9fe0f71779cf5a014807ae4528279c25a,How does the semi-automatic construction process work?,transcription from arabish to arabic script,Automatic transcription of 5000 tokens through sequential neural models trained on the annotated part of the corpus,0.48073655366897583,0.3176342844963074
cf93a209c8001ffb4ef505d306b6ced5936c6b63,From when are many VQA datasets collected?,2014,late 2014,0.1642436981201172,0.8164075016975403
cfbccb51f0f8f8f125b40168ed66384e2a09762b,How are discourse embeddings analyzed?,t - sne clustering bibref20,They perform t-SNE clustering to analyze discourse embeddings,0.2707355320453644,0.4062538146972656
cfffc94518d64cb3c8789395707e4336676e0345,What approaches without reinforcement learning have been tried?,what approaches without reinforcement learning have been tried?,"classification, regression, neural methods",0.08160652965307236,0.276719868183136
d028dcef22cdf0e86f62455d083581d025db1955,What are the strong baselines you have?,what are the strong baselines you have?,optimize single task with no synthetic data,0.05212622135877609,0.13982637226581573
d05d667822cb49cefd03c24a97721f1fe9dc0f4c,How did they get relations between mentions?,if they co - occur within the same document,"Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain.",0.3533403277397156,0.5162062644958496
d0bc782961567dc1dd7e074b621a6d6be44bb5b4,How big is seed lexicon used for training?,,30 words,0.11345618218183517,0.27408045530319214
d0c79f4a5d5c45fe673d9fcb3cd0b7dd65df7636,What are new best results on standard benchmark?,what are new best results on standard benchmark?,"New best results of accuracy (P@1) on Vecmap:
Ours-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43",0.06478334963321686,0.39612242579460144
d0f831c97d345a5b8149a9d51bf321f844518434,What labels are in the dataset?,r / anxiety,binary label of stress or not stress,0.4918172061443329,0.1736638844013214
d2fbf34cf4b5b1fd82394124728b03003884409c,Who was the top-scoring team?,friends,IDEA,0.25205832719802856,0.3203072249889374
d3092f78bdbe7e741932e3ddf997e8db42fa044c,What experimental evaluation is used?,real time streamer,root mean square error between the actual and the predicted price of Bitcoin for every minute,0.27140408754348755,0.05156891047954559
d3bcfcea00dec99fa26283cdd74ba565bc907632,How big is dataset for this challenge?,visdial v1. 0 bibref0,"133,287 images",0.36350691318511963,-0.0037957727909088135
d484a71e23d128f146182dccc30001df35cdf93f,How much is proposed model better in perplexity and BLEU score than typical UMT models?,the more the generated poem resembles the gold poem,"Perplexity of the best model is 65.58 compared to best baseline 105.79.
Bleu of the best model is 6.57 compared to best baseline 5.50.",0.220724418759346,-0.020648576319217682
d5256d684b5f1b1ec648d996c358e66fe51f4904,what is the practical application for this paper?,linguistic analysis,Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools.,0.2662927210330963,0.5066361427307129
d5498d16e8350c9785782b57b1e5a82212dbdaad,How accurate is model trained on text exclusively?,how accurate is model trained on text exclusively?,Relative error is less than 5%,0.054788023233413696,0.1534116417169571
d5fa26a2b7506733f3fa0973e2fe3fc1bbd1a12d,"How are possible sentence transformations represented in dataset, as new sentences?",seed sentences,"Yes, as new sentences.",0.25398367643356323,0.3648230731487274
d60a3887a0d434abc0861637bbcd9ad0c596caf4,What semantic rules are proposed?,ten,rules that compute polarity of words after POS tagging or parsing steps,0.10440987348556519,0.06397596746683121
d653d994ef914d76c7d4011c0eb7873610ad795f,How were breast cancer related posts compiled from the Twitter streaming API?,the primary feed for the analysis collected inlineform0 million tweets containing the keywords ` breast ' and ` cancer '. see figure figref2 for detailed twitter frequency statistics along with the user activity distribution. our secondary feed searched just for the keyword ` cancer ' which served as a comparison,"By using  keywords `breast' AND `cancer' in tweet collecting process. 
",0.11604756861925125,0.7622361183166504
d6e2b276390bdc957dfa7e878de80cee1f41fbca,What models other than standalone BERT is new model compared to?,our architecture,Only Bert base and Bert large are compared to proposed approach.,0.3268197178840637,0.12477361410856247
d6e8b32048ff83c052e978ff3b8f1cb097377786,"How are customer satisfaction, customer frustration and overall problem resolution data collected?",filters as pre - processing methods,By annotators on Amazon Mechanical Turk.,0.2785453200340271,0.10562877357006073
d70ba6053e245ee4179c26a5dabcad37561c6af0,Which datasets did they experiment on?,table 1 : various types of training instances,ConciergeQA and AmazonQA,0.44145500659942627,0.13494956493377686
d71cb7f3aa585e256ca14eebdc358edfc3a9539c,Which models achieve state-of-the-art performances?,syllabifiers,"CELEX (Dutch and English) - SVM-HMM
Festival, E-Hitz and OpenLexique - Liang hyphenation
IIT-Guwahat - Entropy CRF",0.17322050034999847,0.2554291784763336
d77c9ede2727c28e0b5a240b2521fd49a19442e0,What's the input representation of OpenIE tuples into the model?,the concatenation of word embedding and another embedding indicating whether this word is predicate,word embeddings,0.18485145270824432,0.6042171120643616
d79d897f94e666d5a6fcda3b0c7e807c8fad109e,What result from experiments suggest that natural language based agents are more robust?,applied noise,Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances,0.07731114327907562,-0.017083175480365753
d7d611f622552142723e064f330d071f985e805c,How many utterances are in the corpus?,how many utterances are in the corpus?,Total number of utterances available is: 70607 (37344 ENG + 33263 GER),0.10928385704755783,0.7013433575630188
d824f837d8bc17f399e9b8ce8b30795944df0d51,How do they show their model discovers underlying syntactic structure?,use the hidden states of space ( separator ) tokens to summarize previous information,By visualizing syntactic distance estimated by the parsing network,0.10698351263999939,0.43536171317100525
d8de12f5eff64d0e9c9e88f6ebdabc4cdf042c22,How much gain does the model achieve with pretraining MVCNN?,,0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj,0.1098732203245163,0.13857299089431763
da845a2a930fd6a3267950bec5928205b6c6e8e8,How was speed measured?,2 minutes,how long it takes the system to lemmatize a set number of words,0.3005882799625397,0.31902292370796204
da8bda963f179f5517a864943dc0ee71249ee1ce,How many layers does their system have?,1,4 layers,0.22558747231960297,0.3142956495285034
dac2591f19f5bbac3d4a7fa038ff7aa09f6f0d96,what are the three methods presented in the paper?,table 2 : map scoring of new methods,"Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.",0.047815822064876556,0.25821542739868164
dafa760e1466e9eaa73ad8cb39b229abd5babbda,How large is the dataset they generate?,table 3 : number of run - on ( ro ) and non - run - on ( non - ro ) sentences,4.756 million sentences,0.1095704585313797,0.46403706073760986
dbdf13cb4faa1785bdee90734f6c16380459520b,What cluster identification method is used in this paper?,"what cluster identification method is used in this paper?. ms uses a diffusive process on the graph to reveal the multiscale organisation at different resolutions without the need to choose a priori the number or type of clusters. the trained doc2vec model is subsequently used to infer high - dimensional vector descriptions for the text of each document in our target analysis set. we then compute a matrix containing all the pairwise ( cosine ) similarities between the doc2vec document vectors. this similarity matrix can be thought of as the adjacency matrix of a full, weighted graph with documents as nodes and edges weighted by their similarity. we sparsify this graph to the union of a minimum spanning tree and a k - nearest neighbors ( mst - knn ) graph bibref14, a geometric construction that removes less important similarities but preserves global connectivity for the graph and, hence, for the dataset. the mst - knn graph is then analysed with markov stability bibref15, bibref16, bibref17, bibref18","A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18",0.05118286982178688,0.430370569229126
dbfce07613e6d0d7412165e14438d5f92ad4b004,What affective-based features are used?,"different emotion models. ; emolex : it contains 14, 182 words associated with eight primary emotion based on the plutchik model bibref10, bibref11. ; emosenticnet ( emosn ) : it is an enriched version of senticnet bibref12 including 13, 189 words labeled by six ekman ' s basic emotion bibref13, bibref14. ; dictionary of affect in language ( dal ) : includes 8, 742 english words labeled by three scores representing three dimensions : pleasantness, activation and imagery","affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count",0.07307720184326172,0.7463631629943848
dc69256bdfe76fa30ce4404b697f1bedfd6125fe,What are the languages used to test the model?,"hindi, english, and german","Hindi, English and German (German task won)",0.9369887709617615,0.6937756538391113
dcb18516369c3cf9838e83168357aed6643ae1b8,Which retrieval system was used for baselines?,context,The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.,0.19951248168945312,0.07044505327939987
dd155f01f6f4a14f9d25afc97504aefdc6d29c13,What aspects have been compared between various language models?,"word - level perplexity, r @ 3 in next - word prediction, latency ( ms / q ), and energy usage ( mj / q )","Quality measures using perplexity and recall, and performance measured using latency and energy usage. ",0.6841673851013184,0.4914177358150482
dd5c9a370652f6550b4fd13e2ac317eaf90973a8,How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]?,proximity,0.9098 correlation,0.04396134242415428,0.1370544284582138
de12e059088e4800d7d89e4214a3997994dbc0d9,What are the baseline systems that are compared against?,listops,"The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM",0.26255378127098083,0.03711720556020737
df2839dbd68ed9d5d186e6c148fa42fce60de64f,How big is the provided treebank?,how big is the provided treebank?,"1448 sentences more than the dataset from Bhat et al., 2017",0.10693545639514923,0.3416108787059784
df79d04cc10a01d433bb558d5f8a51bfad29f46b,Which languages do they test on?,n - gram,"Answer with content missing: (Applications section) We use Wikipedia articles
in five languages
(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams
et al. (2017).
Select:
Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English",0.08101580291986465,0.22803357243537903
dfbab3cd991f86d998223726617d61113caa6193,"For the purposes of this paper, how is something determined to be domain specific knowledge?","we consider a subset of 20000 reviews from the domains cell phones and accessories ( c ), clothing and shoes ( s ), home and kitchen ( h ) and tools and home improvement ( t ). out of 20000 reviews, 10000 are positive and 10000 are negative. we use 12800 reviews for training, 3200 reviews for validation and 4000 reviews for testing",reviews under distinct product categories are considered specific domain knowledge,0.020332861691713333,0.5507160425186157
e051d68a7932f700e6c3f48da57d3e2519936c6d,Which pre-trained English NER model do they use?,flair,Bidirectional LSTM based NER model of Flair,0.6233583688735962,0.3060843348503113
e09e89b3945b756609278dcffb5f89d8a52a02cd,How many speeches are in the dataset?,speeches are in the dataset?,5575 speeches,0.048325859010219574,0.634364902973175
e0b7acf4292b71725b140f089c6850aebf2828d2,How is annotation projection done when languages have different word order?,word alignments,"Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments.",0.5411538481712341,0.6391220092773438
e111925a82bad50f8e83da274988b9bea8b90005,How do they collect the control corpus?,matched temporally,Randomly from Twitter,0.03500794246792793,0.1865016520023346
e1b36927114969f3b759cba056cfb3756de474e4,By how much does using phonetic feedback improve state-of-the-art systems?,speech enhancement scores for the state - of - the - art system trained with the parallel data available in the chime4 corpus,Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9,0.0389859601855278,0.23208403587341309
e2427f182d7cda24eb7197f7998a02bc80550f15,How is the architecture fault-tolerant?,a read only multi - set of data which can be distributed over a cluster of machines,By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault,0.16490334272384644,0.28302913904190063
e28019afcb55c01516998554503bc1b56f923995,"Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",very offensive ),Personal thought of the annotator.,0.2366843819618225,0.15913476049900055
e286860c41a4f704a3a08e45183cb8b14fa2ad2f,Is the model evaluated?,the evaluation of the german version is in progress,the English version is evaluated. The German version evaluation is in progress ,0.34078022837638855,0.8678472638130188
e292676c8c75dd3711efd0e008423c11077938b1,Which soft-selection approaches are evaluated?,previous attention - based methods,LSTM and BERT ,0.8458185791969299,0.3590553402900696
e2f269997f5a01949733c2ec8169f126dabd7571,Which data sources do they use?,table 1 : an approximate number of sentence pairs,"- En-Fr (WMT14)
- En-De (WMT15)
- Skipthought (BookCorpus)
- AllNLI (SNLI + MultiNLI)
- Parsing (PTB + 1-billion word)",0.1545701026916504,0.38137441873550415
e3981a11d3d6a8ab31e1b0aa2de96f253653cfb2,For which languages most of the existing MRC datasets are created?,english,English,0.4625296890735626,1.0
e414d819f10c443cbefa8bdb9bd486ffc6d1fc6a,What is the average length of the recordings?,30 to 50 minutes,40 minutes,0.8290117979049683,0.8042417168617249
e42fbf6c183abf1c6c2321957359c7683122b48e,How accurate is the aspect based sentiment classifier trained only using the XR loss?,"significantly better than all baselines methods that use the aspect - based data only, with p < 0. 05","BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.
BiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.
",0.3545790910720825,0.15555384755134583
e4a19b91b57c006a9086ae07f2d6d6471a8cf0ce,How are the main international development topics that states raise identified?,how are the main international development topics that states raise identified?," They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.",0.10897462069988251,0.3074984848499298
e4cc2e73c90e568791737c97d77acef83588185f,How long is the dataset?,from 1 to 5,8000,0.07409116625785828,0.11213277280330658
e51d0c2c336f255e342b5f6c3cf2a13231789fed,Which Twitter corpus was used to train the word vectors?,which twitter corpus was used to train the word vectors?,They collected tweets in Russian language using a heuristic query specific to Russian,0.1089833676815033,0.49517396092414856
e5a965e7a109ae17a42dd22eddbf167be47fca75,What are the problems related to ambiguity in PICO sentence prediction tasks?,neither its label nor its position reflect the intervention content,Some sentences are associated to ambiguous dimensions in the hidden state output,0.1733705997467041,0.204800546169281
e63bde5c7b154fbe990c3185e2626d13a1bad171,What is the performance achieved on NarrativeQA?,table 5,"Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87",0.24856729805469513,0.15775127708911896
e7329c403af26b7e6eef8b60ba6fefbe40ccf8ce,How much better does this baseline neural model do?,figure 2 : pr curve on our implicit tuples dataset,"The model outperforms at every point in the
implicit-tuples PR curve reaching almost 0.8 in recall",0.03393929824233055,0.6427328586578369
e76139c63da0f861c097466983fbe0c94d1d9810,Is the model presented in the paper state of the art?,"to make a thorough empirical comparison with previous studies, table 3 ( below the dashed line ) also shows the results of some state - of - the - art supervised coreference resolution systems","No, supervised models perform better for this task.",0.05817943811416626,0.3819400668144226
e8029ec69b0b273954b4249873a5070c2a0edb8a,How much important is the visual grounding in the learning of the multilingual representations?,semantic similarity scores ( spearman ’ s ρ ) across six subtasks for imagevec ( our method ) and previous work. coverage is in brackets,performance is significantly degraded without pixel data,0.04337047412991524,0.17538213729858398
e829f008d62312357e0354a9ed3b0827c91c9401,Which psycholinguistic and basic linguistic features are used?,meta - data,"Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features",0.8425865769386292,0.17888745665550232
e84ba95c9a188fda4563f45e53fbc8728d8b5dab,Do they build one model per topic or on all topics?,do they build one model per topic or on all topics?,One model per topic.,0.11036492884159088,0.8638753294944763
e8fcfb1412c3b30da6cbc0766152b6e11e17196c,What improvement does the MOE model make over the SOTA on language modelling?,what improvement does the moe model make over the sota on language modelling?,Perpexity is improved from 34.7 to 28.0.,0.09891734272241592,0.28335168957710266
e91692136033bbc3f19743d0ee5784365746a820,"It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?",neural networks,using multiple pivot sentences,0.07517145574092865,0.1549910455942154
e9ccc74b1f1b172224cf9f01e66b1fa9e34d2593,What metadata is included?,evidence search snippets,"besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date",0.48783227801322937,0.2997075915336609
e9d9bb87a5c4faa965ceddd98d8b80d4b99e339e,How much do they outperform previous state-of-the-art?,,"On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.
On subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.",0.06642229110002518,0.08940134197473526
ea6764a362bac95fb99969e9f8c773a61afd8f39,What is the highest accuracy score achieved?,threshold 0. 6,82.0%,0.2326328605413437,0.3635258972644806
ead5dc1f3994b2031a1852ecc4f97ac5760ea977,How many category tags are considered?,7. 16,14 categories,0.18429550528526306,0.29180529713630676
eb5ed1dd26fd9adb587d29225c7951a476c6ec28,What are the results of the experiment?,backoffs are computed using the ratio method,"They were able to create a language model from the dataset, but did not test.",0.01648367941379547,-0.02627740241587162
eb95af36347ed0e0808e19963fe4d058e2ce3c9f,What is the accuracy of the proposed technique?,significantly better at structured reasoning than tableilp. 9,51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge,0.29077011346817017,0.292184054851532
ec2b8c43f14227cf74f9b49573cceb137dd336e7,How is the speech recognition system evaluated?,data sets,Speech recognition system is evaluated using WER metric.,0.3227734863758087,0.1727035790681839
ed11b4ff7ca72dd80a792a6028e16ba20fccff66,How do they obtain region descriptions and object annotations?,visual genome dataset,they are available in the Visual Genome dataset,0.6510017514228821,0.8552848100662231
ed522090941f61e97ec3a39f52d7599b573492dd,What is triangulation?,conclusion,"Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.",0.39836782217025757,0.06859439611434937
ed7985e733066cd067b399c36a3f5b09e532c844,What is different in BERT-gen from standard BERT?,a left - to - right attention mask,"They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens.",0.24303793907165527,0.49705103039741516
ed7a3e7fc1672f85a768613e7d1b419475950ab4,Does this approach perform better in the multi-domain or single-domain setting?,multiwoz,single-domain setting,0.1172279417514801,0.1826503574848175
edb068df4ffbd73b379590762125990fcd317862,which benchmark tasks did they experiment on?,stanford sentiment treebank bibref7 and the ag english news corpus bibref3, They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task.,0.5586715340614319,0.6431533098220825
edb2d24d6d10af13931b3a47a6543bd469752f0c,How did the select the 300 Reddit communities for comparison?,inlineform0,They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.,0.7601481080055237,-0.1267302930355072
eddabb24bc6de6451bcdaa7940f708e925010912,How are the EAU text spans annotated?,the sentiment for an eau span ( inlineform1 ) is assigned to the highest possible node covering the eau span which does not contain the context sub - tree,Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.,0.149907648563385,0.2021096646785736
ee9b95d773e060dced08705db8d79a0a6ef353da,How are content clusters used to improve the prediction of incident severity?,additional features,they are used as additional features in a supervised classification task,0.14379586279392242,0.5563259720802307
ef7b62a705f887326b7ebacbd62567ee1f2129b3,What were the baselines?,,"Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations",0.059010643512010574,0.003442011773586273
ef872807cb0c9974d18bbb886a7836e793727c3d,What contextual features are used?,automatically extracted contextual keywords,The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.,0.7303208112716675,0.7016007900238037
efb3a87845460655c53bd7365bcb8393c99358ec,What were their results on the three datasets?,accuracy,"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR",0.27632296085357666,0.5179353952407837
efc65e5032588da4a134d121fe50d49fe8fe5e8c,What supplemental tasks are used for multitask learning?,"oriq / relq, oriq / relc, relq / relc","Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question",0.44219648838043213,0.07487992197275162
f10325d022e3f95223f79ab00f8b42e3bb7ca040,How are discourse features incorporated into the model?,an entity grid is constructed by feeding the document through an nlp pipeline to identify salient entities,They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.,0.18851900100708008,0.4877243936061859
f1831b2e96ff8ef65b8fde8b4c2ee3e04b7ac4bf,How close do clusters match to ground truth tone categories?,"only the first syllable of each word, or all syllables","NMI between cluster assignments and ground truth tones for all sylables is:
Mandarin: 0.641
Cantonese: 0.464",0.31700530648231506,0.22701460123062134
f1bd66bb354e3dabf5dc4a71e6f08b17d472ecc9,How do slot binary classifiers improve performance?,adds extra supervision to generate the slots that will be present in the response more precisely before generating the final textual agent response,by adding extra supervision to generate the slots that will be present in the response,0.3145807981491089,0.8055242300033569
f258ada8577bb71873581820a94695f4a2c223b3,How many samples did they generate for the artificial language?,"30, 000","70,000",0.3626313805580139,0.7394092679023743
f2c5da398e601e53f9f545947f61de5f40ede1ee,How do their interpret the coefficients?,we will also project back coefficients from the embeddings as well as pca models into the dummy variable space,The coefficients are projected back to the dummy variable space.,0.3223356306552887,0.5969076156616211
f37ed011e7eb259360170de027c1e8557371f002,What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?,none,"Humor in headlines (TitleStylist vs Multitask baseline):
Relevance: +6.53% (5.87 vs 5.51)
Attraction: +3.72% (8.93 vs 8.61)
Fluency: 1,98% (9.29 vs 9.11)",0.1675003170967102,0.12074381858110428
f398587b9a0008628278a5ea858e01d3f5559f65,By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?,big margin,"SPNet vs best baseline:
ROUGE-1: 90.97 vs 90.68
CIC: 70.45 vs 70.25",0.2374143898487091,0.24123556911945343
f4238f558d6ddf3849497a130b3a6ad866ff38b3,How is moral bias measured?,when considering two opposite answers,"Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q) − cos(b, q)
Bias is calculated as substraction of cosine similarities of question and some answer for two opposite answers.",0.18416689336299896,0.2863924503326416
f42d470384ca63a8e106c7caf1cb59c7b92dbc27,What evaluation metrics are used?,"align = left, leftmargin = 0em, labelsep = 0. 4em, font =","exact match, f1 score, edit distance and goal match",0.10962098091840744,0.19121232628822327
f463db61de40ae86cf5ddd445783bb34f5f8ab67,what are the baselines?,local features,Perceptron model using the local features.,0.6142593026161194,0.599023699760437
f4e17b14318b9f67d60a8a2dad1f6b506a10ab36,How is the generative model evaluated?,qualitative analysis,Comparing BLEU score of model with and without attention,0.05891335383057594,0.10896790027618408
f513e27db363c28d19a29e01f758437d7477eb24,what are the baselines?,attention sum reader ( as reader ),"AS Reader, GA Reader, CAS Reader",0.36760851740837097,0.4961755871772766
f52b2ca49d98a37a6949288ec5f281a3217e5ae8,How do they condition the output to a given target-source class?,"target forcing in multilingual nmt bibref15, bibref16. we first split the training sentence pairs into three groups according to the target / source length ratio ( in terms of characters ). ideally, we want a group where the target is shorter than the source ( short ), one where they are equally - sized ( normal ) and a last group where the target is longer than the source ( long ). in practice, we select two thresholds and according to the length ratio distribution. all the sentence pairs with length ratio between and are in the normal group, the ones with ratio below in short and the remaining in long. at training time we prepend a length token to each source sentence according to its group ( short, normal, or long ), in order to let a single network to discriminate between the groups","They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.",0.03360682725906372,0.5524562001228333
f5707610dc8ae2a3dc23aec63d4afa4b40b7ec1e,What are resolution model variables?,indicates in which mode the mention should be resolved,"Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.",0.30849725008010864,0.7886202335357666
f5db12cd0a8cd706a232c69d94b2258596aa068c,How much in experiments is performance improved for models trained with generated adversarial examples?,10 %,"Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original
examples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)",0.4410485625267029,0.2588613033294678
f5e571207d9f4701b4d01199ef7d0bfcfa2c0316,What are the linguistic differences between each class?,,"Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes",0.03388684242963791,0.012810986489057541
f62c78be58983ef1d77049738785ec7ab9f2a3ee,what datasets did the authors use?,,"Kaggle
Subversive Kaggle
Wikipedia
Subversive Wikipedia
Reddit
Subversive Reddit ",0.06085933744907379,0.1981971561908722
f6380c60e2eb32cb3a9d3bca17cf4dc5ae584eca,Why is big data not appropriate for this task?,domain - specific data can be very valuable,Training embeddings from small-corpora can increase the performance of some tasks,0.342964768409729,0.22595638036727905
f68bd65b5251f86e1ed89f0c858a8bb2a02b233a,how do they collect the comparable corpus?,gardenhose api,Randomly from a Twitter dump,0.2663499116897583,0.0446169488132
f7817b949605fb04b1e4fec9dd9ca8804fb92ae9,Why does not the approach from English work on other languages?,english does not mark grammatical gender,"Because, unlike other languages, English does not mark grammatical genders",0.4461837410926819,0.8804225921630859
f7c34b128f8919e658ba4d5f1f3fc604fb7ff793,What are all the input modalities considered in prior work in question generation?,"text summarization bibref24, image captioning bibref25 and table - to - text generation bibref26","Textual inputs, knowledge bases, and images.",0.6435257792472839,0.2774648070335388
f8281eb49be3e8ea0af735ad3bec955a5dedf5b3,Is the semantic hierarchy representation used for any task?,"when applying dissim as a preprocessing step, the performance of state - of - the - art open ie systems can be improved by up to 346 % in precision and 52 % in recall","Yes, Open IE",0.012858820147812366,0.3422314524650574
fa2a384a23f5d0fe114ef6a39dced139bddac20e,How big is the dataset?,one dataset per each of the apex courts,903019 references,0.5348163843154907,0.06128498166799545
fa2ffc6b4b046e17bc41e199855c4941673e2caf,What parallel corpus did they use?,inlineform0 ) and mandarin ( inlineform1,Parallel monolingual corpus in English and Mandarin,0.07850593328475952,0.3522999584674835
fa3312ae4bbed11a5bebd77caf15d651962e0b26,What was the performance on the self-collected corpus?,,F1 scores of 86.16 on slot filling and 94.56 on intent detection,0.036733292043209076,0.10462986677885056
fa9df782d743ce0ce1a7a5de6a3de226a7e423df,What are the languages they consider in this paper?,"english, russian, arabic, chinese, german, spanish, french","The languages considered were English, Chinese, German, Russian, Arabic, Spanish, French",0.34419459104537964,0.8015304207801819
fb5ce11bfd74e9d7c322444b006a27f2ff32a0cf,What is task success rate achieved? ,,96-97.6% using the objects color or shape and 79% using shape alone,0.10804103314876556,0.011034195311367512
fb76e994e2e3fa129f1e94f1b043b274af8fb84c,How does the context-aware variational autoencoder learn event background information?,by using the context - aware latent variable," CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.",0.4682806432247162,0.3799329996109009
fbe5e513745d723aad711ceb91ce0c3c2ceb669e,What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?,"the dataset provided by the organizers of trac - 2018 bibref0, bibref2 is actually a code - mixed dataset",nan,0.19238930940628052,0.04472751542925835
fbee81a9d90ff23603ee4f5986f9e8c0eb035b52,What are their initial results on this task?,,"Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.",0.11573216319084167,0.05857554078102112
fc3f0eb297b2308b99eb4661a510c9cdbb6ffba2,What is the size of the dataset?,kvret bibref5,3029,0.22926728427410126,0.19301727414131165
fc8bc6a3c837a9d1c869b7ee90cf4e3c39bcd102,Is the baseline a non-heirarchical model like BERT?,is the baseline a non - heirarchical model like bert?,There were hierarchical and non-hierarchical baselines; BERT was one of those baselines,0.047503650188446045,0.7537052631378174
fd7f13b63f6ba674f5d5447b6114a201fe3137cb,What language is the experiment done in?,natural language,english language,0.49363282322883606,0.6517577171325684
fd8b6723ad5f52770bec9009e45f860f4a8c4321,What QA models were used?,"what qa models were used?. the same highway layer is applied to and produces. ; next, and are fed into a bi - directional long short - term memory network ( bilstm ) bibref44 respectively in order to model the temporal interactions between sequence words : ; here we obtain and. then we feed and into the attention flow layer bibref27 to model the interactions between the input text and query. we obtain the - dimension query - aware context embedding vectors as the result. ; the input of our model are the words in the input text and query. we concatenate pre - trained word embeddings from glove bibref40 and character embeddings trained by charcnn bibref41 to represent input words. the - dimension embedding vectors of input text and query are then fed into a highway layer bibref42 to improve the capability of word embeddings and character embeddings as ; \ begin { split } g _ t & = { \ rm sigmoid } ( w _ gx _ t + b _ g ) \ \ s _ t & = { \ rm relu } ( w _ xx _ t + b _ x ) \ \ u _ t & = g _ t \ odot s _ t + ( 1 - g _ t ) \ odot x _ t ~. \ end { split } ( eq. 18 ) ; here as the result. ; after modeling interactions between the input text and queries, we need to enhance the interactions within the input text words themselves especially for the longer text in ie settings. besides, is the attention weight from the word to the word and is the enhanced contextual embeddings over the word in the input text. we obtain the - dimension query - aware and self - enhanced embeddings of input text after this step. finally we feed the embeddings w _ h, \ tilde { w _ h } \ in \ mathbb { r } ^ { d \ times 8d } w \ in \ mathbb { r } ^ d [ h, c ] \ alpha _ i ^ t., o _ n ] \ beta _ { n + 1 } ^ t { \ rm eos } \ textbf { a } ( eq. 18 ) ; here and are trainable weights, is a - dimension vector. the function relu is the rectified linear units bibref43 and is","A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.",0.10621281713247299,0.6723965406417847
fe90eec1e3cdaa41d2da55864c86f6b6f042a56c,What are the sources of the data?,online data for three domains,"User reviews written in Chinese collected online for hotel, mobile phone, and travel domains",0.11888022720813751,0.28265875577926636
fee5aef7ae521ccd1562764a91edefecec34624d,How does explicit constraint on the KL divergence term that authors propose looks like?,the above objective function satisfies the constraint ( experiment ),"Answer with content missing: (Formula 2) Formula 2 is an answer: 
\big \langle\! \log p_\theta({x}|{z}) \big \rangle_{q_\phi({z}|{x})}  -  \beta |D_{KL}\big(q_\phi({z}|{x}) || p({z})\big)-C|",0.1861037164926529,0.148466095328331
ff28d34d1aaa57e7ad553dba09fc924dc21dd728,What are their correlation results?,"spearman ’ s ρ, kendall ’ s τ and pearson ’ s r",High correlation results range from 0.472 to 0.936,0.5209079384803772,0.2895699739456177
ff2bcf2d8ffee586751ce91cf15176301267b779,What are the characteristics of the city dialect?,"what are the characteristics of the city dialect?. the character of its vocabulary is more uniform as compared with the purple group. while the purple cluster prefer particular utterances, the lexicon of the urban group includes most of the keywords",Lexicon of the cities tend to use most forms of a particular concept,0.03355228528380394,0.6066904664039612
