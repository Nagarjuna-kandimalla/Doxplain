{
 "cells": [
  {
   "cell_type": "code",
   "id": "eab5180e-e79e-4680-b953-8be7fa87b20e",
   "metadata": {},
   "source": [
    "pip install kagglehub"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "553493eb-d631-49e1-a836-83b417d4f05d",
   "metadata": {},
   "source": [
    "pip install transformers datasets sentence-transformers langchain chromadb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "009d5fe1-082e-4560-bce7-e463ed4f9749",
   "metadata": {},
   "source": [
    "pip install sentence-transformers"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d363aec3-ad50-4345-95e7-40ad2aa1b327",
   "metadata": {},
   "source": [
    "pip install --upgrade \"numpy>=2.0\" \"scipy>=1.14\" scikit-learn transformers sentence-transformers chromadb\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61461e13-e6df-4587-a6e8-28819f99e158",
   "metadata": {},
   "source": [
    "pip freeze > requirements.txt\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3325c9ce-0cc8-481b-be62-70766808f08e",
   "metadata": {},
   "source": [
    "import kagglehub\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "809e7323-7e3b-44c3-b746-d4b909056184",
   "metadata": {},
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8545e35c-a075-4668-8df7-1dbc0ab9181e",
   "metadata": {},
   "source": [
    "# Download QASPER dataset using KaggleHub\n",
    "dataset_path = kagglehub.dataset_download(\"thedevastator/qasper-nlp-questions-and-evidence\")\n",
    "print(f\"Path to QASPER dataset: {dataset_path}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6de17e14-3b82-4fb8-91bc-d138489d3868",
   "metadata": {},
   "source": [
    "#Importing QASPER dataset\n",
    "\n",
    "\n",
    "test = pd.read_csv(f\"{dataset_path}/test.csv\")\n",
    "train = pd.read_csv(f\"{dataset_path}/train.csv\")\n",
    "validation = pd.read_csv(f\"{dataset_path}/validation.csv\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25d5f54c-9674-46b2-acf6-d0acde68d6e5",
   "metadata": {},
   "source": [
    "We had issues with the Keggle CSV file for QASPER DATASET. Tried parsing it manually as well as using the nested objects, but it still gave errors. This may be because of the object is converted to a string and the string contains Object types like 'Array()' which cause the json parse error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac752cd-5d1b-4d53-8d31-50697690519f",
   "metadata": {},
   "source": [
    "We are using the same data QASPER sets but via HuggingFace, which provides straightfaward json object to parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68f648-f3a8-4e1f-88b8-64967f501b59",
   "metadata": {},
   "source": [
    "### Load the QASPER from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf51b14f-806e-423a-b2e3-5294f9fb9182",
   "metadata": {},
   "source": [
    "qasper_ds = load_dataset(\"allenai/qasper\", split=\"train\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33823591-f81e-4cb3-b93a-ec9d90798383",
   "metadata": {},
   "source": [
    "qasper_ds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08f5fa59-9fc9-45af-9a76-413eb8d90d7e",
   "metadata": {},
   "source": [
    "rows = []\n",
    "for paper in qasper_ds:\n",
    "    paper_id = paper[\"id\"]\n",
    "    title    = paper[\"title\"]\n",
    "    abstract = paper[\"abstract\"]\n",
    "\n",
    "    # full_text is a dict of columns\n",
    "    sec_names = paper[\"full_text\"][\"section_name\"]\n",
    "    sec_paras = paper[\"full_text\"][\"paragraphs\"]\n",
    "    full_text = \"\\n\\n\".join(\n",
    "        f\"{sec}\\n\" + \"\\n\".join(p) for sec, p in zip(sec_names, sec_paras)\n",
    "    )\n",
    "\n",
    "    qas = paper[\"qas\"]\n",
    "    n_questions = len(qas[\"question\"])\n",
    "\n",
    "    for i in range(n_questions):\n",
    "        question_id   = qas[\"question_id\"][i]\n",
    "        question_text = qas[\"question\"][i]\n",
    "        nlp_bg        = qas[\"nlp_background\"][i]\n",
    "        topic_bg      = qas[\"topic_background\"][i]\n",
    "        paper_read    = qas[\"paper_read\"][i]\n",
    "        search_query  = qas[\"search_query\"][i]\n",
    "        question_writer = qas[\"question_writer\"][i]\n",
    "\n",
    "        # answers is ALSO a dict of parallel lists\n",
    "        answers_block = qas[\"answers\"][i]\n",
    "        for ans, ann_id, worker_id in zip(\n",
    "            answers_block[\"answer\"],\n",
    "            answers_block[\"annotation_id\"],\n",
    "            answers_block[\"worker_id\"]\n",
    "        ):\n",
    "            rows.append({\n",
    "                \"paper_id\"        : paper_id,\n",
    "                \"title\"           : title,\n",
    "                \"abstract\"        : abstract,\n",
    "                \"full_text\"       : full_text,\n",
    "                \"question_id\"     : question_id,\n",
    "                \"question\"        : question_text,\n",
    "                \"nlp_background\"  : nlp_bg,\n",
    "                \"topic_background\": topic_bg,\n",
    "                \"paper_read\"      : paper_read,\n",
    "                \"search_query\"    : search_query,\n",
    "                \"question_writer\" : question_writer,\n",
    "                \"annotation_id\"   : ann_id,\n",
    "                \"worker_id\"       : worker_id,\n",
    "                \"unanswerable\"    : ans[\"unanswerable\"],\n",
    "                \"yes_no\"          : ans[\"yes_no\"],\n",
    "                \"free_form_answer\": ans[\"free_form_answer\"],\n",
    "                \"extractive_spans\": \"; \".join(ans[\"extractive_spans\"]),\n",
    "                \"evidence\"        : \"; \".join(ans[\"evidence\"]),\n",
    "                \"highlighted_evidence\": \"; \".join(ans[\"highlighted_evidence\"])\n",
    "            })\n",
    "\n",
    "qasper_df = pd.DataFrame(rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5bd21744-7b64-4bf6-96b2-baf23c4c1b1b",
   "metadata": {},
   "source": [
    "qasper_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7c6b5df-635f-4fc7-9bde-405c1ff3504c",
   "metadata": {},
   "source": [
    "len(qasper_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba17c08c-d16a-49e5-9958-b33912ad9992",
   "metadata": {},
   "source": [
    "#Drop question rows where the answer is empty\n",
    "qasper_df = qasper_df[qasper_df['free_form_answer'] != \"\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e02724e8-5a7c-46c6-9b14-a958b4b81315",
   "metadata": {},
   "source": [
    "len(qasper_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97cf70e8-5f40-454e-a678-a4b516e974a5",
   "metadata": {},
   "source": [
    "qasper_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "025c326a-beb4-4c3c-b29c-202f96bf20ec",
   "metadata": {},
   "source": [
    "qasper_df.to_csv('processed_qasper_data.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9645a0bf-8e2c-4346-8c37-3982f8548702",
   "metadata": {},
   "source": [
    "### Loading the HotPotQA Dataset from Kegglehub"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd5c58cb-976d-4ef6-8497-99c481cf9339",
   "metadata": {},
   "source": [
    "# Download HotpotQA dataset using KaggleHub\n",
    "path = kagglehub.dataset_download(\"jeromeblanchet/hotpotqa-question-answering-dataset\")\n",
    "print(f\"Path to HotpotQA dataset: {path}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1209d513-d3db-4625-a765-1581caa9bd40",
   "metadata": {},
   "source": [
    "#Importing HotPotQA Dataset\n",
    "import json\n",
    "\n",
    "with open(f\"{path}/hotpot_dev_distractor_v1.json\", \"r\") as f:\n",
    "    hotpot_data = json.load(f)\n",
    "\n",
    "print(len(hotpot_data))        \n",
    "print(hotpot_data[0].keys())   "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ce115bd-5ba2-4e95-815f-533470bfc099",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for ex in hotpot_data:\n",
    "    rows.append({\n",
    "        \"id\": ex[\"_id\"],\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"answer\": ex[\"answer\"],\n",
    "        \"context\": ex[\"context\"],\n",
    "        \"supporting_facts\": ex[\"supporting_facts\"]\n",
    "    })\n",
    "\n",
    "hotpot_df = pd.DataFrame(rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f76473b-c192-4ff3-9bca-c4a8d0b7c1a7",
   "metadata": {},
   "source": [
    "hotpot_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17cdb34a-83a8-460a-a310-7352b26f4934",
   "metadata": {},
   "source": [
    "hotpot_df.to_csv('processed_hotpot_df.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "847d6778-b2f9-42c7-950e-0ba505fc121d",
   "metadata": {},
   "source": [
    "## Step 1: Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be25ef-37a0-49c7-9fd4-46e7ea86e75a",
   "metadata": {},
   "source": [
    "### We will store context in ChromaDb with chunking, for this task we will use 256 as our chunk size"
   ]
  },
  {
   "cell_type": "code",
   "id": "e2a86b50-8171-443e-8922-6809a94d5242",
   "metadata": {},
   "source": [
    "def ingest_qasper_to_chroma(qasper_df, chroma_collection, character_splitter, token_splitter):\n",
    "    for paper_id, group in tqdm(qasper_df.groupby(\"paper_id\")):\n",
    "        full_text = str(group.iloc[0][\"full_text\"])\n",
    "    \n",
    "        char_chunks = character_splitter.split_text(full_text)\n",
    "    \n",
    "        token_chunks = []\n",
    "        for chunk in char_chunks:\n",
    "            token_chunks.extend(token_splitter.split_text(chunk))\n",
    "    \n",
    "        if not token_chunks:\n",
    "            print(f\"Skipping paper {paper_id}: no chunks produced.\")\n",
    "            continue\n",
    "        \n",
    "        ids = [f\"{paper_id}_{i}\" for i in range(len(token_chunks))]\n",
    "        question_ids = group[\"question_id\"].tolist()\n",
    "        metadatas = [\n",
    "            {\n",
    "                \"paper_id\": paper_id,\n",
    "                \"question_ids\": \",\".join(question_ids),\n",
    "            }\n",
    "            for _ in token_chunks\n",
    "        ]\n",
    "    \n",
    "        chroma_collection.add(\n",
    "            documents=token_chunks,\n",
    "            ids=ids,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "    \n",
    "    print(\"All papers processed and stored in Chroma.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1250d0c0-107d-49ce-bf4a-6c1c8ef22e0a",
   "metadata": {},
   "source": [
    "def ingest_hotpot_to_chroma(hotpot_df, chroma_collection, character_splitter, token_splitter):\n",
    "\n",
    "    for q_id, group in tqdm(hotpot_df.groupby(\"id\"), desc=\"Processing Hotpot QA\"):\n",
    "\n",
    "        context_blocks = []\n",
    "        for title, paragraphs in group.iloc[0][\"context\"]:\n",
    "            section_text = \"\\n\".join(paragraphs)\n",
    "            context_blocks.append(f\"{title}\\n{section_text}\")\n",
    "        full_text = \"\\n\\n\".join(context_blocks)\n",
    "        char_chunks = character_splitter.split_text(full_text)\n",
    "\n",
    "        token_chunks = []\n",
    "        for chunk in char_chunks:\n",
    "            token_chunks.extend(token_splitter.split_text(chunk))\n",
    "\n",
    "        if not token_chunks:\n",
    "            print(f\"Skipping question {q_id}: no context found produced.\")\n",
    "            continue\n",
    "\n",
    "        ids = [f\"{q_id}_{i}\" for i in range(len(token_chunks))]\n",
    "        \n",
    "        metadatas = [\n",
    "            {\n",
    "                \"hotpot_id\": q_id\n",
    "            }\n",
    "            for _ in token_chunks\n",
    "        ]\n",
    "\n",
    "        chroma_collection.add(\n",
    "            documents=token_chunks,\n",
    "            ids=ids,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "\n",
    "    print(\"All HotpotQA questions processed and stored in Chroma.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "38f71423-e7f1-401b-a2dc-05402bbb519b",
   "metadata": {},
   "source": [
    "### We have Store all the Data For QASPER DATA SET AND WE MOVE INTO LOAD MODELS NOTEBOOK FOR ANSWER GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "id": "78c3122d-92f8-4d9c-9017-8bd2d84c9d67",
   "metadata": {},
   "source": [
    "DATASET_HOTPOT = \"HotpotQA\"\n",
    "DATASET_QASPER = \"Qasper\"\n",
    "\n",
    "TOKEN_CHUNK_SIZE = 256\n",
    "TOKEN_CHUNK_OVERLAP = 10\n",
    "\n",
    "CHAR_CHUNK_SIZE = 1000\n",
    "CHAR_CHUNK_OVERLAP = 10"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c722c78d-6e96-4502-bb0b-f8dc2f4ecbaa",
   "metadata": {},
   "source": [
    "def run_data_loader_pipeline(DATASET_TYPE, collection, dataframe):\n",
    "    \n",
    "    character_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        chunk_size=CHAR_CHUNK_SIZE,\n",
    "        chunk_overlap=CHAR_CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    token_splitter = SentenceTransformersTokenTextSplitter(\n",
    "        chunk_overlap=TOKEN_CHUNK_OVERLAP,\n",
    "        tokens_per_chunk=TOKEN_CHUNK_SIZE\n",
    "    )\n",
    "\n",
    "    if DATASET_TYPE == DATASET_QASPER:\n",
    "        ingest_qasper_to_chroma(dataframe, collection, character_splitter, token_splitter)\n",
    "    elif DATASET_TYPE == DATASET_HOTPOT:\n",
    "        ingest_hotpot_to_chroma(dataframe, collection, character_splitter, token_splitter)\n",
    "    else:\n",
    "        print(\"Invalid Dataset Type\")    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "618fc462-1d78-46d3-85d7-acab447ee82c",
   "metadata": {},
   "source": [
    "### Execute the data ingestion for Datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa85f67a-dd8b-4846-8539-1041f4dc8366",
   "metadata": {},
   "source": [
    "def create_chroma_collections(db_path: str, configs: dict):\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    collections = {}\n",
    "\n",
    "    for dataset_name, cfg in configs.items():\n",
    "        coll_name  = cfg[\"collection\"]\n",
    "        model_name = cfg[\"model\"]\n",
    "\n",
    "        try:\n",
    "            client.delete_collection(name=coll_name)\n",
    "            print(f\"Deleted old collection: {coll_name}\")\n",
    "        except Exception:\n",
    "            pass  \n",
    "\n",
    "        # create embedding function for this dataset\n",
    "        embedding_fn = SentenceTransformerEmbeddingFunction(model_name=model_name)\n",
    "\n",
    "        # create and store collection\n",
    "        collections[dataset_name] = client.create_collection(\n",
    "            name=coll_name,\n",
    "            embedding_function=embedding_fn\n",
    "        )\n",
    "        print(f\"Created collection '{coll_name}' with model '{model_name}'\")\n",
    "\n",
    "    return collections\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0fe6e8a4-c610-4870-ab3e-b796f403e5d7",
   "metadata": {},
   "source": [
    "DATASET_CONFIGS = {\n",
    "    \"HotpotQA\": {\n",
    "        \"collection\": \"hpqa_data_collection\",\n",
    "        \"model\": \"all-MiniLM-L6-v2\"\n",
    "    },\n",
    "    \"Qasper\": {\n",
    "        \"collection\": \"qasper_data_collection\",\n",
    "        \"model\": \"all-MiniLM-L6-v2\"\n",
    "    },\n",
    "    # Team: Add configs here\n",
    "}\n",
    "\n",
    "collections = create_chroma_collections(\"./ChromaDb\", DATASET_CONFIGS)\n",
    "\n",
    "hotpot_collection = collections[\"HotpotQA\"]\n",
    "qasper_collection = collections[\"Qasper\"]\n",
    "\n",
    "run_data_loader_pipeline(DATASET_HOTPOT, hotpot_collection, hotpot_df)\n",
    "run_data_loader_pipeline(DATASET_QASPER, qasper_collection, qasper_df)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
