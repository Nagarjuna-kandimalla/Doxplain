{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118b485a",
   "metadata": {},
   "source": [
    "# This notebook contains Step 2 : Answer Generation and  Step 3 Assertions part of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "a6816395-caef-48a4-b11c-83a509b533ad",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  \n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import pipeline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee791dc3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "import warnings\n",
    "from transformers.utils import logging as hf_logging"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45234950-067b-486c-8a05-bdab5d015860",
   "metadata": {},
   "source": [
    "import warnings\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38fbdcec-68f7-4254-8961-0d296081aecf",
   "metadata": {},
   "source": [
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "09e61c74",
   "metadata": {},
   "source": [
    "### Fetch data from the ChromaDb Store"
   ]
  },
  {
   "cell_type": "code",
   "id": "608d7aba-e281-4fe3-9fb0-b003cdb75d8b",
   "metadata": {},
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./ChromaDb\")\n",
    "collection_name = \"qasper_data_collection\"\n",
    "\n",
    "chroma_collection = chroma_client.get_collection(collection_name)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0691b560-9e29-438b-8f5e-521d100e57b3",
   "metadata": {},
   "source": [
    "questions_qasper = pd.read_csv('processed_qasper_data.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b16e3ed-c3b5-4c02-a85a-47c2cea19707",
   "metadata": {},
   "source": [
    "questions_qasper.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b570184b-3c97-494d-a694-b98d192e4971",
   "metadata": {},
   "source": [
    "# keep only rows with a question\n",
    "questions = (\n",
    "    questions_qasper.groupby(\"question_id\")\n",
    "      .first()                                     \n",
    "      .reset_index()[[\"question_id\",\"question\",\n",
    "                      \"paper_id\",\"free_form_answer\"]]\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "375002eb-6630-4797-94db-444ea7c62284",
   "metadata": {},
   "source": [
    "questions.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20368b84-2559-4739-b6e9-366bd22f007d",
   "metadata": {},
   "source": [
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdb3c7a1-75ef-4557-919b-498398bfab84",
   "metadata": {},
   "source": [
    "def retrieve_chunks(question, paper_id=None, k=10):\n",
    "    where_clause = {\"paper_id\": str(paper_id)} if paper_id else None\n",
    "\n",
    "    result = chroma_collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=k,\n",
    "        where=where_clause\n",
    "    )\n",
    "    \n",
    "    pairs = [(question, doc) for doc in result[\"documents\"][0]]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # sort by score descending\n",
    "    reranked = [doc for _, doc in sorted(zip(scores, result[\"documents\"][0]), key=lambda x: x[0], reverse=True)]\n",
    "    final_chunks = reranked[:3]\n",
    "\n",
    "    context_text = \" \".join(final_chunks)\n",
    "\n",
    "    \n",
    "    context_text = \" \".join(result[\"documents\"][0])  # combine list of strings into one\n",
    "    clean_chunk = re.sub(r\"\\$.*?\\$\", \"\", context_text)   # remove inline math\n",
    " \n",
    "    # Return list of text chunks\n",
    "    return clean_chunk\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18e8830a-5aac-4c08-8248-856cb107b407",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "QA_MODEL = \"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(QA_MODEL)\n",
    "qa_model     = AutoModelForQuestionAnswering.from_pretrained(QA_MODEL)\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\") \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e5210a1-fd60-478a-a13d-9b8b2e0fc052",
   "metadata": {},
   "source": [
    "def qa_with_confidence(question, context):\n",
    "    inputs = qa_tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = qa_model(**inputs)\n",
    "\n",
    "    start_idx = outputs.start_logits.argmax()\n",
    "    end_idx   = outputs.end_logits.argmax()\n",
    "\n",
    "    answer_tokens = inputs.input_ids[0, start_idx:end_idx+1]\n",
    "    decoded_answer = qa_tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    start_probs = F.softmax(outputs.start_logits, dim=-1)\n",
    "    end_probs   = F.softmax(outputs.end_logits, dim=-1)\n",
    "    confidence  = (start_probs[0, start_idx] * end_probs[0, end_idx]).item()\n",
    "\n",
    "    return decoded_answer, confidence\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a66f471-6aed-4d9a-923b-e23a68b670dd",
   "metadata": {},
   "source": [
    "def cosine_similarity(prediction, ground_truth):\n",
    "    emb_pred = embedder.encode(prediction, convert_to_tensor=True)\n",
    "    emb_gt   = embedder.encode(ground_truth, convert_to_tensor=True)\n",
    "    return util.cos_sim(emb_pred, emb_gt).item() "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69252a79-eba4-4006-9814-c8fdef080318",
   "metadata": {},
   "source": [
    "results = []\n",
    "for _, row in questions.iterrows():\n",
    "    chunks = retrieve_chunks(row.question, row.paper_id)\n",
    "    \n",
    "    if not chunks or chunks.strip() == \"\":\n",
    "        print(f\"Skipping {row.question_id} (no context retrieved)\")\n",
    "        continue\n",
    "    \n",
    "    pred, conf = qa_with_confidence(row.question, chunks)\n",
    "    cos        = cosine_similarity(pred, str(row.free_form_answer))\n",
    "    results.append({\n",
    "        \"question_id\": row.question_id,\n",
    "        \"question\" : row.question,\n",
    "        \"model_answer\": pred,\n",
    "        \"ground_truth\": str(row.free_form_answer),\n",
    "        \"confidence\": conf,\n",
    "        \"cosine_sim\": cos\n",
    "    })\n",
    "\n",
    "scores_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Average Confidence :\", scores_df[\"confidence\"].mean())\n",
    "print(\"Average Cosine Sim :\", scores_df[\"cosine_sim\"].mean())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e26d6408-e2b3-4d6a-bde3-de54acb77fcd",
   "metadata": {},
   "source": [
    "scores_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa0fabed-d405-476b-b373-4b796cf5320a",
   "metadata": {},
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"Starting download and caching of Hugging Face models...\")\n",
    "\n",
    "QA_MODELS = [\n",
    "      \"deepset/tinyroberta-squad2\",\n",
    "      \"deepset/roberta-base-squad2\",\n",
    "      \"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "]\n",
    "\n",
    "for model_name in QA_MODELS:\n",
    "    try:\n",
    "        # print(f\"Loading model: {model_name}\")\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # This is the essential part for downloading and caching the model\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "        \n",
    "        # The following lines simulate a quick inference to ensure everything is loaded correctly\n",
    "        question = \"How many programming languages does BLOOM support?\"\n",
    "        context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\"\n",
    "        inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        answer_start_index = outputs.start_logits.argmax()\n",
    "        answer_end_index = outputs.end_logits.argmax()\n",
    "        predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "        decoded_answer = tokenizer.decode(predict_answer_tokens)\n",
    "        \n",
    "        print(f\"Successfully loaded and tested: {model_name}. Predicted answer: '{decoded_answer}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model {model_name}. Error: {e}\")\n",
    "\n",
    "print(\"Model loading process complete.\")\n",
    "     "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56039ee7-ed19-487a-bb4d-94e72d3924b4",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
